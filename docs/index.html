<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="MSc Global Change Geography" />


<title>Earth Observation</title>
<!-- Material Design fonts -->
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/icon?family=Material+Icons">
<script src="index_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.6/css/bootstrap.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.6/js/bootstrap.min.js"></script>
<script src="index_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<script src="index_files/navigation-1.1/tabsets.js"></script>
<script src="index_files/navigation-1.1/codefolding.js"></script>
<link href="index_files/magnific-popup-1.1.0/magnific-popup.css" rel="stylesheet" />
<script src="index_files/magnific-popup-1.1.0/jquery.magnific-popup.min.js"></script>
<link href="index_files/bootstrap_material-0.1/bootstrap-material-design.min.css" rel="stylesheet" />
<link href="index_files/bootstrap_material-0.1/ripples.min.css" rel="stylesheet" />
<script src="index_files/bootstrap_material-0.1/material.min.js"></script>
<script src="index_files/bootstrap_material-0.1/ripples.min.js"></script>
<link href="index_files/material-0.1/material.css" rel="stylesheet" />
<script src="index_files/material-0.1/material.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #ffffff; color: #a0a0a0; border-right: 1px solid #a0a0a0; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #1f1c1b; background-color: #ffffff; }
code > span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code > span.dt { color: #0057ae; } /* DataType */
code > span.dv { color: #b08000; } /* DecVal */
code > span.bn { color: #b08000; } /* BaseN */
code > span.fl { color: #b08000; } /* Float */
code > span.cn { color: #aa5500; } /* Constant */
code > span.ch { color: #924c9d; } /* Char */
code > span.sc { color: #3daee9; } /* SpecialChar */
code > span.st { color: #bf0303; } /* String */
code > span.vs { color: #bf0303; } /* VerbatimString */
code > span.ss { color: #ff5500; } /* SpecialString */
code > span.im { color: #ff5500; } /* Import */
code > span.co { color: #898887; } /* Comment */
code > span.do { color: #607880; } /* Documentation */
code > span.an { color: #ca60ca; } /* Annotation */
code > span.cv { color: #0095ff; } /* CommentVar */
code > span.ot { color: #006e28; } /* Other */
code > span.fu { color: #644a9b; } /* Function */
code > span.va { color: #0057ae; } /* Variable */
code > span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code > span.op { color: #1f1c1b; } /* Operator */
code > span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code > span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code > span.pp { color: #006e28; } /* Preprocessor */
code > span.at { color: #0057ae; } /* Attribute */
code > span.re { color: #0057ae; } /* RegionMarker */
code > span.in { color: #b08000; } /* Information */
code > span.wa { color: #bf0303; } /* Warning */
code > span.al { color: #bf0303; font-weight: bold; } /* Alert */
code > span.er { color: #bf0303; text-decoration: underline; } /* Error */
code > span. { color: #1f1c1b; } /* Normal */
</style>

<link rel="stylesheet" href="material_adjust.css" type="text/css" />

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->

</head>

<body>

<div class="header-panel shadow z-2">
    <div class="container-fluid">
        <div class="row">
            <div class="col-xs-3">
        <div id="header">
    <h1 class="title">Earth Observation</h1>
                <h4 class="author">MSc Global Change Geography</h4>
                <h4 class="date">Summer term 2019</h4>
        </div>
    </div>
</div>
</div>
</div>


<div class="container-fluid main-container">
    <div class="row">
      <nav class="col-xs-3 menu">
        <div id="toc">
        <ul>
        <li><a href="#hello">Hello!</a></li>
        <li><a href="#introducing-r">Introducing R</a></li>
        <li><a href="#course-materials">Course materials</a></li>
        <li><a href="#session-01-handling-rasters-in-r">Session 01: Handling rasters in R</a></li>
        <li><a href="#session-02-from-dns-to-toa">Session 02: From DNs to TOA</a></li>
        <li><a href="#session-02-data-quality-cloud-masking">Session 02: Data quality &amp; cloud masking</a></li>
        <li><a href="#session-03-vegetation-indices-and-data-transforms">Session 03: Vegetation indices and data transforms</a></li>
        <li><a href="#session-03-training-data-collection">Session 03: Training data collection</a></li>
        <li><a href="#session-04-pixel-based-compositing">Session 04: Pixel-based compositing</a></li>
        <li><a href="#session-05-machine-learning-for-image-classification">Session 05: Machine learning for image classification</a></li>
        <li><a href="#terminology">Terminology</a></li>
        </ul>
        </div>
        
        
        
      </nav>
     <div class="pages col-xs-9">
     <div class="row">
       <div class="col-xs-10">



<div id="hello" class="section level1">
<h1>Hello!</h1>
<div class="figure">
<img src="fig/header.png" />

</div>
<div id="about-earth-observation" class="section level2">
<h2>About Earth Observation</h2>
<p>Earth Observation is an advanced course for students of the Master of Science <a href="https://www.geographie.hu-berlin.de/en/studies/study-programs/master-degree-programs/master-of-science">“Global Change Geography”</a> of Humboldt-Universität zu Berlin. In this course, we cover multiple aspects of optical remote sensing by working with multi-sprectral Landsat and Sentinel 2 imagery. The course is fully based on open source software, including R and QGIS.</p>
<hr />
</div>
<div id="learning-goals-course-contents" class="section level2">
<h2>Learning goals &amp; course contents</h2>
<p>The main goal of this course is to provide you with the necessary knowledge and tools for using optical remote sensing datasets and methods in the geo-scientific context. We want you to enhance your ability of problem-solving, empowering you to perform research independently. To that end, we cover aspects of data acquisition, spatial data handling in R and QGIS, basics of image pre-processing, higher-level processing such as pixel-based compositing and time-series binning. The course contents are related to our lab´s research foci, both in terms of methods and study regions. You may want to check out our <a href="https://www.geographie.hu-berlin.de/en/professorships/geomatics/publications-en">publications</a>, <a href="https://www.geographie.hu-berlin.de/en/professorships/geomatics/projects">current projects</a>, or have a look at <a href="https://www2.hu-berlin.de/geomultisens/europeanLandCover/euroLandCover.html">this example</a>.</p>
<p>In the course you will learn about current state-of-the-art methods in image processing and time series analyses of optical satellite imagery. The course covers methods related to data quality, cloud masking, vegetation indices, multi-temporal image analyses, machine learning classification algorithms, area adjusted accuracy assessment, time series analyses, and image compositing. We use these methods for mapping of forest types, forest cover changes, agricultural dynamics in the Carpathian ecoregion (Poland), the Southern Brazilian Amazon, and Crete in Greece.</p>
<hr />
</div>
<div id="requirements" class="section level2">
<h2>Requirements</h2>
<p>A good understanding of basic principles of remote sensing is needed to follow this course. Participants should furthermore have a basic understanding of R, including syntax, data types and knowledge on how to read, manipulate and write data. As you followed the curriculum of the MSc program, you most likely joined the module “Quantitative Methods for Geographers”, in which you learned using R for statistical problems. Here, we built on your existing knowledge. If you are not enrolled in the MSc program, feel free to look at the <a href="https://github.com/corneliussenf/quantitative_methods">course materials</a>.</p>
<p>Alternatively, you may want to follow one of the numerous tutorials for R fundamentals (e.g., <a href="https://www.rstudio.com/online-learning/">RStudio</a>, <a href="https://www.datacamp.com/courses/free-introduction-to-r">DataCamp</a>, <a href="http://www.r-tutorial.nl/">UMC Utrecht</a>, <a href="http://adv-r.had.co.nz/">Advanced R by Hadley Wickham</a>, <a href="https://r-graphics.org/">R Graphics Cookbook</a>), or one of those specifically for geodata processing (e.g., <a href="https://geoscripting-wur.github.io/">Wageningen University</a>, <a href="https://www.earthdatascience.org/">University of Colorado</a>).</p>
<hr />
<!-- ################################## INTRO R ############################################## -->
</div>
</div>
<div id="introducing-r" class="section level1">
<h1>Introducing R</h1>
<div id="why-do-we-use-r" class="section level2">
<h2>Why do we use R?</h2>
<p>R is a programming language and open source software environment for statistical computing and graphics. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. It was developed by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand. The name R originates the first names of the two authors and refers to the programming language S. The project was conceived in 1992, with an initial version released in 1995 and a stable beta version in 2000.</p>
<p>Learning R has tons of advantages. It is a great starting point for those eager to learn programming. R offers increasingly specialized tools for data wrangling, statistical analyses, and visualization. The CRAN package repository currently features &gt;13,000 packages serving a variety of purposes, e.g. data manipulation (<code>tidyr</code>, <code>dplyr</code>, <code>caret</code>), visualization (<code>ggplot2</code>, <code>ggmap</code>, <code>rasterVis</code>), and geodata handling (<code>raster</code>, <code>rgdal</code>, <code>sp</code>, <code>sf</code>). You will notice that a huge share of figures in scientific publications was produced using R. The R community is huge, and offers great support. R is extremely popular in science &amp; industry, so a proficiency in R opens a wide array of job opportunities. Everything is free and open source.</p>
<div class="figure">
<img src="fig/fig00.png" alt="A rising tide for R (Tipman 2015; doi: 10.1038/517109a)" />
<p class="caption">A rising tide for R (Tipman 2015; doi: 10.1038/517109a)</p>
</div>
<hr />
</div>
<div id="coding-style" class="section level2">
<h2>Coding style</h2>
<p>A few basic rules apply to coding in R. Here is a short summary of <a href="http://adv-r.had.co.nz/Style.html">Hadley Wickham´s style guide</a>:</p>
<ul>
<li>Regularly save your progress.</li>
<li><p>Script names should be meaningful and end in ‘.R’.</p></li>
<li>Comment (#) your code &amp; separate it into readable chunks.</li>
<li><p>Try to limit your code to 80 characters per line.</p></li>
<li>Variable and function names should be lowercase.</li>
<li><p>Variable names should be nouns and function names verbs.</p></li>
<li>Place spaces around operators (=, +, -, &lt;-, etc.) and after commas.</li>
<li><p>Use &lt;-, not =, for assignment.</p></li>
</ul>
<p>An example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">######################################################
<span class="co"># Creating random data and a correlated response </span>
<span class="co"># Philippe Rufin, 2019</span>

<span class="co"># Load all required packages</span>
<span class="kw">library</span>(ggplot2)

<span class="co"># Create random data</span>
x &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">50</span>, <span class="dv">0</span>, <span class="dv">2</span>)

<span class="co"># Build function to simulate response</span>
create.response &lt;-<span class="st"> </span><span class="cf">function</span>(x){x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>, <span class="dv">0</span>, <span class="fl">0.2</span>)}

<span class="co"># Apply function to random data</span>
y &lt;-<span class="st"> </span><span class="kw">create.response</span>(x)

<span class="co"># Make a dataframe</span>
data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&#39;x&#39;</span> =<span class="st"> </span>x, <span class="st">&#39;y&#39;</span> =<span class="st"> </span>y)

<span class="co"># Plot the simulated dataset</span>
<span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="index_files/figure-html/style-1.png" width="288" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Investigate correlation in the data</span>
<span class="kw">cor</span>(data<span class="op">$</span>x, data<span class="op">$</span>y)</code></pre></div>
<pre><code>[1] 0.9346357</code></pre>
<hr />
</div>
<div id="help" class="section level2">
<h2>Help!</h2>
<p>If you get stuck, there are plenty of things you can do:</p>
<ul>
<li>Seek the function´s help page (i.e. highlight the function and hit F1)</li>
<li>Search your problem or error message</li>
<li>Ask your colleagues</li>
<li>Use the moodle course forum</li>
<li>Check forums (e.g., <a href="https://stackoverflow.com/">StackOverflow</a>)</li>
</ul>
<hr />
</div>
</div>
<div id="course-materials" class="section level1">
<h1>Course materials</h1>
<div id="readings" class="section level2">
<h2>Readings</h2>
<p>The first sessions of the course contain reading materials, such as are peer-reviewed papers and technical reports. You will find the reading materials for the next session at the end of each session. We highlight aspects to focus upon to streamline the reading process and facilitate the discussion. We are looking forward to lively discussions of the reading materials and critical questions from your end.</p>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>All data used in the course is openly accessible. Mostly, we´ll be working with Landsat images, which you can access through the USGS Earth Explorer. We provide download links to the datasets for each session. It will be helpful if you organize your data in a course directory on your local machine (MSc students might want to use drive <code>O:/Student_Data/your_name/EO/</code>). We will refer to this folder as <code>course.dir</code> throughout this course. Create subdirectories for each session, e.g. <code>course.dir/S01/</code> and separate data, code and course materials in additional sub-directories (e.g. <code>/data</code>, <code>/code</code>, <code>/docs</code>).</p>
</div>
<div id="exercises" class="section level2">
<h2>Exercises</h2>
<p>The weekly exercises are defined in the respective session. Each session comprises several tasks that involve scipting in R. Course participants must submit completed exercises, documented as R scripts, in <a href="http://moodle.hu-berlin.de/">moodle</a> to pass. Weekly submission deadlines are every sunday, 23:59. Please name the script of your work group as SXX_name1_name2.R, e.g. S01_ernst_rufin.R. Please structure your script for every exercise as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">#############################################################################
<span class="co"># MSc Earth Observation Exercise [Session number]</span>
<span class="co"># [Your Name]</span>
#############################################################################

<span class="co"># Load packages, use install.packages(&#39;packagename&#39;) to install if needed</span>
<span class="kw">library</span>(raster)

<span class="co"># Change raster options to store large rasters in temp files on disk</span>
<span class="kw">rasterOptions</span>(<span class="dt">maxmemory =</span> <span class="fl">1e6</span>)

<span class="co"># Define the folder that contains your data...</span>
data.dir &lt;-<span class="st"> &#39;course.dir/S01/data/&#39;</span>

#############################################################################
<span class="co"># 1)    </span>
#############################################################################

<span class="co"># Comments for task 1</span>


#############################################################################
<span class="co"># 2)    </span>
#############################################################################

<span class="co"># ...</span></code></pre></div>
<!-- ################################## SESSION 01 ############################################## -->
</div>
</div>
<div id="session-01-handling-rasters-in-r" class="section level1">
<h1>Session 01: Handling rasters in R</h1>
<div id="learning-goals" class="section level2">
<h2>Learning goals</h2>
<p>In this session, you will</p>
<ul>
<li>Acquire multi-spectral satellite data</li>
<li>Read &amp; write raster data</li>
<li>Manipulate the spatial extent of rasters</li>
<li>Extract cell values &amp; plot a spectral profile</li>
</ul>
<hr />
</div>
<div id="the-raster-package" class="section level2">
<h2>The <code>raster</code> package</h2>
<p>It´s great, as it facilitates raster data handling. It allows us to access file characteristics before loading data into memory, facilitates handling of coordinate reference systems and spatial extents. We can use it to perform raster algebra, to combine raster and vector datasets (e.g. ESRI shapefiles), or to convert raster files into matrices, which are compatible with the base functions to access image statistics, develop models, slice data dimensions etc.</p>
<p>There are a couple of things that the raster package does not provide. For example, advanced visualization of spatial datasets and manual operations, such collecting training or validation data, are preferably done in a GIS environment (e.g. QGIS). Furthermore, processing large data volumes in R can be quite time-consuming (we often use Python instead, it´s syntax is quite similar to R).</p>
<p>You install the raster package just like any other package in R. Dependencies will automatically be installed. On some machines, you might need to install <code>rgdal</code> manually.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Install the raster and rgdal packages</span>
<span class="kw">install.packages</span>(<span class="st">&#39;raster&#39;</span>)
<span class="kw">install.packages</span>(<span class="st">&#39;rgdal&#39;</span>)

<span class="co"># Load the package</span>
<span class="kw">library</span>(raster)</code></pre></div>
<hr />
</div>
<div id="exercise" class="section level2">
<h2>Exercise</h2>
<div id="data-acquisition" class="section level3">
<h3>1) Data acquisition</h3>
<p>Let´s get some Landsat data. Visit the <a href="http://earthexplorer.usgs.gov/">USGS Earth Explorer</a> and use the Adress/Place field to navigate to Wisła, Poland (lat,lon: 49.6473,18.8677). Switch to the ‘Data Sets’ tab and select Landsat -&gt; Landsat Collection 1 Level-1. Tick the ‘Landsat 8 OLI/TIRS C1 Level 1’ box and click on ‘Results &gt;&gt;’.</p>
<p>You´ll get several hundreds of results, so let´s narrow down the search. Under ‘Search Criteria’, define an acquisition date range between February 2014 and August 2014. Switch to the ‘Additional Criteria’ tab. Let´s choose a scene cloud cover of ‘Less than 40%’, and select the ‘Tier 1’ category.</p>
<p>Find the following images:</p>
<ul>
<li>LC08_L1TP_189025_20140716_20170421_01_T1</li>
<li>LC08_L1TP_189025_20140310_20170425_01_T1</li>
</ul>
<p>Check <a href="https://www.usgs.gov/land-resources/nli/landsat/landsat-collection-1">this website</a> to get an overview of the Landsat Collection 1 file naming convention (product identifiers) and further information such as processing levels.</p>
<p>Visualize the images in the Earth Explorer interface by clicking on the small image icon. For downloading the data, you will need an EarthExplorer account. You may register and download the .tar.gz files. If you prefer not to register, you can <a href="https://box.hu-berlin.de/f/5248da1584054eb6ba51/?dl=1">download the files from our repository</a>. Unpack the files in your session directory.</p>
<hr />
</div>
<div id="reading-data" class="section level3">
<h3>2) Reading data</h3>
<p>Today, you will make use of R´s raster package classes and functions which are well described in the package documentation. Get acquainted with the following classes and functions and find out what they are useful for: <code>raster()</code>, <code>stack()</code> ,<code>extent()</code>, <code>crop()</code>, <code>extract()</code>, <code>CRS()</code>, <code>projectRaster()</code>, <code>plotRGB()</code>, <code>writeRaster()</code></p>
<p>Visit the folder containing the unpacked Landsat image. Did you take a close look at the <a href="https://www.usgs.gov/media/images/landsat-collection-1-product-identifier">Landsat file naming convention</a>? Practically, it provides some basic meta-information. For instance, <code>LC08_L1TP_189025_20140310_20170425_01_T1</code> is a sequence of information on the sensor, processing level, WRS path and row, acquisition date, processing date, collection, and collection tier, separated by ’_’.</p>
<p>As you can see, the Landsat images are delivered as single-band files. The single bands should be stacked for further analyses. For stacking, all input files must have matching extents and the identical projection. Create a stack for each of the two Landsat 8 images.</p>
<p>Important: Please include only the following bands: blue, green, red, near infrared, shortwave infrared 1, shortwave infrared 2 (in this order). Check the list of <a href="https://landsat.usgs.gov/what-are-band-designations-landsat-satellites/">Landsat spectral bands</a> for a recap. Always keep the band designations in mind, as this can cause confusion, e.g. when combining Landsat 5 and Landsat 8 data.</p>
<div class="figure">
<img src="fig/ls_bands.jpg" alt="Band designations for Landsat satellites" style="width:70.0%" />
<p class="caption">Band designations for Landsat satellites</p>
</div>
<p>Try to create the two stacks with a minimum amount of code as possible! Consider using helper functions such as <code>paste0()</code>, <code>dir()</code> or <code>list.files()</code>.</p>
<hr />
</div>
<div id="manipulating-data" class="section level3">
<h3>3) Manipulating data</h3>
<p>Investigate the stack. In which projection is the data delivered?</p>
<p>Compare the extent of the two images. You will notice that they vary. Trying to stack images of different extent will cause an error message claiming:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">image.stack &lt;-<span class="st"> </span><span class="kw">stack</span>(image.one, image.two)
Error <span class="cf">in</span> <span class="kw">compareRaster</span>(x) <span class="op">:</span><span class="st"> </span>different extent</code></pre></div>
<p>To stack both images, we need to crop (i.e. clip, or cut) the images to their common extent. Find an efficient way to identify the common extent of the images, defined as <code>common.extent &lt;- c(xmin, xmax, ymin, ymax)</code> (in projected coordinates).</p>
<div class="figure">
<img src="fig/fig01.png" alt="Identifying the common extent of several images" />
<p class="caption">Identifying the common extent of several images</p>
</div>
<p>The common extent of the two images is pretty large. In order to reduce the amount of data for the next steps, we should crop the images to our region of interest, a part of the Western Beskids, defined by <code>roi.extent &lt;- c(327945, 380325, 5472105, 5521095)</code></p>
<hr />
</div>
<div id="writing-data" class="section level3">
<h3>4) Writing data</h3>
<p>Write the cropped stacks to your folder using <code>writeRaster()</code>. Use the <code>GTiff</code> format and the <a href="https://www.rdocumentation.org/packages/raster/versions/2.5-8/topics/dataType">appropriate datatype</a>. Why is this important?</p>
<hr />
</div>
<div id="visualizing-data" class="section level3">
<h3>5) Visualizing data</h3>
<p>Open the cropped images in QGIS. Seek the symbology to create a true-color (red, green, blue) and a false-color representation (e.g., RGB: swIR1, nIR, red) of each image. Make sure to properly consider the order of bands in your stack (blue, green, red, nIR, swIR 1, swIR 2) in relation to your computer screen´s color channels (RGB).</p>
<p>Use the <code>plotRGB()</code> function in R to create another false-color visualization of the images in R.</p>
<div class="figure">
<img src="fig/s01_falsecolor_432.png" alt="False color visualization (RGB: nIR, red, green)" />
<p class="caption">False color visualization (RGB: nIR, red, green)</p>
</div>
<hr />
</div>
<div id="extracting-spectral-profiles" class="section level3">
<h3>6) Extracting spectral profiles</h3>
<p>Use the <code>extract()</code> function to get spectral profiles from both images. Use the following coordinate:</p>
<p><code>coordinate &lt;- data.frame('x' = 355623, 'y' = 5486216)</code>.</p>
<p>Visualize the results in R. Can you create one plot that shows two spectral profiles (one for each image in the stack), while accounting for the band wavelength and acquisition date? Can you guess what type of surface we are looking at?</p>
<div class="figure">
<img src="fig/s01_spectra.png" alt="Spectral profiles for two observation dates" style="width:60.0%" />
<p class="caption">Spectral profiles for two observation dates</p>
</div>
<hr />
</div>
</div>
<div id="reading-materials" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following paper:</p>
<p><a href="https://doi.org/10.1016/j.rse.2011.10.028">Zhu, Z., &amp; Woodcock, C.E. (2012). Object-based cloud and cloud shadow detection in Landsat imagery. Remote Sensing of Environment, 118, 83–94.</a></p>
<p>This is a rather technical reading, which introduces the Fmask algorithm for automated cloud and cloud shadow detection. It has been widely used for cloud detection on Landsat TM and ETM+ data, and was enhanced for the use with Landsat OLI and Sentinel 2 data (documented in <a href="https://doi.org/10.1016/j.rse.2014.12.014">Zhu et al. 2015</a>).</p>
<p>While reading focus on the following broad questions:</p>
<ul>
<li>Why do we need automated cloud masking?</li>
<li>How does it work in principle?</li>
<li>Where are the limitations?</li>
</ul>
<p>More specifically, think about the following:</p>
<ul>
<li>How were the thresholds for spectral tests derived?</li>
<li>How will different error types impact further analyses?</li>
</ul>
<!-- ################################## SESSION 02 ############################################## -->
</div>
</div>
<div id="session-02-from-dns-to-toa" class="section level1">
<h1>Session 02: From DNs to TOA</h1>
<hr />
<div id="learning-goals-1" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Convert digital number values to top-of-atmosphere reflectance</li>
<li>Compare top-of-atmosphere reflectance to surface reflectance</li>
</ul>
<hr />
</div>
<div id="what-is-radiance-what-are-dns" class="section level2">
<h2>What is radiance / what are DNs?</h2>
<p>This session is the most technical of the entire course. Today we will be dealing with physical units, conversion, and data quality. Let´s kick it off with a short recap. Towards the end of the last exercise, you produced a plot of measurements in the different bands of a Landsat image, similar to this one:</p>
<div class="figure">
<img src="fig/s01_spectra.png" alt="Spectral profiles extracted from a Landsat Level 1 image" style="width:60.0%" />
<p class="caption">Spectral profiles extracted from a Landsat Level 1 image</p>
</div>
<p>Our y-axis label was “DN”, or digital number. Now, what is that again? Earth observing sensors, such as the ones on board the Landsat satellites register radiance at the top of the atmosphere. Radiance is expressed in watt per <a href="https://en.wikipedia.org/wiki/Steradian">steradian</a> per square meter.</p>
<p>Storing data in radiance units is difficult and therefore sensors translate measured radiance into DNs. Therefore, the range of energy measured by the sensor is broken into distinct units (DNs). Sensor-specific calibration determines the minimum and maximum amount of radiance that can be measured. DNs express the amount of radiance in relation to these sensor-specific calibration coefficients. The total number of possible DNs is what we refer to as the radiometric resolution of the sensor.</p>
<p>As an example, Landsat 4, 5, and 7 worked with a radiometric resolution of 8 bit. This allows for <code>2^8</code>, or 255 distinct DNs values. Landsat 8 works with 12 bit radiometric resolution, which is artificially quantized to 16 bit. The 16 bit resolution can represent much more different grey shades, exactly <code>2^16</code>, or 65,536 values. Let´s put this into code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(raster)

<span class="co"># Define the number of bits</span>
bit &lt;-<span class="st"> </span><span class="dv">4</span>

<span class="co"># How many grey tones can we represent given bit?</span>
<span class="kw">print</span>(<span class="kw">paste0</span>(bit, <span class="st">&#39; bits produce &#39;</span>, <span class="dv">2</span><span class="op">^</span>bit, <span class="st">&#39; grey tones.&#39;</span>))</code></pre></div>
<pre><code>[1] &quot;4 bits produce 16 grey tones.&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a raster with one line and 2^bit columns,</span>
<span class="co"># whereas cell values are filled with a vector of numbers from 1 2^bit</span>
r &lt;-<span class="st"> </span><span class="kw">raster</span>(<span class="dt">nrows =</span> <span class="dv">1</span>, <span class="dt">ncols =</span> <span class="dv">2</span><span class="op">^</span>bit, <span class="dt">vals =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span><span class="op">^</span>bit))

<span class="co"># Plot the image and assign grey-scale values to the cells</span>
<span class="kw">image</span>(r, <span class="dt">col =</span> <span class="kw">grey.colors</span>(<span class="dv">2</span><span class="op">^</span>bit, <span class="dt">start =</span> <span class="dv">0</span>, <span class="dt">end =</span> <span class="dv">1</span>), <span class="dt">main =</span> <span class="kw">paste0</span>(bit, <span class="st">&quot; bit raster&quot;</span>))</code></pre></div>
<p><img src="index_files/figure-html/greyscale-1.png" width="576" /></p>
<hr />
</div>
<div id="dns-to-toa" class="section level2">
<h2>DNs to TOA</h2>
<p>As DNs are dependent on the sensor calibration, identical measurements yield differing DNs across sensors. DNs are physically not meaningful. Instead of DNs, often we are interested in reflectance. Reflectance expresses the fraction of reflected radiance relative to the total incoming energy (sun), and is scaled between 0 and 1. Many applications require data converted to reflectance and/or corrected for the influence of the atmosphere.</p>
<ul>
<li>Sensor calibration (DN to radiance)</li>
<li>Conversion to top-of-atmosphere reflectance (radiance to TOA)</li>
<li>Atmospheric correction yielding bottom-of-atmosphere reflectance (TOA to BOA, or surface reflectance)</li>
</ul>
<p>A conversion of DNs into radiance can be easily undertaken. By accounting for sun-sensor geometries and solar irradiance, we can easily infer top-of-atmosphere reflectance (TOA) from radiance. This facilitates, e.g., a comparison of measurements from different sensors.</p>
<p>Practically, this involves a linear scaling of the DN values which uses two band-specific rescaling factors. One is multiplicative, one is additive. Formulas are explained in the <a href="https://www.usgs.gov/land-resources/nli/landsat/using-usgs-landsat-level-1-data-product">USGS guide for conversion to TOA Reflectance</a>. With Landsat 8 Collection 1 data, we can use a single set of coefficients to convert DNs to TOA reflectance. Note that this is not the case for all sensors.</p>
<p><span class="math inline">\(ρλ&#39; = M_ρ * Q_{cal} + A_ρ\)</span></p>
<p>where</p>
<p><span class="math inline">\(ρλ&#39;\)</span> = TOA planetary reflectance, without correction for solar angle.</p>
<p><span class="math inline">\(M_ρ\)</span> = Band-specific multiplicative rescaling factor from the metadata.</p>
<p><span class="math inline">\(Q_{cal}\)</span> = Quantized and calibrated standard product pixel values (DN).</p>
<p><span class="math inline">\(A_ρ\)</span> = Band-specific additive rescaling factor from the metadata.</p>
<p>In a next step, we can correct for solar angle during image acquisition.</p>
<p><span class="math inline">\(ρλ = ρλ&#39; / cos(θ_{SZ}) = ρλ&#39; / sin(θ_{SE})\)</span></p>
<p>where</p>
<p><span class="math inline">\(ρλ\)</span> = TOA planetary reflectance corrected for solar angle.</p>
<p><span class="math inline">\(θ_{SZ}\)</span> = Local solar zenith angle, whereas <span class="math inline">\(θ_{SZ} = 90° - θ_{SE}\)</span>.</p>
<p><span class="math inline">\(θ_{SE}\)</span> = Local sun elevation angle.</p>
<hr />
</div>
<div id="toa-to-boa" class="section level2">
<h2>TOA to BOA</h2>
<p>A final step towards comparable measurements of the Earth surface is the atmospheric correction. By doing atmospheric correction, we (theoretically) eliminate the influence of the atmosphere. We therefore call the product “surface reflectance” or “bottom of atmosphere reflectance” (BOA). Theoretically, the space-borne sensor´s measurements should be identical to ground-based measurements.</p>
<hr />
</div>
<div id="exercise-1" class="section level2">
<h2>Exercise</h2>
<p>During this exercise you will learn how to convert Landsat-8 Collection 1 DNs to TOA reflectance, and compare and characterize spectral appearance of different land cover types in TOA and BOA imagery. The data are provided in <a href="https://box.hu-berlin.de/f/97b7bd39ac6d48488f74/?dl=1">our repository</a>. After unpacking, you find the following folders:</p>
<ul>
<li><p>DN/LC08_L1TP_189025_20141105_20170417_01_T1/ - the directory containing the “L1TP” Landsat product (i.e. terrain-corrected (“orthorectified”) data in DNs), including the metadata file (MTL.txt) and the Quality Band (BQA.tif) which contains information on radiometric saturation, clouds, cloud shadows and more.</p></li>
<li><p>SR/LC081890252014110501T1-SC20170927102137/ – the directory containing Level-2 data, i.e. atmospherically corrected, or bottom-of-atmosphere data.</p></li>
</ul>
<div id="dn-to-toa-conversion" class="section level3">
<h3>1) DN to TOA conversion</h3>
<p>Familiarize yourself with the L1TP data. Visualize individual bands, have a look at the Level-1 metadata file (LC08_L1TP_ XXX_MTL.txt) and the information provided within. Again, we only use the six reflective bands of the OLI sensor in this exercise (3xVIS, nIR, 2xswIR)</p>
<p>Read the Landsat metadata file in R using <code>read.delim()</code> and extract the necessary information for the top-of-atmosphere conversion: - <code>REFLECTANCE_MULT_BAND</code> - <code>REFLECTANCE_ADD_BAND</code> - <code>SUN_ELEVATION</code></p>
<p>Use the pattern matching function <code>grep()</code> to find the corresponding entries in the metadata file and store them as numeric vectors. Do this as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define the folder that contains your data...</span>
path &lt;-<span class="st"> &#39;course.dir/S02/data/DN&#39;</span>

<span class="co"># Read MTL file</span>
mtl &lt;-<span class="st"> </span><span class="kw">list.files</span>(data.path, <span class="dt">pattern=</span><span class="st">&quot;MTL.txt$&quot;</span>, <span class="dt">recursive=</span>T, <span class="dt">full.names=</span>T)
mtl.txt &lt;-<span class="st"> </span><span class="kw">read.delim</span>(mtl, <span class="dt">sep =</span> <span class="st">&#39;=&#39;</span>, <span class="dt">stringsAsFactors =</span> F)

<span class="co"># Extract numeric values</span>
REFLECTANCE_MULT_BAND &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(mtl.txt[<span class="kw">grep</span>(<span class="st">&quot;REFLECTANCE_MULT_BAND&quot;</span>,mtl.txt<span class="op">$</span>GROUP),][<span class="dv">2</span><span class="op">:</span><span class="dv">7</span>,<span class="dv">2</span>])</code></pre></div>
<p>Convert the DN values to TOA following the “Conversion to TOA Reflectance” section in the <a href="https://www.usgs.gov/land-resources/nli/landsat/using-usgs-landsat-level-1-data-product">USGS guide for conversion to TOA Reflectance</a>. Important things to keep in mind:</p>
<ul>
<li>You can apply the conversion to all bands in a stack simultaneously.</li>
<li>The Landsat data is provided as integer values. When applying the equation, the data will be cast to float. You will have to re-convert to integer at some point. Use a reflectance scaling factor of 10,000 during the conversion.</li>
<li>The sun elevation angle provided in the metadata file is reported in degrees. The <code>sin()</code> function expects that angles are provided in radians. For the conversion to work correctly, you need to convert <code>SUN_ELEVATION</code> into radians. Make use of this helper function:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Helper-function to convert degrees to radians</span>
deg2rad &lt;-<span class="st"> </span><span class="cf">function</span>(deg){ (deg <span class="op">*</span><span class="st"> </span>pi) <span class="op">/</span><span class="st"> </span>(<span class="dv">180</span>) }</code></pre></div>
<p>Write the result of your TOA conversion to disk in the <code>GTiff</code> format. Make sure to use an integer data type to save disk space.</p>
</div>
<div id="compare-toa-with-boa" class="section level3">
<h3>2) Compare TOA with BOA</h3>
<p>Open the BOA and the TOA file in QGIS and visually assess the differences in the spectral signature between the SR and TOA files. Use the info tool´s graph view to investigate the spectral signatures of each of the following land cover types:</p>
<ul>
<li>Deciduous forest</li>
<li>Coniferous forest</li>
<li>Grassland</li>
<li>Cropland</li>
<li>Impervious surfaces</li>
<li>Water</li>
</ul>
<p>In your R script, take notes on the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li><p>What are the most evident differences between TOA and SR reflectance spectra?</p></li>
<li><p>Briefly summarize the spectral appearance of the six land cover types, and the main difference between the TOA and SR.</p></li>
</ol>
<hr />
</div>
</div>
</div>
<div id="session-02-data-quality-cloud-masking" class="section level1">
<h1>Session 02: Data quality &amp; cloud masking</h1>
<div id="learning-goals-2" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Understand how to use the Landsat Collection 1 quality bands</li>
<li>Produce cloud / cloud shadow masks of differing confidence levels</li>
<li>Mask Landsat images</li>
</ul>
<hr />
</div>
<div id="landsat-data-quality" class="section level2">
<h2>Landsat data quality</h2>
<p>Think about the following question and exchange with your neighbor: Which issues affect the data quality of optical satellite images?</p>
<p>How to find out if a pixel is affected by any of these issues? Luckily, the USGS provides quality bands for the Landsat Collection 1 products. If we look up the <a href="https://www.usgs.gov/land-resources/nli/landsat/landsat-collection-1-level-1-quality-assessment-band">Landsat QA band website</a> to find out what this band contains, we find the following information:</p>
<p><em>“Each pixel in the QA band contains unsigned integers that represent bit-packed combinations of surface, atmospheric, and sensor conditions that can affect the overall usefulness of a given pixel.”</em></p>
<div class="figure">
<img src="fig/s02_qa_band.png" alt="False-color representation of a cloudy Landsat Level 1 image (left) and a grey-scale visualization of the Landsat QA band (right)" style="width:60.0%" />
<p class="caption">False-color representation of a cloudy Landsat Level 1 image (left) and a grey-scale visualization of the Landsat QA band (right)</p>
</div>
<p>In the following, you will learn to understand what that means. Please read this section carefully and exchange with your neighbor.</p>
<div id="the-binary-system" class="section level3">
<h3>The binary system</h3>
<p>Remember the binary numeral system (0 / FALSE or 1 / TRUE)?</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/7/75/Binary_counter.gif" alt="The binary number system. Source: Wikipedia" style="width:100.0%" />
<p class="caption">The binary number system. Source: Wikipedia</p>
</div>
<p>The binary system represents numbers through sequences of bits. Each bit has a position and a state. The positions are numbered from 0 to the total number of bits, read from right to left. The state of a bit can be either, 0, or 1. Consider the meaning of 0 as „FALSE“ or „NO“, 1 equals „TRUE“ or „YES“.</p>
<p>In the example above, five bit positions are given. Each position is sequentially numbered, starting with 0, read from right to left. By calculating 2 to the power of the bit position, we receive the value of each bit. For all bits with the value 1, we sum the respective values. Different combinations of states and positions therefore enable us to represent integer numbers.</p>
<p>The more positions or bits we have the higher numbers we can generate. With 5 bits, we can represent the numbers 0 – 31 (32 unique values), with 16 bits, we can represent the numbers 0 - 65,535 (65,536 unique values). Following the example with 5 bits, we can for instance represent these numbers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 00001</span>
<span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">0</span></code></pre></div>
<pre><code>[1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 00111</span>
<span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">0</span></code></pre></div>
<pre><code>[1] 7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 11111</span>
<span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">0</span></code></pre></div>
<pre><code>[1] 31</code></pre>
</div>
<div id="landsat-qa-band" class="section level3">
<h3>Landsat QA band</h3>
<p>The Landsat QA band is coded in 16 bit, which are arranged from right to left. Each bit has a specific meaning in terms of data quality e.g. bit 4 (or position 5 from the right) means „cloud“. If bit 4 has the state 1 (or”YES“, or”TRUE“), there was a cloud detected in that pixel. If bit 4 has the state 1, the resulting integer value will be 2^4 = 16. Following this logic, different combinations of true / false conditions result in different integer values.</p>
<div class="figure">
<img src="fig/s02_landsat_qa.png" alt="Landsat QA band bit designation" style="width:80.0%" />
<p class="caption">Landsat QA band bit designation</p>
</div>
<p>In the Landsat QA band, there are furthermore single and double bits. Single bits inform about one condition in a binary manner:</p>
<p>Single bits (0, 1, and 4): - 0 = “No” = This condition does not exist - 1 = “Yes” = This condition exists</p>
<p>Double bits can represent conditions in more detail. Some double bits (5-6, 7-8, 9-10, 11-12, read from left to right) represent levels of confidence that a condition exists:</p>
<ul>
<li>00 = “Not Determined” / This condition does not exist</li>
<li>01 = “Low” = Algorithm has low to no confidence that this condition exists (0-33 percent confidence)</li>
<li>10 = “Medium” = Algorithm has medium confidence that this condition exists (34-66 percent confidence)</li>
<li>11 = “High” = Algorithm has high confidence that this condition exists (67-100 percent confidence)</li>
</ul>
<p>The radiometric saturation bits (2-3), read from left to right, represent how many bands contain radiometric saturation:</p>
<ul>
<li>00 - No bands contain saturation</li>
<li>01 - 1-2 bands contain saturation</li>
<li>10 - 3-4 bands contain saturation</li>
<li>11 - 5 or more bands contain saturation</li>
</ul>
<hr />
</div>
</div>
<div id="de-coding-quality-band-values" class="section level2">
<h2>De-coding quality band values</h2>
<p>Let´s look at the integer value 2804 to illustrate how this works. R offers a good way of dealing with bit-packed information. The <code>intToBits()</code> function converts integer numbers into sequences of bits, whereas 32 double-digit bits are returned by default.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Convert integer to bit sequence</span>
<span class="kw">intToBits</span>(<span class="dv">2804</span>)</code></pre></div>
<pre><code> [1] 00 00 01 00 01 01 01 01 00 01 00 01 00 00 00 00 00 00 00 00 00 00 00
[24] 00 00 00 00 00 00 00 00 00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Convert the double-digit into single-digit bits</span>
<span class="kw">as.numeric</span>(<span class="kw">intToBits</span>(<span class="dv">2804</span>))</code></pre></div>
<pre><code> [1] 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We´re looking at 16 bit data, so let´s deprecate the unused bits</span>
<span class="kw">as.numeric</span>(<span class="kw">intToBits</span>(<span class="dv">2804</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">16</span>])</code></pre></div>
<pre><code> [1] 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Bit sequences are read from right to left, so we need to reverse the order</span>
<span class="kw">rev</span>(<span class="kw">as.numeric</span>(<span class="kw">intToBits</span>(<span class="dv">2804</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">16</span>]))</code></pre></div>
<pre><code> [1] 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0</code></pre>
<p>Now we can compare with the Landsat 8 Collection 1 Level 1 QA band bit designation. What´s the pixel quality information of the integer value 2804?</p>
<div class="figure">
<img src="fig/s02_landsat_qa_decode.PNG" alt="Landsat QA band bit designation" style="width:70.0%" />
<p class="caption">Landsat QA band bit designation</p>
</div>
<p>We are looking at a cloudy pixel with radiometric saturation affecting 1-2 bands.</p>
<hr />
</div>
<div id="exercise-2" class="section level2">
<h2>Exercise</h2>
<div id="investigating-the-qa-band" class="section level3">
<h3>1) Investigating the QA band</h3>
<p>Let´s have a look at the Landsat Collection 1 Quality Band (BQA). First, load the BQA band and find the three most frequent values using <code>freq()</code>.</p>
<p>As you can see, the band contains integer values. By decoding the integer values into 16 bit binary strings, we can read the quality information for each pixel. Use the <code>intToBits()</code> function to decode the three most frequent BQA values and decipher their meaning using the Landsat quality band documentation. <code>intToBits()</code> returns 32 bits by default. Make sure you only look at bits 1 to 16. Also, keep in mind the right-to-left order when comparing the decoded bits with the table. You might want to use <code>rev()</code> to invert the order of the outputs by <code>intToBits()</code>. Note your findings as a comment in the script:</p>
<pre><code># What do the most frequent values mean? Decode each integer value into bits and 
# describe their meaning here: 

# Most frequent value: 
# Second most frequent value: 
# Third most frequent value: </code></pre>
</div>
<div id="creating-a-cloud-mask" class="section level3">
<h3>2) Creating a cloud mask</h3>
<p>By converting integer values into binary bits, we can extract specific attributes from the BQA. Let´s to this in a systematic manner by defining a function, which yields TRUE for binary codes with bit 0 = 1:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define function to find fill values from Landsat BQA</span>
fill_pixels &lt;-<span class="st"> </span><span class="cf">function</span>(x) {<span class="kw">intToBits</span>(x)[<span class="dv">1</span>] <span class="op">==</span><span class="st"> </span>T}</code></pre></div>
<p>Next, use indexing and Boolean expressions to define functions which return TRUE for</p>
<ol style="list-style-type: lower-alpha">
<li>high confidence clouds or high confidence cloud shadows or fill values</li>
<li>high and medium confidence clouds or high and medium confidence cloud shadows or fill values</li>
</ol>
<p>Create a mask using the above functions. You can use <code>calc()</code>. Plot the mask and check if clouds, cloud shadows, and fill values are labeled as 1 and clear observations as 0. Write both masks to disk.</p>
<p>Open both masks in QGIS, together with an RGB representation of the image. Which mask is more accurate?</p>
</div>
<div id="masking-images" class="section level3">
<h3>3) Masking images</h3>
<p>Next, load the BOA data as a stack (VIS, nIR, swIR) and use the <code>mask()</code> function to mask clouds, cloud shadows and fill values from the image. Use the mask which you found to be more accurate. Make sure to specify the maskvalue argument accordingly. Write the masked BOA stack to disk in the <code>GTiff</code> format.</p>
</div>
</div>
<div id="reading-materials-1" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following paper:</p>
<p><a href="https://doi.org/10.1016/j.rse.2013.04.022">Griffiths, P. et al. (2014). Forest disturbances, forest recovery, and changes in forest types across the Carpathian ecoregion from 1985 to 2010 based on Landsat image composites. Remote Sensing of Environment, 151, 72–88.</a></p>
<p>In this paper, a long time series of Landsat image composites at five year intervals is used to study the dynamics of forest disturbance, recovery and changes in forest types across the Carpathian ecoregion. Please make sure to read the paper thoroughly and focus on the following broad questions:</p>
<ul>
<li>What is the motivation for this article?</li>
<li>How was it done in principle?</li>
<li>What are the key findings?</li>
<li>Are there uncertainties related to the findings?</li>
</ul>
<hr />
<!-- ################################## SESSION 03 ############################################## -->
</div>
</div>
<div id="session-03-vegetation-indices-and-data-transforms" class="section level1">
<h1>Session 03: Vegetation indices and data transforms</h1>
<div id="learning-goals-3" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Learn how to calculate NDVI, EVI and Tasseled Cap components</li>
</ul>
</div>
<div id="spectral-behavior-of-vegetation" class="section level2">
<h2>Spectral behavior of vegetation</h2>
<p>Vegetation produces a distinct spectral reflectance pattern due to its leaf and cell structure, its physiognomy, and complex stand structure. Photosynthetically inactive plant parts differ considerably from active ones across different wavelength regions. The reflectance of photosynthetically active vegetation is characterized by different factors in the VIS, nIR and SWIR:</p>
<ul>
<li><p>VIS – leaf pigments - In the visible bands the reflectance is relatively low as the majority of light is absorbed by the leaf pigments. Chlorophyll strongly absorbs energy in the blue and red wavelengths and reflects more in the green parts of the spectrum. This is why healthy vegetation appears green to the human eye.</p></li>
<li><p>nIR – cell structure - For healthy vegetation, the reflectance is much higher in the near infrared (NIR) region than in the visible region due to the cellular structure of the leaves, specifically the spongy mesophyll. Therefore healthy vegetation can be easily identified by the high NIR reflectance and generally low visible reflectance.</p></li>
<li><p>SWIR – water content - The reflectance in the shortwave infrared wavelengths is related to the water content of the vegetation and its structure. Water has strong absorption around 1.45, 1.95 and 2.50 µm . Outside these absorption bands in the SWIR region, reflectance of leaves generally increases when water content in the leaf decreases.</p></li>
</ul>
<div class="figure">
<img src="fig/s03_vegetation_spectrum.png" alt="Spectral reflectance curve of vegetation. Source: gsp.humboldt.edu" style="width:80.0%" />
<p class="caption">Spectral reflectance curve of vegetation. Source: gsp.humboldt.edu</p>
</div>
</div>
<div id="vegetation-indices" class="section level2">
<h2>Vegetation indices</h2>
<p>Vegetation indices make use of this particular reflectance signal. Most commonly known are the Normalized Difference Vegetation Index (NDVI) and the Enhanced Vegetation Index (EVI).</p>
<hr />
<div id="normalized-difference-vegetation-index-ndvi" class="section level3">
<h3>Normalized Difference Vegetation Index (NDVI)</h3>
<p>The NDVI relates the difference between the nIR and red reflectance to their sum.</p>
<p><span class="math inline">\(NDVI = (nIR – red) / (nIR + red)\)</span></p>
<p>For instance, a red reflectance of 10% and a nIR reflectance of 50% result in</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">red &lt;-<span class="st"> </span><span class="fl">0.1</span>
nIR &lt;-<span class="st"> </span><span class="fl">0.5</span>

ndvi &lt;-<span class="st"> </span>(nIR <span class="op">-</span><span class="st"> </span>red) <span class="op">/</span><span class="st"> </span>(nIR <span class="op">+</span><span class="st"> </span>red)
<span class="kw">print</span>(ndvi)</code></pre></div>
<pre><code>[1] 0.6666667</code></pre>
<p>The NDVI is not a physical measure, but a proxy integrating different factors, such as land use / cover, incl. the amount of background signal visible in a pixel, photosynthetic activity, vitality and overall vegetation condition. It relates well to vegetation density and structure, e.g., represented by the leaf area index (LAI)</p>
<hr />
</div>
<div id="enhanced-vegetation-index-evi" class="section level3">
<h3>Enhanced Vegetation Index (EVI)</h3>
<p>The EVI often has a better correlation with biomass than NDVI, specifically in vegetation canopies with low and high LAI values.</p>
<div class="figure">
<img src="fig/s03_ndvi_evi.PNG" alt="NDVI and EVI from MODIS image composites (5-20 March 2000). Source: http://earthobservatory.nasa.gov/" style="width:100.0%" />
<p class="caption">NDVI and EVI from MODIS image composites (5-20 March 2000). Source: <a href="http://earthobservatory.nasa.gov/" class="uri">http://earthobservatory.nasa.gov/</a></p>
</div>
<p><span class="math inline">\(EVI = G * ((nIR – red) / (nIR + (C1 * red – C2 * blue) + L))\)</span></p>
<p>The EVI aims at reducing saturation effects which are common for NDVI. It includes a correction for soil background effects (L) to improve sensitivity for low density vegetation canopies. It is less sensitive to high aerosol loads, since the additional coefficients (C1 and C2) steer the aerosol resistance term, and the visible blue reflectance is used to correct for scattering that also affects the visible red.</p>
<p>Indices enhance differences in the reflectance to highlight certain features. Vegetation indices have the advantage of being simple, but the disadvantage of disregarding parts of the spectral feature space. Linear transformations, such as the Tasseled Cap Transformation, can help to overcome this limitation.</p>
<hr />
</div>
</div>
<div id="tasseled-cap-transformation" class="section level2">
<h2>Tasseled Cap Transformation</h2>
<p>The Tasseled Cap Transformation (TC) is a linear transformation of the Landsat spectral bands. It was first presented in 1976 by R.J. Kauth and G.S. Thomas of Environmental Research Institute of Michigan in an article titled “The Tasseled Cap – A Graphic Description of the Spectral-Temporal Development of Agricultural Crops as Seen by Landsat.” The TC was thus developed for analyzing agricultural lands with Landsat MSS data. The name „Tasseled Cap“ was chosen because the of the shape of phenological trajectories of crops in the nIR ~ red feature space.</p>
<div class="figure">
<img src="fig/s03_tc_concept.png" alt="Crop phenological trajectories in near infrared ~ red featurespace. Band numbers relate to Landsat MSS bands, where band 3 (6) is the near infrared and band 2 (5) is the red band." style="width:40.0%" />
<p class="caption">Crop phenological trajectories in near infrared ~ red featurespace. Band numbers relate to Landsat MSS bands, where band 3 (6) is the near infrared and band 2 (5) is the red band.</p>
</div>
<p>We can observe these trajectories by producing near infrared ~ red scatterplots for different points in time.</p>
<div class="figure">
<img src="fig/s03_sct_cln.gif" alt="Animated scatterplot of 10,000 locations of an agricultural system in southeastern Turkey in near infrared ~ red featurespace, observed within the course of one year (2015)." style="width:50.0%" />
<p class="caption">Animated scatterplot of 10,000 locations of an agricultural system in southeastern Turkey in near infrared ~ red featurespace, observed within the course of one year (2015).</p>
</div>
<p>The concept was further developed for the use with other sensors, including Landsat TM, ETM+ and OLI. It is still widely used for different applications in the context of urban, agricultural, or forest-related remote-sensing studies. Resulting from the TC, we mostly analyze three components called Brightness, Greenness, and Wetness:</p>
<ul>
<li><em>Brightness</em>: an axis along the line of soils, indicating soil brightness.</li>
<li><em>Greenness</em>: axis is perpendicular to the soil line, emphasizes near infrared and hence vegetation.</li>
<li><em>Wetness</em>: emphasizes shortwave infrared and is thereby related to water content.</li>
</ul>
<hr />
</div>
<div id="exercise-3" class="section level2">
<h2>Exercise</h2>
<p>We provide the following datasets in <a href="https://box.hu-berlin.de/f/7bb994e5cb414bf49940/?dl=1">our repository</a>:</p>
<p>…sr_data/: Four cloud-masked image chips from Landsat 8 (surface reflectance):</p>
<ul>
<li>LC081890252014031001T1-SC20170927101754 (10 March 2014)</li>
<li>LC081890252014071601T1-SC20171024094741 (16 July 2014)</li>
<li>LC081890252015082001T1-SC20170927120710 (20 August 2015)</li>
<li>LC081890252014110501T1-SC20170927102137 (05 November 2014)</li>
</ul>
<div id="compute-vegetation-indices" class="section level3">
<h3>1) Compute vegetation indices</h3>
<p>Read the March surface reflectance stack in R and calculate NDVI as well as EVI. The correction factors for calculating EVI from Landsat data are:</p>
<ul>
<li><p><span class="math inline">\(G = 2.5\)</span></p></li>
<li><p><span class="math inline">\(C1 = 6\)</span></p></li>
<li><p><span class="math inline">\(C2 = 7.5\)</span>, and</p></li>
<li><p><span class="math inline">\(L = 1\)</span>.</p></li>
</ul>
<p>Always be aware that the reflectance values in our datasets is scaled by 10,000. This will not make a difference when computing the NDVI, as we are only looking at relative differences. However, for the EVI, <span class="math inline">\(L\)</span> must be scaled by 10,000. Also, keep in mind that we commonly want to store datasets in <code>INT2S</code> data type, and therefore we need to scale our results by 10,000. Consequently, we compute the EVI from Landsat data as follows:</p>
<p><code>evi &lt;- 2.5 * ((nIR – red) / (nIR + 6 * red – 7.5 * blue + 10000))</code></p>
<p>While the computation is ongoing, proceed with preparing the code for the next task. When the computation is done, write the results to disk, using the <code>INT2S</code> data type.</p>
</div>
<div id="perform-a-tasseled-cap-transformation" class="section level3">
<h3>2) Perform a Tasseled Cap transformation</h3>
<p>Perform the TC using the March surface reflectance stack. We use the coefficients derived by <a href="https://www.sciencedirect.com/science/article/pii/0034425785901026">Crist (1985)</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tcc &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>( <span class="fl">0.2043</span>,  <span class="fl">0.4158</span>,  <span class="fl">0.5524</span>, <span class="fl">0.5741</span>,  <span class="fl">0.3124</span>,  <span class="fl">0.2303</span>, 
                <span class="op">-</span><span class="fl">0.1603</span>, <span class="op">-</span><span class="fl">0.2819</span>, <span class="op">-</span><span class="fl">0.4934</span>, <span class="fl">0.7940</span>, <span class="op">-</span><span class="fl">0.0002</span>, <span class="op">-</span><span class="fl">0.1446</span>,
                 <span class="fl">0.0315</span>,  <span class="fl">0.2021</span>,  <span class="fl">0.3102</span>, <span class="fl">0.1594</span>, <span class="op">-</span><span class="fl">0.6806</span>, <span class="op">-</span><span class="fl">0.6109</span>), 
                <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;green&#39;</span>, <span class="st">&#39;red&#39;</span>, <span class="st">&#39;nIR&#39;</span>, <span class="st">&#39;swIR1&#39;</span>, <span class="st">&#39;swIR2&#39;</span>), <span class="kw">c</span>(<span class="st">&#39;brightness&#39;</span>, <span class="st">&#39;greenness&#39;</span>, <span class="st">&#39;wetness&#39;</span>)),
                <span class="dt">ncol =</span> <span class="dv">3</span>)

<span class="kw">print</span>(tcc)</code></pre></div>
<pre><code>      brightness greenness wetness
blue      0.2043   -0.1603  0.0315
green     0.4158   -0.2819  0.2021
red       0.5524   -0.4934  0.3102
nIR       0.5741    0.7940  0.1594
swIR1     0.3124   -0.0002 -0.6806
swIR2     0.2303   -0.1446 -0.6109</code></pre>
<p>The TC is a linear band transformation. We can simply multiply the individual bands in the march stack with the corresponding factor and sum up the result, e.g., for TC Brightness:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">brightness &lt;-<span class="st"> </span>march.stack[[<span class="dv">1</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">2</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">3</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">3</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">4</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">4</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">5</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">5</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">6</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">6</span>,<span class="dv">1</span>]</code></pre></div>
<p>Make sure to create an individual layer for Brightness, Greenness, and Wetness. Create a stack from the three layers. While the computation is ongoing, proceed with task 1) of the block on training data collection.</p>
<p>When the computation is completed, write the results to disk, using the <code>INT2S</code> data type. Add the NDVI, EVI, and the TC stack to the QGIS project you started in the next block. Visually explore the vegetation indices and TC components.</p>
<hr />
</div>
</div>
</div>
<div id="session-03-training-data-collection" class="section level1">
<h1>Session 03: Training data collection</h1>
<div id="learning-goals-4" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Gather training data for a broad forest type classification</li>
<li>Understand how forest types differ spectrally</li>
</ul>
<hr />
</div>
<div id="background" class="section level2">
<h2>Background</h2>
<p>Collecting training is an essential step on your way to a classified map. The training pixels will be considered representative for the classes you want to map, as classification algorithms determine class labels for unknown pixels based on their similarity to the training dataset.</p>
<div class="figure">
<img src="fig/s03_training.PNG" alt="Training points for forest type classification and resulting map output" style="width:80.0%" />
<p class="caption">Training points for forest type classification and resulting map output</p>
</div>
<p>Collecting training data is time consuming, regardless if you are collecting in the field or digitally. Small conceptual mistakes may require a revision of your training dataset. As a consequence, training data collection should be well prepared. Consider the following points.</p>
<ul>
<li><p>A precise and robust definition of your target classes based on the study region characteristics is key. Targeting a high thematic detail is beneficial, but spectral(-temporal) similarities between classes might pose limitations to a robust distinction of classes, such as tree species or crop types. In such cases, it is advised to think about a hierarchical structure to aggregate similar classes into higher level classes, such as forest types, or annual / perennial croplands.</p></li>
<li><p>Gathering as much reference information as possible. Can we find additional datasets that guide our interpretation? Is any very high resolution (VHR) imagery available? GoogleEarth is a valuable source of VHR imagery, but it is critical to account for the exact acquisition date.</p></li>
<li><p>Good knowledge of the target classes, and their spectral (temporal) characteristics in the study region is beneficial. We should consider spectrally similar classes and identify potential ways to prevent confusion, e.g., by aggregating those classes or identifying spectral features which help to separate them better.</p></li>
<li><p>A purely random point sampling is not neccessarily the best option (different from collecting independent validation data), as we might want to train small classes that are hardly captured by a random sample. Manual selection of training points is advised to circumvent this problem.</p></li>
<li><p>The image below shows the spatial distribution of six training datasets collected during an earlier iteration of the course. Some training points cluster in a subset of the study region. Ideally, however, training data should be well distributed across the study region to cover regional biophysical variability, such as different soil types, weather patterns, or topography.</p></li>
</ul>
<div class="figure">
<img src="fig/s03_train_dist.PNG" alt="Spatial distribution of six training datasets" />
<p class="caption">Spatial distribution of six training datasets</p>
</div>
<ul>
<li><p>The classification algorithm of your choice might have specific requirements towards the training data, e.g., concerning the number of samples, their distribution in the spectral feature space, or their purity (pure vs. mixed pixel). We discuss these aspects later in the course.</p></li>
<li><p>In practice, it´s important to know your training data well. Are the classes separable with the data at hand? Are essential class characteristics well represented? Are there any outliers? To learn more, it is always wise to explore the spectral characteristics of your training data points. We can do this through investigating the spectral reflectances at our training data locations (e.g., through histograms / boxplots) and comparing them between classes. That´s what we want to do today.</p></li>
</ul>
<hr />
</div>
<div id="exercise-4" class="section level2">
<h2>Exercise</h2>
<p>This exercise has two larger aims. First, you will learn to collect training data for a broad forest type classification. We provide forestry data to find representative sample pixels in QGIS. We will use the data you generate in this exercise for classification in the next session. Second, you will learn how the broad forest types appear spectrally in images acquired in different parts of the growing season.</p>
<p>We provide the following datasets in <a href="https://box.hu-berlin.de/f/7bb994e5cb414bf49940/?dl=1">our repository</a>:</p>
<p>…sr_data/: Four cloud-masked image chips from Landsat 8 (surface reflectance):</p>
<ul>
<li>LC081890252014031001T1-SC20170927101754 (10 March 2014)</li>
<li>LC081890252014071601T1-SC20171024094741 (16 July 2014)</li>
<li>LC081890252015082001T1-SC20170927120710 (20 August 2015)</li>
<li>LC081890252014110501T1-SC20170927102137 (05 November 2014)</li>
</ul>
<p>…vector/: A shapefile and a *.kmz file for GoogleEarth, which will help you to accurately delineate the Landsat pixel locations and extents for training data collection.</p>
<p>…BDL/: Forestry data collected in 2015 which is publicly available <a href="https://www.bdl.lasy.gov.pl/portal/wniosek-en">here</a>. We prepared a shapefile for the use in this session.</p>
<p>It contains the following attributes:</p>
<table>
<thead>
<tr class="header">
<th>Attribute field</th>
<th>Definition</th>
<th>Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>species_en</td>
<td>Dominant genus in each stand</td>
<td>Ash, Beech, Fir, Spruce…</td>
</tr>
<tr class="even">
<td>part_cd</td>
<td>Share of this genus within the stand</td>
<td>0 – 100 (in %)</td>
</tr>
<tr class="odd">
<td>spec_age</td>
<td>Average age of the trees in this stand</td>
<td>Age in years</td>
</tr>
</tbody>
</table>
<div id="prepare-the-training-data-collection" class="section level3">
<h3>1) Prepare the training data collection</h3>
<p>Visualize and arrange all abovementioned datasets in QGIS. Ask us for help if you´re not familiar with QGIS. Consider the following steps:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Find a good <a href="https://www.harrisgeospatial.com/Learn/Blogs/Blog-Details/ArtMID/10198/ArticleID/15691/The-Many-Band-Combinations-of-Landsat-8">false-color representation</a> of the Landsat 8 bands to highlight vegetation.</p></li>
<li><p>Visualize the forestry data by choosing distinct colors for the different tree genera (species_en).</p></li>
</ol>
<p>Which genera are dominant in the study area?</p>
<p>Generate a new point shapefile for storing the training data you will collect in the next task. It should contain the attribute fields ‘classID’ and ‘confID’ (both of type integer).</p>
<p>Watch out - make sure the shapefile has the same spatial reference system as the Landsat data.</p>
</div>
<div id="collect-training-data" class="section level3">
<h3>2) Collect training data</h3>
<p>Switch into the editing mode to locate training points and assign the corresponding class and confidence number. Please collect at least 15 pixels per class and assign them the class numbers and confidence numbers given below.</p>
<table>
<thead>
<tr class="header">
<th>Class name</th>
<th>classID</th>
<th>Confidence level</th>
<th>confID</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deciduous forest</td>
<td>1</td>
<td>Very certain</td>
<td>1</td>
</tr>
<tr class="even">
<td>Mixed forest</td>
<td>2</td>
<td>Some uncertainties</td>
<td>2</td>
</tr>
<tr class="odd">
<td>Coniferous forest</td>
<td>3</td>
<td>Very uncertain</td>
<td>3</td>
</tr>
<tr class="even">
<td>Non-forest</td>
<td>4</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Use the multi-temporal Landsat imagery, the forestry polygons and very high resolution imagery in GoogleEarth to identify training points. The historic imagery tool in Google Earth can be extremely useful to guide your interpretation between deciduous and evergreen trees, as it contains imagery from the leaf-off phenological phase. The Landsat grid shapefile and .kmz will help you to identify and label the precise training locations for the four classes.</p>
<p>Regularly save the collected points and store the final shapefile with 60+ points in your course folder.</p>
</div>
<div id="explore-your-training-data" class="section level3">
<h3>3) Explore your training data</h3>
<p>Load your shape in R using <code>readOGR()</code>. Extract the spectral values at your point locations from the March image in R using the <code>extract()</code> function. Specify <code>sp = TRUE</code> to append the spectral values to the point shapefile.</p>
<p>Make sure the result of this task is an object of type <code>data.frame</code> named <code>sr.march</code>. Your sample points should be represented as rows and the measured variables as columns (i.e. point id, classID, confID, and 6 spectral bands).</p>
<p>Create boxplots of your surface reflectance measurements for all spectral bands, grouped according to the class number.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load required packages</span>
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(reshape2)

<span class="co"># Melt dataframe containing point id, classID, confID, and 6 spectral bands</span>
spectra.df &lt;-<span class="st"> </span><span class="kw">melt</span>(sr.march, <span class="dt">id.vars=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>), <span class="dt">measure.vars=</span><span class="kw">c</span>(<span class="dv">4</span><span class="op">:</span><span class="dv">9</span>))

<span class="co"># Create boxplots of spectral bands per class</span>
<span class="kw">ggplot</span>(spectra.df, <span class="kw">aes</span>(<span class="dt">x=</span>variable, <span class="dt">y=</span>value, <span class="dt">color=</span>classID)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() </code></pre></div>
<p>Make sure you understand what the <code>melt()</code> function is doing. Feel free to adjust the plot layout.</p>
<p>Similarly to the previous task, extract the values at your point locations from the Tasseled Cap stack. Create boxplots of the three Tasseled Cap components, grouped by the target class. Investigate the boxplots in order to investigate the differences between your target classes. Try to answer the following questions:</p>
<ul>
<li>Do the Tasseled Cap components allow for discriminating your target classes?</li>
<li>Which classes are likely difficult to separate?</li>
</ul>
<p><em>Voluntary exercise</em>: If you´re keen on exploring spectral changes over time, repeat the above procedures for the remaining images (July, August, November).</p>
<hr />
</div>
</div>
<div id="reading-materials-2" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following paper:</p>
<p><a href="https://www.researchgate.net/publication/259486369_A_Pixel-Based_Landsat_Compositing_Algorithm_for_Large_Area_Land_Cover_Mapping">Griffiths et al. (2013): Pixel-Based Landsat Compositing for Large Area Land Cover Mapping. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 6(5), 2088-2101.</a></p>
<p>This paper describes the methods used in the article you read last week. Here, a novel algorithm for large-area pixel-based compositing from Landsat data was developed. Please read the paper thoroughly, make sure you understand the underlying concept and write down any question for the discussion in our next session. Also, answer the following broad questions:</p>
<ul>
<li>What is the motivation for this article?</li>
<li>How does the algorithm work in principle?</li>
<li>Which key parameters were used for the parametric scoring?</li>
<li>What is the difference between annual and seasonal consistency?</li>
</ul>
<hr />
<!-- ################################## SESSION 04 ############################################## -->
</div>
</div>
<div id="session-04-pixel-based-compositing" class="section level1">
<h1>Session 04: Pixel-based compositing</h1>
<hr />
<div id="learning-goals-5" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Understand the fundamentals of pixel-based image compositing</li>
<li>Parameterize a compositing function and create your own composite from multiple Landsat images</li>
</ul>
<hr />
</div>
<div id="background-1" class="section level2">
<h2>Background:</h2>
<p>The availability of Landsat images varies by region. Even in areas where obersvation density is high, such as North America, coverage can be reduced by clouds and quality of observations can be impacted by haze or sensor saturation. <img src="fig/s04_landsat_availability.jpg" alt="Global availability of Landsat images. Source: Wulder et al. 2016; https://doi.org/10.1016/j.rse.2015.11.032" /></p>
<p>Using pixel-based-compositing, we can integrate multiple obervations (i.e., from multiple Landsat images) and combine them to one consistent image composite. One of the first global Landsat composites was produced using the WELD compositing approach (Roy et al. 2011, <a href="https://doi.org/10.1016/j.rse.2009.08.011" class="uri">https://doi.org/10.1016/j.rse.2009.08.011</a>). <img src="fig/s04_global_weld.jpg" alt="Monthly global WELD surface reflectance composite for June 2009. Source: https://worldview.earthdata.nasa.gov/" /></p>
<p>Pixel-based compositing allows us to create cloud-free, radiometrically and phenologically consistent image composites that are contiguous over large areas. A set of parameters is used to determine the observations best suited for analysis. These parameters can, e.g., include the distance to clouds in the image, or the temporal proximity to a target day of the year. Defining scoring functions allows for a flexible parametrization according to user´s needs and study area characteristics. Different parameters can also be weighted according to their relevance for a given application.</p>
<div class="figure">
<img src="fig/s04_compositing.png" alt="Creating one pixel-based composite from three images." />
<p class="caption">Creating one pixel-based composite from three images.</p>
</div>
<hr />
</div>
<div id="best-pixel-based-compositing-in-seven-steps" class="section level2">
<h2>Best-pixel-based compositing in seven steps</h2>
<ol style="list-style-type: decimal">
<li><p>Determine compositing parameters, e.g. Target DOY: June 15 +/- 30 days and target year: 2015 +/- 1 year</p></li>
<li><p>Calculate DOY and year suitability (0-1) according to parameters</p></li>
</ol>
<div class="figure">
<img src="fig/s04_suitability_offsets.png" alt="Five images to be used for best-pixel compositing and their DOY and year offsets" />
<p class="caption">Five images to be used for best-pixel compositing and their DOY and year offsets</p>
</div>
<p>We use linear functions to determine the suitability of an observation. In the plot for the DOY function below, we’re chosing a parameterization that favors observations closest to the target DOY and assign a suitability score of 0 for observations that are 50 or more days from the target date. Being 30 days from the target DOY, the May 16 2015 observation receives a suitability score of 0.4.<br />
A second function determines the suitability score for the year of observation. Again, observations close to the target year are assigned high values, while observations acquired five or more years from the target year are considered unsuitable. The resulting suitability score for the May 15 2015 observation is 1.</p>
<div class="figure">
<img src="fig/s04_suitabilityplot_doy_year.png" alt="Linear functions for the DOY and year suitability score" style="width:90.0%" />
<p class="caption">Linear functions for the DOY and year suitability score</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>Calculate pixel-level suitability (0-1), e.g. through min. distance to clouds.</li>
</ol>
<div class="figure">
<img src="fig/s04_suitability_clouds.png" alt="Distance to next cloud in pixels" />
<p class="caption">Distance to next cloud in pixels</p>
</div>
<p>To minimize the likelihood of pixels being affected by clouds, we favor observations further away from clouds. The cloud distance scoring function below assigns a suitability score of 1 as soon as the distance to the next cloud is equal or greater than 100 pixels. An observation that has a distance of 60 pixels (=1800m) to the next cloud receives a cloud distance suitability score of 0.6.</p>
<div class="figure">
<img src="fig/s04_suitabilityplot_clouds.png" alt="Linear function for the cloud suitability score" style="width:40.0%" />
<p class="caption">Linear function for the cloud suitability score</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>Define which criteria are most important for you (W = weights):</li>
</ol>
<p>We can weigh the different parameters according to our needs. For instance, by prioritizing the DOY score over the year score.</p>
<p><span class="math display">\[W_{DOY} = 0.5\]</span> <span class="math display">\[W_{year} = 0.2\]</span> <span class="math display">\[W_{CloudDist} = 0.3\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li>Calculate a score for each pixel in each image. We can use the weighted sum of suitabilities (S = score):</li>
</ol>
<p><span class="math display">\[score = S_{DOY} * W_{DOY} + S_{Year} * W_{Year} + S_{CloudDist} * W_{CloudDist}\]</span></p>
<p>Using the values from the example above yields:<br />
<span class="math display">\[score = 0.4 * 0.5 + 1 * 0.2 + 0.6 * 0.3\]</span> <span class="math display">\[score = 0.58\]</span></p>
<div class="figure">
<img src="fig/s04_suitability_score_weighted.png" alt="Resulting suitability scores for each image" />
<p class="caption">Resulting suitability scores for each image</p>
</div>
<ol start="6" style="list-style-type: decimal">
<li><p>Use the best observation (= highest score) for each pixel to create the final composite.</p></li>
<li><p>Evaluate the results by checking for seasonal/annual consistency and cloud distance (and re-iterate) <img src="fig/s04_final_composite.jpg" /></p></li>
</ol>
<hr />
</div>
<div id="exercise-5" class="section level2">
<h2>Exercise</h2>
<p>The goal of this week’s exercise is to combine several images into best-pixel composites using a parametric compositing function. We provide the data for this exercise in <a href="https://box.hu-berlin.de/f/17f0d06cdc164564a019/?dl=1">our repository</a>. After unpacking, you will find the following folders:</p>
<ul>
<li>sr_data: a total of 43 cloud-masked bottom of atmosphere image chips for our study region, acquired in various seasons, years, and with different degrees of cloud cover.</li>
<li>cloud_dist: data on the distance (in pixels) to the closest cloud for each of the BOA images.</li>
<li>fmask: the cloud masks used to mask the BOA images and to derive the cloud distance layers.</li>
</ul>
<p>Today we will be working with pre-defined scripts. We provide both a script containing the compositing function and a main script which sources and executes the compositing function. Outsourcing functions - especially extensive ones - improves the readability of scripts and allows for using functions in different scripts. Check the help for source() if you are not familiar with sourcing R code in your script. You find both codes below. Please copy them into an empty R script and save to disk.</p>
<p><details> <summary>Click here to see code for the parametric compositing function</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">#############################################################################
<span class="co"># MSc Earth Observation Exercise 4</span>
<span class="co"># Function for creating cloud-free composites of multiple Landsat images</span>
<span class="co"># Requires an input data.frame (here img_list) and eight compositing </span>
<span class="co"># parameters. Please see exercise sheet for further details.</span>
#############################################################################

#############################################################################
<span class="co"># Loading required packages here...</span>
<span class="kw">library</span>(raster)
<span class="kw">library</span>(lubridate)

<span class="co"># Change raster options to store large rasters in temp files on disk</span>
<span class="kw">rasterOptions</span>(<span class="dt">maxmemory =</span> <span class="fl">1e6</span>)

#############################################################################
<span class="co"># Function definition starts here</span>
parametric_compositing &lt;-<span class="st"> </span><span class="cf">function</span>(img_list, target_date, 
                                   W_DOY, W_year, W_cloud_dist, 
                                   max_DOY_offset, max_year_offset, 
                                   min_cloud_dist, max_cloud_dist) {
  <span class="co">#...</span>
  tic &lt;-<span class="st"> </span><span class="kw">Sys.time</span>()
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Start of compositing process: &#39;</span>, tic))
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Target date: &#39;</span>, target_date))

  <span class="co"># Extract target DOY and year from target_date</span>
  target_DOY &lt;-<span class="st"> </span><span class="kw">yday</span>(target_date)
  target_year &lt;-<span class="st"> </span><span class="kw">year</span>(target_date)
  
  <span class="co">#...</span>
  <span class="cf">if</span>(<span class="kw">sum</span>(W_DOY, W_year, W_cloud_dist)<span class="op">!=</span><span class="dv">1</span>) { <span class="kw">stop</span>(<span class="st">&#39;Error: something wrong.&#39;</span>) }
  
  #############################################################################
  <span class="co"># Calculate the scores for the DOY, year, and cloud distance criteria</span>
  <span class="kw">print</span>(<span class="st">&#39;Calculating compositing scores&#39;</span>)
  
  <span class="co">#...</span>
  obs_DOY &lt;-<span class="st"> </span>img_list<span class="op">$</span>DOY
  DOY_score &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="kw">abs</span>(target_DOY <span class="op">-</span><span class="st"> </span>img_list<span class="op">$</span>DOY) <span class="op">/</span><span class="st"> </span>max_DOY_offset)
  DOY_score[DOY_score<span class="op">&lt;</span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="ot">NA</span>
  
  <span class="co">#...</span>
  obs_year &lt;-<span class="st"> </span>img_list<span class="op">$</span>year
  year_score &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">-</span>(<span class="kw">abs</span>(target_year <span class="op">-</span><span class="st"> </span>obs_year) <span class="op">/</span><span class="st"> </span>max_year_offset)
  
  <span class="cf">if</span> (max_year_offset <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {year_score[obs_year<span class="op">==</span>target_year] &lt;-<span class="st"> </span><span class="dv">1</span>}
  year_score[year_score<span class="op">&lt;</span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="ot">NA</span>
  
  <span class="co"># Get candidate images within max_DOY_offset and max_year_offset</span>
  ix &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="op">!</span><span class="kw">is.na</span>(DOY_score) <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(year_score))
  <span class="cf">if</span> (<span class="kw">length</span>(ix)<span class="op">&gt;</span><span class="dv">1</span>) { <span class="kw">print</span>(<span class="kw">paste</span>(<span class="kw">length</span>(ix),  <span class="st">&#39;candidate images selected, calculating scores.&#39;</span>)) }
  <span class="cf">if</span> (<span class="kw">length</span>(ix)<span class="op">&lt;</span><span class="dv">2</span>) { <span class="kw">stop</span>(<span class="st">&#39;Another error because something is wrong.&#39;</span>) }
  
  <span class="co"># Stack cloud distance layers of candidate images and reclassify </span>
  <span class="co"># values &lt; min_cloud_dist to NA, and values &gt; max_cloud_dist to max_cloud_dist</span>
  cloud_dist &lt;-<span class="st"> </span><span class="kw">stack</span>(<span class="kw">as.character</span>(img_list<span class="op">$</span>cloud_dist_files[ix]))
  cloud_dist &lt;-<span class="st"> </span><span class="kw">reclassify</span>(cloud_dist, <span class="dt">rcl=</span><span class="kw">c</span>(<span class="dv">0</span>, min_cloud_dist, <span class="ot">NA</span>), <span class="dt">right=</span><span class="ot">NA</span>, <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
  cloud_dist &lt;-<span class="st"> </span><span class="kw">reclassify</span>(cloud_dist, <span class="dt">rcl=</span><span class="kw">c</span>(max_cloud_dist, <span class="kw">sqrt</span>(<span class="kw">nrow</span>(cloud_dist)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">ncol</span>(cloud_dist)<span class="op">^</span><span class="dv">2</span>), max_cloud_dist), <span class="dt">right=</span><span class="ot">NA</span>, <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
  
  <span class="co">#...</span>
  cloud_score &lt;-<span class="st"> </span>(cloud_dist <span class="op">-</span><span class="st"> </span>min_cloud_dist) <span class="op">/</span><span class="st"> </span>(max_cloud_dist <span class="op">-</span><span class="st"> </span>min_cloud_dist)
  
  <span class="co">#...</span>
  obs_score &lt;-<span class="st"> </span>DOY_score[ix] <span class="op">*</span><span class="st"> </span>W_DOY <span class="op">+</span><span class="st"> </span>year_score[ix] <span class="op">*</span><span class="st"> </span>W_year <span class="op">+</span><span class="st"> </span>cloud_score <span class="op">*</span><span class="st"> </span>W_cloud_dist
  
  <span class="co">#...</span>
  select &lt;-<span class="st"> </span><span class="kw">which.max</span>(obs_score)
  
  <span class="co">#...</span>
  candidates &lt;-<span class="st"> </span><span class="kw">unique</span>(select)
  
  #############################################################################
  <span class="co"># Fill composite image with pixels from the candidate images</span>
  <span class="cf">for</span> (i <span class="cf">in</span> candidates){
    
    <span class="co">#...</span>
    fill_image &lt;-<span class="st"> </span><span class="kw">brick</span>(<span class="kw">as.character</span>(img_list<span class="op">$</span>image_files[ix[i]]), <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
    
    <span class="co">#...</span>
    <span class="cf">if</span> (i <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(candidates)) { 
      composite &lt;-<span class="st"> </span><span class="kw">brick</span>(fill_image, <span class="dt">values=</span><span class="ot">FALSE</span>) 
      <span class="kw">dataType</span>(composite) &lt;-<span class="st"> &#39;INT2S&#39;</span>
      <span class="kw">values</span>(composite) &lt;-<span class="st"> </span><span class="dv">0</span>
    }
    
    <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&#39;Filling raster with acquisition from date &#39;</span>, img_list<span class="op">$</span>date[ix[i]]))
    fill_image.masked &lt;-<span class="st"> </span><span class="kw">mask</span>(fill_image, select, <span class="dt">maskvalue=</span>i, <span class="dt">inverse=</span>T, <span class="dt">updatevalue=</span><span class="dv">0</span>, <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
    fill_image.masked[<span class="kw">is.na</span>(fill_image.masked)] &lt;-<span class="st"> </span><span class="dv">0</span>
    composite &lt;-<span class="st"> </span>composite <span class="op">+</span><span class="st"> </span>fill_image.masked
    
  }
  
  <span class="co">#...</span>
  composite_na &lt;-<span class="st"> </span><span class="kw">mask</span>(composite, select, <span class="dt">maskvalue=</span><span class="ot">NA</span>, <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
  
  #############################################################################
  <span class="co">#...</span>
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;NAs: &#39;</span>, <span class="kw">round</span>(<span class="kw">freq</span>(composite[[<span class="dv">1</span>]], <span class="dt">value=</span><span class="ot">NA</span>)<span class="op">/</span><span class="kw">ncell</span>(composite[[<span class="dv">1</span>]])<span class="op">*</span><span class="dv">100</span>, <span class="dt">digits=</span><span class="dv">3</span>), <span class="st">&#39; %&#39;</span>))
  
  <span class="co">#...</span>
  rcl_DOY &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">data=</span><span class="kw">c</span>(candidates, obs_DOY[ix[candidates]]))
  select_DOY &lt;-<span class="st"> </span><span class="kw">reclassify</span>(select, rcl_DOY, <span class="dt">datatype =</span> <span class="st">&#39;INT2S&#39;</span>)
  
  <span class="co">#...</span>
  rcl_year &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">data=</span><span class="kw">c</span>(candidates, obs_year[ix[candidates]]))
  select_year &lt;-<span class="st"> </span><span class="kw">reclassify</span>(select, rcl_year)

  <span class="co">#...</span>
  output &lt;-<span class="st"> </span><span class="kw">stack</span>(composite_na, select_DOY, select_year)
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;End of compositing process: &#39;</span>, <span class="kw">Sys.time</span>()))
  
  <span class="co">#...</span>
  <span class="kw">return</span>(output)
  
}</code></pre></div>
<p></details></p>
<p><details> <summary>Click here to see the code for the main script calling the compositing function</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  #############################################################################
<span class="co"># MSc Earth Observation Exercise 4</span>
<span class="co"># [Your Name]</span>
#############################################################################

#############################################################################
<span class="kw">library</span>(rgdal)
<span class="kw">library</span>(raster)
<span class="kw">library</span>(lubridate)
<span class="kw">library</span>(ggplot2)
<span class="kw">source</span>(<span class="st">&#39;&#39;</span>) <span class="co">#path to the parametric_compositing function</span>

<span class="co"># Change raster options to store large rasters in temp files on disk</span>
<span class="kw">rasterOptions</span>(<span class="dt">maxmemory =</span> <span class="fl">1e6</span>)

######## Define the folder that contains your data...
data.path &lt;-<span class="st"> &#39;O:/ST19_MSc-EO/S04/data/&#39;</span>

#############################################################################
<span class="co"># 1)</span>
#############################################################################

sr &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="kw">paste0</span>(data.path, <span class="st">&#39;/sr_data&#39;</span>), <span class="dt">pattern=</span><span class="st">&quot;.tif$&quot;</span>, <span class="dt">full.names=</span>T, <span class="dt">recursive=</span>F)
fmask &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="kw">paste0</span>(data.path, <span class="st">&#39;/fmask&#39;</span>), <span class="dt">pattern=</span><span class="st">&quot;.tif$&quot;</span>, <span class="dt">full.names=</span>T, <span class="dt">recursive=</span>F)
cd &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="kw">paste0</span>(data.path, <span class="st">&#39;/cloud_dist&#39;</span>), <span class="dt">pattern=</span><span class="st">&quot;.tif$&quot;</span>, <span class="dt">full.names=</span>T, <span class="dt">recursive=</span>F)

sta &lt;-<span class="st"> </span><span class="kw">nchar</span>(<span class="kw">paste0</span>(data.path,<span class="st">&#39;/sr_data/LT05228082&#39;</span>)) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
end &lt;-<span class="st"> </span>sta <span class="op">+</span><span class="st"> </span><span class="dv">6</span>

dates &lt;-<span class="st"> </span><span class="kw">as.Date</span>(<span class="kw">substr</span>(sr, sta, end), <span class="dt">format=</span><span class="st">&#39;%Y%j&#39;</span>)

sr.sorted &lt;-<span class="st"> </span>sr[<span class="kw">order</span>(dates)]
cd.sorted &lt;-<span class="st"> </span>cd[<span class="kw">order</span>(dates)]

img_list &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&quot;image_files&quot;</span>=<span class="kw">as.character</span>(sr.sorted), <span class="st">&quot;cloud_dist_files&quot;</span>=<span class="kw">as.character</span>(cd.sorted) ,<span class="st">&quot;date&quot;</span>=<span class="kw">sort</span>(dates), <span class="st">&quot;DOY&quot;</span>=<span class="kw">yday</span>(<span class="kw">sort</span>(dates)), <span class="st">&quot;year&quot;</span>=<span class="kw">year</span>(<span class="kw">sort</span>(dates)))


#############################################################################
<span class="co"># 2)</span>
#############################################################################
target_date_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">ymd</span>(<span class="st">&#39;YYYYMMDD&#39;</span>)
target_date_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">ymd</span>(<span class="st">&#39;YYYYMMDD&#39;</span>)

W_DOY &lt;-<span class="st"> </span><span class="fl">0.0</span>
W_year &lt;-<span class="st"> </span><span class="fl">0.0</span>
W_cloud_dist &lt;-<span class="st"> </span><span class="fl">0.0</span>
  
max_DOY_offset &lt;-<span class="st"> </span><span class="dv">0</span>
max_year_offset &lt;-<span class="st"> </span><span class="dv">0</span>

min_cloud_dist &lt;-<span class="st"> </span><span class="dv">0</span>
max_cloud_dist &lt;-<span class="st"> </span><span class="dv">0</span>

composite_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">parametric_compositing</span>(img_list, target_date_<span class="dv">1</span>, 
                                       W_DOY, W_year, W_cloud_dist, 
                                       max_DOY_offset, max_year_offset, 
                                       min_cloud_dist, max_cloud_dist)

#############################################################################
<span class="co"># 4)</span>
#############################################################################

composite_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">parametric_compositing</span>(img_list, target_date_<span class="dv">2</span>, 
                                      W_DOY, W_year, W_cloud_dist, 
                                      max_DOY_offset, max_year_offset, 
                                      min_cloud_dist, max_cloud_dist)</code></pre></div>
<p></details></p>
<div id="parameterization" class="section level3">
<h3>1) Parameterization</h3>
<p>The function above allows you to produce cloud-free best-pixel composites for a pre-defined target DOY. This function requires one input data.frame (see code template on how to create it) and eight input parameters:</p>
<ol style="list-style-type: lower-alpha">
<li><p><em>img_list</em>: a data frame containing five variables for each of the Landsat images:</p>
<p>$image_files: the full paths to the files in …sr_data.<br />
$cloud_dist_files: the full paths to the files in …cloud_dist.<br />
$date: the acquisition day in Date format (YYYY-MM-DD).<br />
$DOY: the acquisition day of the year.<br />
$year: the acquisition year.</p></li>
<li><p><em>target_date</em>: the target date for your composite in Date format (YYYY-MM-DD)</p></li>
<li><p><em>W_DOY</em>, <em>W_year</em>, <em>W_cloud_dist</em>: weights for the three available parameters DOY, year and distance to clouds. Must be scaled between 0 and 1 and sum up to 1, the higher the weight, the higher the importance of the criterion.</p></li>
<li><p><em>max_DOY_offset</em>, <em>max_year_offset</em>: Thresholds for the maximum allowed differences between target DOY and acquisition DOY, as well target year and acquisition year. Images exceeding these thresholds (further away in time) will be fully ignored. For instance, max_year_offset = 0 will not allow observations from a year other than the target year. By choosing these parameters, you will determine whether you prefer seasonal consistency (close to target DOY but from different years) over annual consistency (observations from same year but potentially distant DOYs). Discuss the parametrization in your group.</p></li>
<li><p><em>min_cloud_dist</em>, <em>max_cloud_dist</em>: The minimum and maximum distance to clouds. min_cloud_dist = 10 will exclude all observations which are less than 10 pixels away from a cloud. The cloud scores are linearly scaled between the minimum (score = 0) and maximum cloud distance (score = 1). Pixels with distances above max_cloud_dist will receive a score of 1.</p></li>
</ol>
</div>
<div id="defining-target-dates-and-parameters" class="section level3">
<h3>2) Defining target dates and parameters</h3>
<p>Relying on last week´s insights during training data collection, define two target days of the year (DOY) to capture contrasting phenological stages of the different forest types. These will be used as target_date parameters later on. Next, make a decision concerning the compositing parameters explained in 1c, 1d and 1e.</p>
</div>
<div id="exploring-the-script-and-adding-documentation" class="section level3">
<h3>3) Exploring the script and adding documentation</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Open the parametric_compositing.R script and take time to read through it in groups. Run the code line by line. Make sure you understand how the function operates. Discuss questions in your group and seek the help pages of functions you don´t know.</p></li>
<li><p>The developer did not spend sufficient time on the documentation. Make the script a bit more user-friendly by adding missing comments (#…). Make sure your comments explain what happens in each step of the function, and why. Are there bugs or sections which you would code differently?</p></li>
<li><p>Next, run the parametric_compositing() function with your parameters and write the result to disk. Include the target DOY in the filename. While the function executes, proceed with the next exercise.</p></li>
</ol>
</div>
<div id="second-run-for-target-date-2" class="section level3">
<h3>4) Second run for target date 2</h3>
<p>Repeat the compositing for the second target DOY you specified in 2) and write the results to disk.</p>
</div>
<div id="visual-inspection-and-evaluation-of-results" class="section level3">
<h3>5) Visual inspection and evaluation of results</h3>
<p>Visually inspect the quality of your compositing results in QGIS. Look at the bands containing the DOY and year flags (band 7 and 8). What worked out well, what did not? How could the quality of the composites be improved? Re-iterate with different parameters if you wish.</p>
</div>
<div id="improving-the-user-friendliness-of-the-script-optionalvoluntary-task" class="section level3">
<h3>6) Improving the user-friendliness of the script (optional/voluntary task)</h3>
<p>Make the compositing function more user friendly. Insert a couple of plot and print commands to enable the users to follow the progress of the compositing while the function is running.</p>
<p>For instance, print() how many images were used for the final composite, their acquisition dates, etc. Also, you might want to plot() the composited image after each iteration. You could add further status messages telling the user how much time single steps took.</p>
<p>Don´t forget to save the script and run the source() command in your R script to update the function after you made these changes.</p>
<hr />
</div>
</div>
<div id="reading-materials-3" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following paper:</p>
<p><a href="https://doi.org/10.1080/01431161.2018.1433343">Maxwell et al. (2018): Implementation of machine-learning classification in remote sensing: an applied review. International Journal of Remote Sensing 39(9), 2784-2817</a></p>
<p>In this review, Maxwell and colleagues outline the use of several machine learning classification algorithms for application in remote sensing. Please read the article and note questions and points to discuss. Feel free to omit the details of ANNs and boosted DTs. Please read the paper thoroughly, make sure you understand the underlying concept and write down question for the discussion in our next session. Also, answer the following broad questions:</p>
<ul>
<li>What is the motivation for this article?</li>
<li>What are the key differences between classification algorithms from a user perspective?</li>
<li>Which classifiers do you reckon to be most suitable for typical EO applications?</li>
</ul>
<hr />
<!-- ################################## SESSION 05 ############################################## -->
</div>
</div>
<div id="session-05-machine-learning-for-image-classification" class="section level1">
<h1>Session 05: Machine learning for image classification</h1>
<div id="learning-goals-6" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Understanding fundamentals of the Random Forest classification algorithm</li>
<li>Conduct sensitivity analyses to parametrize the classification model</li>
<li>Produce a forest type map for the Carpathian study region</li>
</ul>
<hr />
</div>
<div id="background-2" class="section level2">
<h2>Background</h2>
<p>This session widely focuses on the Random Forest (RF) algorithm <a href="https://doi.org/10.1023/A:1010933404324">(Breimann 2001)</a> and its implementation in R. Some background might be useful in this context. The basic principles of the RF algorithm can be easily understood when disentangling its name:</p>
<p><strong>“Forest”</strong>: The RF is an ensemble of self-learning decision trees, which - metaphorically speaking - shape a forest. The idea behind the ensemble is that many weak learners can come to one strong decision. Each decision tree consists of a large number of splits, which essentially represent simple binary (yes / no) decisions, e.g. “is reflectance in band 4 &gt; 0.65?”. The sequential binary branching creates a tree-like shape. The user defines the number of decision trees in the forest (e.g. via the <code>ntrees</code> parameter in the R implementation).</p>
<div class="figure">
<img src="fig/s05_dt.PNG" alt="Exemplary illustration of a two-dimensional feature space, divided by binary thresholds (left), as well as the corresponding decision tree (right)." style="width:80.0%" />
<p class="caption">Exemplary illustration of a two-dimensional feature space, divided by binary thresholds (left), as well as the corresponding decision tree (right).</p>
</div>
<p><strong>“Random”</strong>: The RF has two layers of randomness. First, it uses a bootstrapped random sample (with replacement) of the training dataset for growing each individual decision tree. The user can control for the fraction of training data used (e.g. via the <code>sampsize</code> parameter in the R implementation). The second random component is the selection of the features considered at each binary split.</p>
<div id="how-a-self-learning-tree-grows" class="section level3">
<h3>How a self-learning tree grows</h3>
<p>Self-learning decision trees automatically find the “best split” in the feature space to break the training data into two groups, which will be further subdivided until a clear decision for each sample can be made. The algorithm attempts to create splits that divide the data into relatively homogeneous subgroups.</p>
<div class="figure">
<img src="fig/s05_split.PNG" alt="A good split creates relatively homogeneous subgroups of the data." style="width:90.0%" />
<p class="caption">A good split creates relatively homogeneous subgroups of the data.</p>
</div>
<p>Finding a good split requires infomration on which feature (e.g. spectral band) and which value (e.g. reflectance) should be used to separate the data. The best split is identified based on a measure of dataset heterogeneity (Gini impurity index). The RF selects the split (feature and value) which minimizes the heterogeneity of the resulting datasets:</p>
<p><span class="math inline">\(Gini_{D} = 1 - \sum_{j=1}^n p_j^2\)</span></p>
<p>where <span class="math inline">\(D\)</span> is the dataset at hand and <span class="math inline">\(p_j\)</span> is the fraction of class <span class="math inline">\(j\)</span> in the dataset. High class proportions in the dataset produce low Gini index values. The Gini index for a split is a combined measure of the Gini index for the two datasets <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> resulting from the split. To account for different sizes of the two datasets, the Gini of each sub-dataset is weighted according to the number of observations <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> in the resulting sub-groups of each dataset.</p>
<p><span class="math inline">\(Gini_{split} = \frac{N_1}{N} Gini_{D_1} + \frac{N_2}{N} Gini_{D_2}\)</span></p>
<p>In case we would consider all available features at each split, the decision trees would probably be very similar. This is why we produce a random selection of features, for which identify the split (feature and value) that produces the most homogeneous sub-groups of training data. The user controls for the number of features which should be considered (or tried) at each split (e.g. via the <code>mtry</code> parameter in the R implementation). Decreasing the number of variables tried at each split de-correlates the structure of the trees.</p>
<p>Once <code>ntree</code> decision trees are built, we have a readily applicable rule-set to classify each pixel in our image. Every tree produces one “vote” regarding the final class outcome and the majority of votes determines the final class label for each pixel.</p>
</div>
<div id="the-out-of-bag-error" class="section level3">
<h3>The out-of-bag error</h3>
<p>The RF has a great feature: the out-of-bag error. After growing each tree, the left-out samples (i.e. not used for training of this particular tree) can be classified to investigate the performance of our classification model. Doing this allows us to calculate a classification error. For illustration, consider 8 OOB samples, of which 2 are wrongly and 6 are correctly classified. The OOB error will be 2/8 = 0.25. This simple measure of model performance can be useful to parametrize our classification model. We can for instance test various combinations of input features, training datasets, or investigate the RF performance with increasing <code>ntrees</code>.</p>
<div class="figure">
<img src="fig/s05_oob.PNG" alt="Grid representation of the out-of-bag classification error for various combinations of two input images from different days of the year (left). Decreasing out-of-bag error with increasing number of trees in the RF model (right)." style="width:80.0%" />
<p class="caption">Grid representation of the out-of-bag classification error for various combinations of two input images from different days of the year (left). Decreasing out-of-bag error with increasing number of trees in the RF model (right).</p>
</div>
</div>
<div id="variable-importances-partial-dependence-plots" class="section level3">
<h3>Variable importances &amp; partial dependence plots</h3>
<p>Next, the RF allows us to investigate which features (e.g. spectral bands) were most useful for our classification problem. Variable importances measures for each individual feature can be expressed either as mean decrease in Gini (how much “better”&quot; can we split the data based on this feature) or mean increase in accuracy (how much does the out-of-bag error increase when the feature is left out of the model). It is important to note that these variable importances should be interpreted cautiously, as they are always strongly dependent on the available input features and their collinearity structure, as well as the training data used. Still, the variable importance of a hypothetical model might reveal, for instance, that the swIR 1 reflectance at DOY 196 is quite important.</p>
<div class="figure">
<img src="fig/s05_varimp.png" alt="Mean decrease in accuracy (increase in OOB error) caused by the features included in the classification model." style="width:60.0%" />
<p class="caption">Mean decrease in accuracy (increase in OOB error) caused by the features included in the classification model.</p>
</div>
<p>In a next step, we can use partial dependence plots to further investigate the relationship between individual features and the likelihood of class occurrence. We can try to interpret this graph and assess relationships between individual features and our classes of interest. While generating meaningful insights is somewhat tricky with reflectance values, we could also imagine including, e.g., a digital elevation model into the RF model to learn about the distribution of our classes across elevation gradients.</p>
<div class="figure">
<img src="fig/s05_pdp.png" alt="Change in class likelihood across the value range of the swIR 1 reflectance at DOY 196." style="width:80.0%" />
<p class="caption">Change in class likelihood across the value range of the swIR 1 reflectance at DOY 196.</p>
</div>
<p>Summing it up, there are a some key advantages of the RF algorithm, which make it highly popular in data science and remote sensing. The RF is easy to understand, computationally efficient, parametrization is pretty straightforward, and it often produces great results. It provides quick insights into model performance via the out of bag error and allows for investigating variable importance measures and partial dependence plots.</p>
<hr />
</div>
</div>
<div id="exercise-6" class="section level2">
<h2>Exercise</h2>
<p>In this exercise, we will deal with image classification using the Random Forest algorithm. Specifically, we will use your training data and your pixel-based composites from the last exercises to map forest types in the Western Beskids. We will assess the performance of multiple classification models through the out of bag error and investigate variable importances. In case you missed the last sessions, we provide exemplary composites and training data <a href="https://box.hu-berlin.de/f/85a35c373cdd490180fe/?dl=1">here</a>.</p>
<div id="training-a-random-forest-model" class="section level3">
<h3>1) Training a Random Forest model</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Load the vector file containing your training data points using <code>readOGR()</code>. Next, create a <code>stack()</code> / <code>brick()</code> of your favourite pixel-based composite (last week´s result).</p></li>
<li><p>Use <code>extract()</code> to create a <code>data.frame</code> with training points as rows, and class labels (<code>classID</code>) as well as the spectral bands of your composites as columns. Remove the day of year and year flags (band 7 and 8) for the next steps.</p></li>
</ol>
<p>As we want to train a classification (and not a regression), the <code>randomForest()</code> function expects the dependent variable to be of type factor. Use <code>as.factor()</code> for conversion of the <code>classID</code> column. The RF algorithm cannot deal with NoData (<code>NA</code>) values. Remove <code>NAs</code> from the <code>data.frame</code>.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Train a <code>randomForest()</code> classification model with the <code>data.frame</code> created in the prior step.</li>
</ol>
</div>
<div id="training-additional-models" class="section level3">
<h3>2) Training additional models</h3>
<p>Repeat the RF training procedure based on…</p>
<ol style="list-style-type: lower-alpha">
<li>the second composite.<br />
</li>
<li>a stack of both composites.</li>
</ol>
</div>
<div id="investigating-model-performance" class="section level3">
<h3>3) Investigating model performance</h3>
<p>The model objects resulting from 2) contain a wealth of information on the model parameters and performance. Assess the out of bag (OOB) error estimates of the trained models by inspecting the <code>err.rate</code> attribute of your model objects. Answer the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li>Which model has the lowest OOB error?</li>
<li>How does the OOB behave when increasing the number of trees in your model (<code>ntrees</code>)? You can access the OOB per number of trees via <code>err.rate</code>. Use this information to roughly determine a suitable value for <code>ntrees</code>.</li>
<li>Which of the four classes has the highest OOB error?</li>
</ol>
<p>In case you are not satisfied with your model performance, consider using only high-quality training samples from your dataset, i.e. those samples with <code>confID = 1</code>. You may join forces with your neighbor and merge your training datasets. Feel free to experiment and document your findings.</p>
</div>
<div id="final-model-parametrization-and-variable-importances" class="section level3">
<h3>4) Final model parametrization and variable importances</h3>
<ol style="list-style-type: lower-alpha">
<li>Train a final model with the best combination of images and <code>ntrees</code>.</li>
<li>Investigate the variable importances using <code>varImpPlot()</code>. Use <code>partialPlot()</code> to produce partial dependence plots for your most important predictor and all four classes. Can you explain the differences between classes?</li>
</ol>
</div>
<div id="classification" class="section level3">
<h3>5) Classification</h3>
<p>Perform a classification of the image stack using the <code>predict()</code> function. Write the resulting map to disk in <code>GTiff</code> format. When doing so, consider choosing the appropriate <code>datatype</code> argument.</p>
</div>
</div>
<div id="reading-materials-4" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following reading (until page 23):</p>
<p><a href="http://www.fao.org/3/a-i5601e.pdf">FAO (2016): Map Accuracy Assessment and Area Estimation - A Practical Guide</a></p>
<p>This is a user guide for map accuracy assessment and area estimation, following the good practice recommendations outlined by <a href="https://doi.org/10.1016/j.rse.2014.02.015">Olofsson et al. (2014)</a>. A great piece of literature focussing on the applied side. Please read the document thoroughly, make sure you understand the basic idea behind it and write down question for the discussion in our next session. Also, answer the following broad questions:</p>
<ul>
<li>What is the motivation for this technical report?</li>
<li>Why is a stratified sampling advised?</li>
<li>How can we determine how many samples are needed for accuracy assessment?</li>
<li>What is the difference of populating an error matrix with <span class="math inline">\(p_{ij}\)</span> instead of <span class="math inline">\(n_{ij}\)</span>?</li>
<li>Which data source should be used for producing class area estimates?</li>
</ul>
<p>Please take a look at the Excel sheet provided <a href="https://box.hu-berlin.de/f/269470b96af54ba687ef/?dl=1">here</a>. Use this table to improve your understanding of the area adjusted accuracy assessment. You can manipulate values in the confusion matrix / class proportions and trace the formulas which link these values with the resulting class-specific and overall accuracies as well as the area estimates.</p>
<hr />
<!-- ################################## SESSION 06 ############################################## 

# Session 06: Accuracy assessment and area estimation

## Learning goals

- Improve your understanding of area-adjusted accuracy assessment
- Assess the map errors of your forest type classification
- Calculate error-adjusted class area estimates

## Background

Independent map accuracy assessment aims at getting an unbiased estimate of map accuracy. Generally, we determine the "true" class label at a variety of sample locations and compare those with the map. It should be noted that sample-based estimates of map accuracy are statistical estimates. In order to get a statistically sound estimate of map accuracy, we need to pay some attention to the way our sample locations are selected. Also, the more samples are included, the more robust our estimates will become. Collecting good reference data can be time consuming and we thus strive to reduce these activities to a required minimum.

Typical sampling schemes include pure random, systematic random, and stratified random sampling. When is it useful to use stratified sampling over a systematic or pure random? Imagine you want to map forest loss in an area dominated by forest, where deforestation accounts for roughly 0.2% of the area. In order to get a sufficiently large sample (let´s assume 60 samples) for the deforestation class using pure random sampling, we would approximately need to sample 30,000 locations. Stratified random sample helps us to precisely allocate the number of desired samples within each class of interest. 

Let´s bring this into the context of our study site. A map of our study region reveals imbalances in class extent, e.g., the deciduous forest class accounts for only 7% of the study area, while coniferous forests account for 40%.

|Class name       | Proportion ($w_i$)|
|-------------    |-------------      |
|Deciduous forest   |$0.07$             |
|Mixed forest       |$0.25$             |
|Coniferous forest|$0.40$             |
|Non-forest       |$0.28$             |

Let´s assume we want to validate our map using 50 points for each for the four classes. This is how a confusion matrix of the map validation could look like. Rows here relate to the map classes $i$ and columns indicate reference classes $j$. Note that the number of samples ($n$) per class differs quite substantially between map and reference. This will be of importance later on. 


|Class name         |Deciduous forest  |Mixed forest  |Coniferous forest  |Non-forest   |             |$n$          |
|-------------      |-------------     |------------- |-------------      |-------------|-------------|-------------|
|Deciduous forest     |$39$              |$5$           |$1$                |$5$          |             |$50$         |
|Mixed forest         |$15$              |$19$          |$10$               |$6$          |             |$50$         |
|Coniferous forest  |$0$               |$7$           |$39$               |$4$          |             |$50$         |
|Non-forest         |$11$              |$1$           |$4$                |$34$         |             |$50$         |
|                   |                  |              |                   |             |             |             |
|$n$                |$65$              |$32$          |$54$               |$49$         |             |$200$        |

From this table, we can derive the unadjusted accuracy values, such as the overall accuracy as follows: 

$OA_{unadjusted} = \frac{(39 + 19 + 34 + 34)}{200} = 0.63 = 63\%$. 

However, this estimate of overall accuracy incorporates all classes equally (independent of their true area proportion) and does not account for the sampling bias introduced through the stratified sampling. 

An example: If our map contains a total of 10,000 pixels, we have mapped 700 pixels as deciduous forest and 4,000 pixels as coniferous forest. By verifying 50 pixels of each class, we checked $\frac{50}{700} = 0.071 = 7.1\%$ of the deciduous forest class pixels, while only $\frac{50}{4,000} = 0.013 = 1.3\%$ of the coniferous forest class were investigated. 

**An assumption**: The samples drawn from the deciduous forest stratum further represent a much smaller area in our map (7%), as compared to the coniferous forest class (40%). In terms of overall accuracy, this means that the 39 correctly classified pixels of the deciduous forest class are probably less "relevant" than the 39 correctly classified pixels in the coniferous class. 

We have to account for this bias when estimating map accuracies from a stratified random sample. This can be accomplished by populating the confusion matrix with probabilities of encountering the combination of map class $i$ and reference class $j$, expressed as: 

$p_{ij} = \frac{n_{ij}}{n_i} * w_i$

$n_{ij}$ = Number of pixels belonging to map class $i$ and reference class $j$
$n_i$ = Total number of pixels of map class $i$
$w_i$ = Proportion of map class $i$

In the resulting matrix, each cell value represents the probability of occurrence of map class $i$ and reference class $j$. 

|Class name         |Deciduous forest      |Mixed forest        |Coniferous forest  |Non-forest         |
|-------------      |-------------         |-------------       |-------------      |-------------      |
|Deciduous forest     |$\frac{39}{50}0.07$   |$\frac{5/}{50}0.07$ |$\frac{1}{50}0.07$ |$\frac{5}{50}0.07$ |
|Mixed forest         |$\frac{15}{50}0.25$   |$\frac{19}{50}0.25$ |$\frac{10}{50}0.25$|$\frac{6}{50}0.25$ |
|Coniferous forest  |$\frac{0}{50}0.40$    |$\frac{7}{50}0.40$  |$\frac{39}{50}0.40$|$\frac{4}{50}0.40$ |
|Non-forest         |$\frac{11}{50}0.28$   |$\frac{1}{50}0.28$  |$\frac{4}{50}0.28$ |$\frac{34}{50}0.28$|

Which yields the confusion matrix populated with probabilities:

|Class name         |Deciduous forest      |Mixed forest        |Coniferous forest  |Non-forest         |               |$\sum p_{i}$ |
|-------------      |-------------         |-------------       |-------------      |-------------      | ------------- |-------------|
|Deciduous forest     |$0.055$               |$0.007$             |$0.001$            |$0.007$            |               |$0.070$      |
|Mixed forest         |$0.075$               |$0.095$             |$0.050$            |$0.030$            |               |$0.250$      |
|Coniferous forest  |$0.000$               |$0.056$             |$0.312$            |$0.032$            |               |$0.400$      |
|Non-forest         |$0.062$               |$0.006$             |$0.022$            |$0.190$            |               |$0.280$      |
|                   |                      |                    |                   |                   |               |             |
|$\sum p_{j}$       |$0.192$               |$0.164$             |$0.385$            |$0.259$            |               |$1.000$      |

This table sums to 1 and the row sums correspond to the map class proportions from above. We can now confirm our assumption from above. The 39 correct pixels of the deciduous forest class represents a smaller map proportion as compared to the 34 correctly classified pixels of the coniferous forest class (5.5% vs. 27.2%). The deciduous forest class is therefore less relevant for the area-adjusted overall map accuracy.

Based on this probability confusion matrix, we can calculate the overall and class-wise accuracy scores as usual, while implicitly accounting for different class proportions and the sampling bias. 

$OA_{adjusted} = 0.055 + 0.095 + 0.312 + 0.190 = 0.652$

We can notice that our overall accuracy increased after area-adjustment. This makes sense, as we were doing quite good in mapping coniferous forests accurately. 

Let´s have a look at the user´s and producer´s accuracy of the deciduous forest class. First, we calculate the unadjusted accuracy scores:

$UA_{unadjusted} =  \frac{39}{50} = 0.78 = 78\%$
$PA_{unadjusted} =  \frac{39}{65} = 0.60 = 60\%$

Let´s do the same using the probability matrix: 

$UA_{adjusted} =  \frac{0.055}{0.070} = 0.78 = 78\%$
$PA_{adjusted} =  \frac{0.055}{0.192} = 0.29 = 29\%$

We can see that the user´s accuracy stays the same after adjustment. This makes sense, as we do not incorporate the sampling bias when considering only samples drawn from one map stratum. For the producer´s accuracy, we combine information from samples across all strata. We hence automatically consider the different class proportions and sampling bias when calculating the producer´s accuracy from the confusion matrix populated with probabilities. The producer´s accuracy is relatively low after adjustment, owing to the fact that we likely omit a lot of deciduous forest, which we falsely classified as mixed forest or non-forest.

### Error-adjusted area estimates

Class-wise area estimates can be derived directly from the map by multiplying the map´s class proportions with the total size of the study area. Assuming a study area of $S = 10,000 ha$, our area estimates for deciduous forest will be: 

$area_{DF} = w_{i=DF} * S$
$area_{DF} = 0.070 * 10,000 ha = 700 ha$

We already know that the reference data at hand is of better quality as compared to our map. We therefore preferably estimate class area directly from the reference data:

$area_{DF} = \sum_{p_{j=DF}}^i * S$
$area_{DF} = 0.192 * 10,000 ha = 1,920 ha$

So the better area estimate for deciduous forest in our study region is 1,920 ha, as compared to the map-based estimate of 700 ha. Here again, the substantial omission of deciduous forest in our map was taken into account, which in turn largely increases the area estimate for this class. 

Wrapping it up, area adjustment can make quite a substantial difference for overall accuracy and producer´s accuracy. User´s accuracy remains unaffected. Producing area estimates directly from the reference data is advised in order to produce more accurate estimations of land cover, land use, or land change. All procedures which were only briefly described here are explained in more detail in [Olofsson et al. (2014)](https://doi.org/10.1016/j.rse.2014.02.015). This is a great read documenting the current state-of-the-art in regards to accuracy assessment. 

## Exercise

In this exercise, you will perform an accuracy assessment of your forest type map. We will perform an area-adjusted accuracy assessment and estimate error-adjusted area for each forest type class. 

### 1) Producing reference data

Load your forest type map in R. Create a stratified reference sample using ```sampleStratified()```. We want to have a stratified equalized sample of 20 pixels per class. Specify the ```size``` parameter accordingly. Specify ```sp = TRUE``` to receive a spatial points object and ```na.rm = TRUE``` to ignore unclassified regions. Write the points to disk using ```writeOGR()```.

Load the point shapefile in QGIS and add two fields of type Integer: ```classID``` and ```confID```. Visit each point and determine the class label according to the high resolution imagery in GoogleEarth. You can install and use the Send2GE plugin in QGIS to identify the precise pixel location. Enter the class and confidence labels in the attribute table. Work efficiently through the points and save your changes regularly. 

|Class name       | classID     | Confidence level | confID       |
|-------------    |-------------|-------------     |------------- |
|Deciduous forest   | 1           |Very certain      | 1            |
|Mixed forest       | 2           |Some uncertainties| 2            |
|Coniferous forest| 3           |Very uncertain    | 3            |
|Non-forest       | 4           |                  |              | 


### 2) Area-adjusted accuracy assessment

Assess the class proportions of your map in R. To do so, use the ```freq()``` function to get pixel counts. Use these for calculating class propotions (0-1). Remember to exclude ```NAs```, so the proportions of your 4 classes sum up to 1.
 
Load the reference point shapefile in R and ```extract()``` the class values at the point locations. Create a confusion matrix from the resulting ```data.frame``` using ```table()```.

Copy & paste the values from your confusion matrix as well as the class proportions into the Excel spreadsheet. Next, answer the following questions: 

a. Which class has the highest / lowest user´s accuracy?
b. Which class has the highest / lowest producer´s accuracy?
c. How does the overall accuracy differ after area-adjustment? Why?
d. How do the map-based area estimates differ from those obtained using the reference data?

### 3) Knowledge transfer

Implement three basic components of the accuracy assessment in R. 

a.  Generate the confusion matrix containing probabilities. In this matrix, each cell value represents the probability of occurrence on map class $i$ and reference class $j$: 

$p_{ij} = \frac{n_{ij}}{n_i} * w_i$

$n_{ij}$ = Number of pixels belonging to map class $i$ and reference class $j$
$n_i$ = Total number of pixels of map class $i$
$w_i$ = Proportion of map class $i$

b.  Calculate overall accuracy and class-wise user´s and producer´s accuracy from the confusion matrix. 
c.  Produce error-adjusted area estimates from the confusion matrix. 

To do this, track the underlying formulas of the respective sections in the Excel sheet. Find a way on how to do this in R. Use helper functions such as ```diag()```, ```sum()``` and ```apply()```. Avoid ```for``` loops. 

Compare the results with the values in the Excel table. If they differ, something went wrong. Check again! 


## Reading materials

In this paper, [Hansen et al. (2013)](http://doi.org/10.1126/science.1244693) presented the first global Landsat-based map of forest cover change processes. Please make sure to read the paper and the supplementary materials and take notes on questions of understanding and issues to discuss during the course.

------

<!-- 

# Session 07: Multi-temporal change detection

## Learning goals

## Reading materials

------

# Session 08: Spectral-temporal metrics

## Learning goals

## Reading materials.

------

# Project work

## Phase I

## Phase II

## Phase III

------ 

-->
</div>
</div>
<div id="terminology" class="section level1">
<h1>Terminology</h1>
<p>This is a non-exhaustive list of common terms and their definitions.</p>
<div id="observations" class="section level2">
<h2>Observations</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Pixel">Pixel</a></li>
<li>Observation</li>
<li>Path/Row; WRS-2; Footprint; Scene: Partitions of Landsat images into approximately 185 × 185 km squares.</li>
<li>Grid: an arbitrary subdivision with square units in the target coordinate system.</li>
<li>Tile: an entity of the grid with a unique tile identifier.</li>
<li>Chip: the individual gridded images that are affiliated with the tile.</li>
<li>Scene, Image</li>
<li>Data cube, Stack</li>
<li>Time series</li>
<li>Archive</li>
<li>Collection, Tier, Processing level</li>
<li>DN, TOA, BOA</li>
<li>VIS, nIR, swIR</li>
</ul>
</div>
<div id="sensor-characteristics" class="section level2">
<h2>Sensor characteristics</h2>
<ul>
<li>Satellite</li>
<li>Sensor</li>
<li>Spatial resolution</li>
<li>Temporal resolution</li>
<li>Temporal coverage</li>
<li>Spectral resolution</li>
<li>Radiometric resolution</li>
<li>Bits, Bytes</li>
</ul>
</div>
<div id="data-characteristics" class="section level2">
<h2>Data characteristics</h2>
<ul>
<li>Spectral band</li>
<li>Image quality</li>
<li>Very High Resolution (VHR)</li>
<li>High Resolution</li>
<li>Medium Resolution</li>
<li>Moderate Resolution</li>
<li>Coarse Resolution</li>
<li>Time Series</li>
<li>Observation density</li>
<li>Multi-temporal</li>
<li>Hyper-temporal</li>
</ul>
</div>
<div id="higher-level-products" class="section level2">
<h2>Higher-level products</h2>
<ul>
<li>Composite / Mosaic</li>
<li>Pixel-based composites (Best observation composite)</li>
<li>Phenology-adaptive composites</li>
<li>Spectral-temporal metrics</li>
<li>Rank-band composite / metric</li>
<li>Phenometrics</li>
</ul>
</div>
<div id="time-intervals" class="section level2">
<h2>Time intervals</h2>
<ul>
<li>Multi-annual</li>
<li>Inter-annual</li>
<li>Annual</li>
<li>Intra-annual</li>
<li>Seasonal</li>
</ul>
</div>
</div>

    </div>
    <div class="col-xs-2">
        </div>
  </div>
  </div>
  </div>
  <div class="row">
    </div>
  </div>

<script>
$(document).ready(function () {
  // add bootstrap table styles to pandoc tables
  $('tr.header').parent('thead').parent('table').addClass('table table-striped table-hover');

    var images = $('.pages img');
  images.filter(function() {
      if ($(this).parent().attr("class") == "figure") {
          return(false)
      } else {
          return(true);
      }
  }).wrap("<div class='figure'></div>");
  images.addClass("image-thumb").wrap("<div class='panel-body'></div>");
  $('.figure p.caption').wrap("<div class='panel-footer'></div>");
  $('.figure').addClass('panel panel-default');
  
    $('.pages img')
 	  .addClass("image-lb");
  $('.pages').magnificPopup({
	      type:'image',
	      closeOnContentClick: false,
	      closeBtnInside: false,
        delegate: 'img',
	      gallery: {enabled: false },
          removalDelay: 500,
          callbacks: {
              beforeOpen: function() {
                // just a hack that adds mfp-anim class to markup
                this.st.image.markup = this.st.image.markup.replace('mfp-figure', 'mfp-figure mfp-with-anim');
              }
          },
          mainClass: 'mfp-move-from-top',
	      image: {
	        verticalFit: true,
            titleSrc: 'alt'
	      }
 	    });
 	
    
    $('#toc ul li').first().addClass("active");
    $('#toc ul li').attr("data-target", function() {
        return($(this).children("a").attr("href"));
    })
    $('body .section.level1').first().addClass("active");
    
    $('#toc a[href*="#"]').click(function() {

      var id = $(this).attr("href");
      if (id === "#") return;
      if (id.substring(0, 8) === "#dyntab-") return;
      toggle_page(id);

      // Menu
      var menu_entry = $(".menu li[data-target='"+id+"']");
      menu_entry.addClass("active");
      $(".menu li").not(menu_entry).removeClass("active"); 
      

    });

    function toggle_page(id) {
      $(".page").not(page).removeClass("active").hide();
      window.page = id;
      var page = $(window.page);
      window.location.hash = window.page;
      //$(this).addClass("active");

      page.show();

      var totop = setInterval(function () {
        $(".pages").animate({scrollTop: 0}, 0);
      }, 10);

      setTimeout(function () {
        page.addClass("active");
        setTimeout(function () {
          clearInterval(totop);
        }, 1000);
      }, 100);

      window.dispatchEvent(new Event('resize'));

    }


    $(".menu li").click(function () {

      toggle_page($(this).data("target"));

      // Menu
      if (!$(this).data("target")) return;
      if ($(this).is(".active")) return;
      $(".menu li").not($(this)).removeClass("active");
      $(this).addClass("active");

    });
  
    


    window.page = window.location.hash;
    if (window.page != "") {
      $(".menu").find("li[data-target=" + window.page + "]").trigger("click");
    }

    /* init material bootstrap js */
    $.material.init();
});
</script>




<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
