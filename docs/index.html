<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="MSc Global Change Geography" />


<title>Earth Observation</title>
<!-- Material Design fonts -->
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/icon?family=Material+Icons">
<script src="index_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.6/css/bootstrap.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.6/js/bootstrap.min.js"></script>
<script src="index_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<script src="index_files/navigation-1.1/tabsets.js"></script>
<script src="index_files/navigation-1.1/codefolding.js"></script>
<link href="index_files/magnific-popup-1.1.0/magnific-popup.css" rel="stylesheet" />
<script src="index_files/magnific-popup-1.1.0/jquery.magnific-popup.min.js"></script>
<link href="index_files/bootstrap_material-0.1/bootstrap-material-design.min.css" rel="stylesheet" />
<link href="index_files/bootstrap_material-0.1/ripples.min.css" rel="stylesheet" />
<script src="index_files/bootstrap_material-0.1/material.min.js"></script>
<script src="index_files/bootstrap_material-0.1/ripples.min.js"></script>
<link href="index_files/material-0.1/material.css" rel="stylesheet" />
<script src="index_files/material-0.1/material.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #ffffff; color: #a0a0a0; border-right: 1px solid #a0a0a0; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #1f1c1b; background-color: #ffffff; }
code > span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code > span.dt { color: #0057ae; } /* DataType */
code > span.dv { color: #b08000; } /* DecVal */
code > span.bn { color: #b08000; } /* BaseN */
code > span.fl { color: #b08000; } /* Float */
code > span.cn { color: #aa5500; } /* Constant */
code > span.ch { color: #924c9d; } /* Char */
code > span.sc { color: #3daee9; } /* SpecialChar */
code > span.st { color: #bf0303; } /* String */
code > span.vs { color: #bf0303; } /* VerbatimString */
code > span.ss { color: #ff5500; } /* SpecialString */
code > span.im { color: #ff5500; } /* Import */
code > span.co { color: #898887; } /* Comment */
code > span.do { color: #607880; } /* Documentation */
code > span.an { color: #ca60ca; } /* Annotation */
code > span.cv { color: #0095ff; } /* CommentVar */
code > span.ot { color: #006e28; } /* Other */
code > span.fu { color: #644a9b; } /* Function */
code > span.va { color: #0057ae; } /* Variable */
code > span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code > span.op { color: #1f1c1b; } /* Operator */
code > span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code > span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code > span.pp { color: #006e28; } /* Preprocessor */
code > span.at { color: #0057ae; } /* Attribute */
code > span.re { color: #0057ae; } /* RegionMarker */
code > span.in { color: #b08000; } /* Information */
code > span.wa { color: #bf0303; } /* Warning */
code > span.al { color: #bf0303; font-weight: bold; } /* Alert */
code > span.er { color: #bf0303; text-decoration: underline; } /* Error */
code > span. { color: #1f1c1b; } /* Normal */
</style>

<link rel="stylesheet" href="material_adjust.css" type="text/css" />

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->

</head>

<body>

<div class="header-panel shadow z-2">
    <div class="container-fluid">
        <div class="row">
            <div class="col-xs-3">
        <div id="header">
    <h1 class="title">Earth Observation</h1>
                <h4 class="author">MSc Global Change Geography</h4>
                <h4 class="date">Summer term 2019</h4>
        </div>
    </div>
</div>
</div>
</div>


<div class="container-fluid main-container">
    <div class="row">
      <nav class="col-xs-3 menu">
        <div id="toc">
        <ul>
        <li><a href="#hello">Hello!</a></li>
        <li><a href="#introducing-r">Introducing R</a></li>
        <li><a href="#course-materials">Course materials</a></li>
        <li><a href="#session-01-handling-rasters-in-r">Session 01: Handling rasters in R</a></li>
        <li><a href="#session-02-from-dns-to-toa">Session 02: From DNs to TOA</a></li>
        <li><a href="#session-02-data-quality-cloud-masking">Session 02: Data quality &amp; cloud masking</a></li>
        <li><a href="#session-03-vegetation-indices-and-data-transforms">Session 03: Vegetation indices and data transforms</a></li>
        <li><a href="#session-03-training-data-collection">Session 03: Training data collection</a></li>
        <li><a href="#session-04-pixel-based-compositing">Session 04: Pixel-based compositing</a></li>
        <li><a href="#session-05-machine-learning-for-image-classification">Session 05: Machine learning for image classification</a></li>
        <li><a href="#session-06-accuracy-assessment-and-area-estimation">Session 06: Accuracy assessment and area estimation</a></li>
        <li><a href="#session-07-multi-temporal-change-detection">Session 07: Multi-temporal change detection</a></li>
        <li><a href="#session-08-spectral-temporal-metrics">Session 08: Spectral-temporal metrics</a></li>
        <li><a href="#session-09-project-phase-i">Session 09: Project phase I</a></li>
        <li><a href="#session-10-project-phase-ii">Session 10: Project phase II</a></li>
        <li><a href="#terminology">Terminology</a></li>
        </ul>
        </div>
        
        
        
      </nav>
     <div class="pages col-xs-9">
     <div class="row">
       <div class="col-xs-10">



<div id="hello" class="section level1">
<h1>Hello!</h1>
<div class="figure">
<img src="fig/header.png" />

</div>
<div id="about-earth-observation" class="section level2">
<h2>About Earth Observation</h2>
<p>Earth Observation is an advanced course for students of the Master of Science <a href="https://www.geographie.hu-berlin.de/en/studies/study-programs/master-degree-programs/master-of-science">“Global Change Geography”</a> of Humboldt-Universität zu Berlin. In this course, we cover multiple aspects of optical remote sensing by working with multi-sprectral Landsat and Sentinel 2 imagery. The course is fully based on open source software, including R and QGIS.</p>
<hr />
</div>
<div id="learning-goals-course-contents" class="section level2">
<h2>Learning goals &amp; course contents</h2>
<p>The main goal of this course is to provide you with the necessary knowledge and tools for using optical remote sensing datasets and methods in the geo-scientific context. We want you to enhance your ability of problem-solving, empowering you to perform research independently. To that end, we cover aspects of data acquisition, spatial data handling in R and QGIS, basics of image pre-processing, higher-level processing such as pixel-based compositing and time-series binning. The course contents are related to our lab´s research foci, both in terms of methods and study regions. You may want to check out our <a href="https://www.geographie.hu-berlin.de/en/professorships/geomatics/publications-en">publications</a>, <a href="https://www.geographie.hu-berlin.de/en/professorships/geomatics/projects">current projects</a>, or have a look at <a href="https://www2.hu-berlin.de/geomultisens/europeanLandCover/euroLandCover.html">this example</a>.</p>
<p>In the course you will learn about current state-of-the-art methods in image processing and time series analyses of optical satellite imagery. The course covers methods related to data quality, cloud masking, vegetation indices, multi-temporal image analyses, machine learning classification algorithms, area adjusted accuracy assessment, time series analyses, and image compositing. We use these methods for mapping of forest types, forest cover changes, agricultural dynamics in the Carpathian ecoregion (Poland), the Southern Brazilian Amazon, and Crete in Greece.</p>
<hr />
</div>
<div id="requirements" class="section level2">
<h2>Requirements</h2>
<p>A good understanding of basic principles of remote sensing is needed to follow this course. Participants should furthermore have a basic understanding of R, including syntax, data types and knowledge on how to read, manipulate and write data. As you followed the curriculum of the MSc program, you most likely joined the module “Quantitative Methods for Geographers”, in which you learned using R for statistical problems. Here, we built on your existing knowledge. If you are not enrolled in the MSc program, feel free to look at the <a href="https://github.com/corneliussenf/quantitative_methods">course materials</a>.</p>
<p>Alternatively, you may want to follow one of the numerous tutorials for R fundamentals (e.g., <a href="https://www.rstudio.com/online-learning/">RStudio</a>, <a href="https://www.datacamp.com/courses/free-introduction-to-r">DataCamp</a>, <a href="http://www.r-tutorial.nl/">UMC Utrecht</a>, <a href="http://adv-r.had.co.nz/">Advanced R by Hadley Wickham</a>, <a href="https://r-graphics.org/">R Graphics Cookbook</a>), or one of those specifically for geodata processing (e.g., <a href="https://geoscripting-wur.github.io/">Wageningen University</a>, <a href="https://www.earthdatascience.org/">University of Colorado</a>).</p>
<hr />
<!-- ################################## INTRO R ############################################## -->
</div>
</div>
<div id="introducing-r" class="section level1">
<h1>Introducing R</h1>
<div id="why-do-we-use-r" class="section level2">
<h2>Why do we use R?</h2>
<p>R is a programming language and open source software environment for statistical computing and graphics. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. It was developed by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand. The name R originates the first names of the two authors and refers to the programming language S. The project was conceived in 1992, with an initial version released in 1995 and a stable beta version in 2000.</p>
<p>Learning R has tons of advantages. It is a great starting point for those eager to learn programming. R offers increasingly specialized tools for data wrangling, statistical analyses, and visualization. The CRAN package repository currently features &gt;13,000 packages serving a variety of purposes, e.g. data manipulation (<code>tidyr</code>, <code>dplyr</code>, <code>caret</code>), visualization (<code>ggplot2</code>, <code>ggmap</code>, <code>rasterVis</code>), and geodata handling (<code>raster</code>, <code>rgdal</code>, <code>sp</code>, <code>sf</code>). You will notice that a huge share of figures in scientific publications was produced using R. The R community is huge, and offers great support. R is extremely popular in science &amp; industry, so a proficiency in R opens a wide array of job opportunities. Everything is free and open source.</p>
<div class="figure">
<img src="fig/fig00.png" alt="A rising tide for R (Tipman 2015; doi: 10.1038/517109a)" />
<p class="caption">A rising tide for R (Tipman 2015; doi: 10.1038/517109a)</p>
</div>
<hr />
</div>
<div id="coding-style" class="section level2">
<h2>Coding style</h2>
<p>A few basic rules apply to coding in R. Here is a short summary of <a href="http://adv-r.had.co.nz/Style.html">Hadley Wickham´s style guide</a>:</p>
<ul>
<li>Regularly save your progress.</li>
<li><p>Script names should be meaningful and end in ‘.R’.</p></li>
<li>Comment (#) your code &amp; separate it into readable chunks.</li>
<li><p>Try to limit your code to 80 characters per line.</p></li>
<li>Variable and function names should be lowercase.</li>
<li><p>Variable names should be nouns and function names verbs.</p></li>
<li>Place spaces around operators (=, +, -, &lt;-, etc.) and after commas.</li>
<li><p>Use &lt;-, not =, for assignment.</p></li>
</ul>
<p>An example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">######################################################
<span class="co"># Creating random data and a correlated response </span>
<span class="co"># Philippe Rufin, 2019</span>

<span class="co"># Load all required packages</span>
<span class="kw">library</span>(ggplot2)

<span class="co"># Create random data</span>
x &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">50</span>, <span class="dv">0</span>, <span class="dv">2</span>)

<span class="co"># Build function to simulate response</span>
create.response &lt;-<span class="st"> </span><span class="cf">function</span>(x){x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>, <span class="dv">0</span>, <span class="fl">0.2</span>)}

<span class="co"># Apply function to random data</span>
y &lt;-<span class="st"> </span><span class="kw">create.response</span>(x)

<span class="co"># Make a dataframe</span>
data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&#39;x&#39;</span> =<span class="st"> </span>x, <span class="st">&#39;y&#39;</span> =<span class="st"> </span>y)

<span class="co"># Plot the simulated dataset</span>
<span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="index_files/figure-html/style-1.png" width="288" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Investigate correlation in the data</span>
<span class="kw">cor</span>(data<span class="op">$</span>x, data<span class="op">$</span>y)</code></pre></div>
<pre><code>[1] 0.9444786</code></pre>
<hr />
</div>
<div id="help" class="section level2">
<h2>Help!</h2>
<p>If you get stuck, there are plenty of things you can do:</p>
<ul>
<li>Seek the function´s help page (i.e. highlight the function and hit F1)</li>
<li>Search your problem or error message</li>
<li>Ask your colleagues</li>
<li>Use the moodle course forum</li>
<li>Check forums (e.g., <a href="https://stackoverflow.com/">StackOverflow</a>)</li>
</ul>
<hr />
</div>
</div>
<div id="course-materials" class="section level1">
<h1>Course materials</h1>
<div id="readings" class="section level2">
<h2>Readings</h2>
<p>The first sessions of the course contain reading materials, such as are peer-reviewed papers and technical reports. You will find the reading materials for the next session at the end of each session. We highlight aspects to focus upon to streamline the reading process and facilitate the discussion. We are looking forward to lively discussions of the reading materials and critical questions from your end.</p>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>All data used in the course is openly accessible. Mostly, we´ll be working with Landsat images, which you can access through the USGS Earth Explorer. We provide download links to the datasets for each session. It will be helpful if you organize your data in a course directory on your local machine (MSc students might want to use drive <code>O:/Student_Data/your_name/EO/</code>). We will refer to this folder as <code>course.dir</code> throughout this course. Create subdirectories for each session, e.g. <code>course.dir/S01/</code> and separate data, code and course materials in additional sub-directories (e.g. <code>/data</code>, <code>/code</code>, <code>/docs</code>).</p>
</div>
<div id="exercises" class="section level2">
<h2>Exercises</h2>
<p>The weekly exercises are defined in the respective session. Each session comprises several tasks that involve scipting in R. Course participants must submit completed exercises, documented as R scripts, in <a href="http://moodle.hu-berlin.de/">moodle</a> to pass. Weekly submission deadlines are every sunday, 23:59. Please name the script of your work group as SXX_name1_name2.R, e.g. S01_ernst_rufin.R. Please structure your script for every exercise as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">#############################################################################
<span class="co"># MSc Earth Observation Exercise [Session number]</span>
<span class="co"># [Your Name]</span>
#############################################################################

<span class="co"># Load packages, use install.packages(&#39;packagename&#39;) to install if needed</span>
<span class="kw">library</span>(raster)

<span class="co"># Change raster options to store large rasters in temp files on disk</span>
<span class="kw">rasterOptions</span>(<span class="dt">maxmemory =</span> <span class="fl">1e6</span>)

<span class="co"># Define the folder that contains your data...</span>
data.dir &lt;-<span class="st"> &#39;course.dir/S01/data/&#39;</span>

#############################################################################
<span class="co"># 1)    </span>
#############################################################################

<span class="co"># Comments for task 1</span>


#############################################################################
<span class="co"># 2)    </span>
#############################################################################

<span class="co"># ...</span></code></pre></div>
<!-- ################################## SESSION 01 ############################################## -->
</div>
</div>
<div id="session-01-handling-rasters-in-r" class="section level1">
<h1>Session 01: Handling rasters in R</h1>
<div id="learning-goals" class="section level2">
<h2>Learning goals</h2>
<p>In this session, you will</p>
<ul>
<li>Acquire multi-spectral satellite data</li>
<li>Read &amp; write raster data</li>
<li>Manipulate the spatial extent of rasters</li>
<li>Extract cell values &amp; plot a spectral profile</li>
</ul>
<hr />
</div>
<div id="the-raster-package" class="section level2">
<h2>The <code>raster</code> package</h2>
<p>It´s great, as it facilitates raster data handling. It allows us to access file characteristics before loading data into memory, facilitates handling of coordinate reference systems and spatial extents. We can use it to perform raster algebra, to combine raster and vector datasets (e.g. ESRI shapefiles), or to convert raster files into matrices, which are compatible with the base functions to access image statistics, develop models, slice data dimensions etc.</p>
<p>There are a couple of things that the raster package does not provide. For example, advanced visualization of spatial datasets and manual operations, such collecting training or validation data, are preferably done in a GIS environment (e.g. QGIS). Furthermore, processing large data volumes in R can be quite time-consuming (we often use Python instead, it´s syntax is quite similar to R).</p>
<p>You install the raster package just like any other package in R. Dependencies will automatically be installed. On some machines, you might need to install <code>rgdal</code> manually.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Install the raster and rgdal packages</span>
<span class="kw">install.packages</span>(<span class="st">&#39;raster&#39;</span>)
<span class="kw">install.packages</span>(<span class="st">&#39;rgdal&#39;</span>)

<span class="co"># Load the package</span>
<span class="kw">library</span>(raster)</code></pre></div>
<hr />
</div>
<div id="exercise" class="section level2">
<h2>Exercise</h2>
<div id="data-acquisition" class="section level3">
<h3>1) Data acquisition</h3>
<p>Let´s get some Landsat data. Visit the <a href="http://earthexplorer.usgs.gov/">USGS Earth Explorer</a> and use the Adress/Place field to navigate to Wisła, Poland (lat,lon: 49.6473,18.8677). Switch to the ‘Data Sets’ tab and select Landsat -&gt; Landsat Collection 1 Level-1. Tick the ‘Landsat 8 OLI/TIRS C1 Level 1’ box and click on ‘Results &gt;&gt;’.</p>
<p>You´ll get several hundreds of results, so let´s narrow down the search. Under ‘Search Criteria’, define an acquisition date range between February 2014 and August 2014. Switch to the ‘Additional Criteria’ tab. Let´s choose a scene cloud cover of ‘Less than 40%’, and select the ‘Tier 1’ category.</p>
<p>Find the following images:</p>
<ul>
<li>LC08_L1TP_189025_20140716_20170421_01_T1</li>
<li>LC08_L1TP_189025_20140310_20170425_01_T1</li>
</ul>
<p>Check <a href="https://www.usgs.gov/land-resources/nli/landsat/landsat-collection-1">this website</a> to get an overview of the Landsat Collection 1 file naming convention (product identifiers) and further information such as processing levels.</p>
<p>Visualize the images in the Earth Explorer interface by clicking on the small image icon. For downloading the data, you will need an EarthExplorer account. You may register and download the .tar.gz files. If you prefer not to register, you can <a href="https://box.hu-berlin.de/f/5248da1584054eb6ba51/?dl=1">download the files from our repository</a>. Unpack the files in your session directory.</p>
<hr />
</div>
<div id="reading-data" class="section level3">
<h3>2) Reading data</h3>
<p>Today, you will make use of R´s raster package classes and functions which are well described in the package documentation. Get acquainted with the following classes and functions and find out what they are useful for: <code>raster()</code>, <code>stack()</code> ,<code>extent()</code>, <code>crop()</code>, <code>extract()</code>, <code>CRS()</code>, <code>projectRaster()</code>, <code>plotRGB()</code>, <code>writeRaster()</code></p>
<p>Visit the folder containing the unpacked Landsat image. Did you take a close look at the <a href="https://www.usgs.gov/media/images/landsat-collection-1-product-identifier">Landsat file naming convention</a>? Practically, it provides some basic meta-information. For instance, <code>LC08_L1TP_189025_20140310_20170425_01_T1</code> is a sequence of information on the sensor, processing level, WRS path and row, acquisition date, processing date, collection, and collection tier, separated by ’_’.</p>
<p>As you can see, the Landsat images are delivered as single-band files. The single bands should be stacked for further analyses. For stacking, all input files must have matching extents and the identical projection. Create a stack for each of the two Landsat 8 images.</p>
<p>Important: Please include only the following bands: blue, green, red, near infrared, shortwave infrared 1, shortwave infrared 2 (in this order). Check the list of <a href="https://landsat.usgs.gov/what-are-band-designations-landsat-satellites/">Landsat spectral bands</a> for a recap. Always keep the band designations in mind, as this can cause confusion, e.g. when combining Landsat 5 and Landsat 8 data.</p>
<div class="figure">
<img src="fig/ls_bands.jpg" alt="Band designations for Landsat satellites" style="width:70.0%" />
<p class="caption">Band designations for Landsat satellites</p>
</div>
<p>Try to create the two stacks with a minimum amount of code as possible! Consider using helper functions such as <code>paste0()</code>, <code>dir()</code> or <code>list.files()</code>.</p>
<hr />
</div>
<div id="manipulating-data" class="section level3">
<h3>3) Manipulating data</h3>
<p>Investigate the stack. In which projection is the data delivered?</p>
<p>Compare the extent of the two images. You will notice that they vary. Trying to stack images of different extent will cause an error message claiming:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">image.stack &lt;-<span class="st"> </span><span class="kw">stack</span>(image.one, image.two)
Error <span class="cf">in</span> <span class="kw">compareRaster</span>(x) <span class="op">:</span><span class="st"> </span>different extent</code></pre></div>
<p>To stack both images, we need to crop (i.e. clip, or cut) the images to their common extent. Find an efficient way to identify the common extent of the images, defined as <code>common.extent &lt;- c(xmin, xmax, ymin, ymax)</code> (in projected coordinates).</p>
<div class="figure">
<img src="fig/fig01.png" alt="Identifying the common extent of several images" />
<p class="caption">Identifying the common extent of several images</p>
</div>
<p>The common extent of the two images is pretty large. In order to reduce the amount of data for the next steps, we should crop the images to our region of interest, a part of the Western Beskids, defined by <code>roi.extent &lt;- c(327945, 380325, 5472105, 5521095)</code></p>
<hr />
</div>
<div id="writing-data" class="section level3">
<h3>4) Writing data</h3>
<p>Write the cropped stacks to your folder using <code>writeRaster()</code>. Use the <code>GTiff</code> format and the <a href="https://www.rdocumentation.org/packages/raster/versions/2.5-8/topics/dataType">appropriate datatype</a>. Why is this important?</p>
<hr />
</div>
<div id="visualizing-data" class="section level3">
<h3>5) Visualizing data</h3>
<p>Open the cropped images in QGIS. Seek the symbology to create a true-color (red, green, blue) and a false-color representation (e.g., RGB: swIR1, nIR, red) of each image. Make sure to properly consider the order of bands in your stack (blue, green, red, nIR, swIR 1, swIR 2) in relation to your computer screen´s color channels (RGB).</p>
<p>Use the <code>plotRGB()</code> function in R to create another false-color visualization of the images in R.</p>
<div class="figure">
<img src="fig/s01_falsecolor_432.png" alt="False color visualization (RGB: nIR, red, green)" />
<p class="caption">False color visualization (RGB: nIR, red, green)</p>
</div>
<hr />
</div>
<div id="extracting-spectral-profiles" class="section level3">
<h3>6) Extracting spectral profiles</h3>
<p>Use the <code>extract()</code> function to get spectral profiles from both images. Use the following coordinate:</p>
<p><code>coordinate &lt;- data.frame('x' = 355623, 'y' = 5486216)</code>.</p>
<p>Visualize the results in R. Can you create one plot that shows two spectral profiles (one for each image in the stack), while accounting for the band wavelength and acquisition date? Can you guess what type of surface we are looking at?</p>
<div class="figure">
<img src="fig/s01_spectra.png" alt="Spectral profiles for two observation dates" style="width:60.0%" />
<p class="caption">Spectral profiles for two observation dates</p>
</div>
<hr />
</div>
</div>
<div id="reading-materials" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following paper:</p>
<p><a href="https://doi.org/10.1016/j.rse.2011.10.028">Zhu, Z., &amp; Woodcock, C.E. (2012). Object-based cloud and cloud shadow detection in Landsat imagery. Remote Sensing of Environment, 118, 83–94.</a></p>
<p>This is a rather technical reading, which introduces the Fmask algorithm for automated cloud and cloud shadow detection. It has been widely used for cloud detection on Landsat TM and ETM+ data, and was enhanced for the use with Landsat OLI and Sentinel 2 data (documented in <a href="https://doi.org/10.1016/j.rse.2014.12.014">Zhu et al. 2015</a>).</p>
<p>While reading focus on the following broad questions:</p>
<ul>
<li>Why do we need automated cloud masking?</li>
<li>How does it work in principle?</li>
<li>Where are the limitations?</li>
</ul>
<p>More specifically, think about the following:</p>
<ul>
<li>How were the thresholds for spectral tests derived?</li>
<li>How will different error types impact further analyses?</li>
</ul>
<!-- ################################## SESSION 02 ############################################## -->
</div>
</div>
<div id="session-02-from-dns-to-toa" class="section level1">
<h1>Session 02: From DNs to TOA</h1>
<hr />
<div id="learning-goals-1" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Convert digital number values to top-of-atmosphere reflectance</li>
<li>Compare top-of-atmosphere reflectance to surface reflectance</li>
</ul>
<hr />
</div>
<div id="what-is-radiance-what-are-dns" class="section level2">
<h2>What is radiance / what are DNs?</h2>
<p>This session is the most technical of the entire course. Today we will be dealing with physical units, conversion, and data quality. Let´s kick it off with a short recap. Towards the end of the last exercise, you produced a plot of measurements in the different bands of a Landsat image, similar to this one:</p>
<div class="figure">
<img src="fig/s01_spectra.png" alt="Spectral profiles extracted from a Landsat Level 1 image" style="width:60.0%" />
<p class="caption">Spectral profiles extracted from a Landsat Level 1 image</p>
</div>
<p>Our y-axis label was “DN”, or digital number. Now, what is that again? Earth observing sensors, such as the ones on board the Landsat satellites register radiance at the top of the atmosphere. Radiance is expressed in watt per <a href="https://en.wikipedia.org/wiki/Steradian">steradian</a> per square meter.</p>
<p>Storing data in radiance units is difficult and therefore sensors translate measured radiance into DNs. Therefore, the range of energy measured by the sensor is broken into distinct units (DNs). Sensor-specific calibration determines the minimum and maximum amount of radiance that can be measured. DNs express the amount of radiance in relation to these sensor-specific calibration coefficients. The total number of possible DNs is what we refer to as the radiometric resolution of the sensor.</p>
<p>As an example, Landsat 4, 5, and 7 worked with a radiometric resolution of 8 bit. This allows for <code>2^8</code>, or 255 distinct DNs values. Landsat 8 works with 12 bit radiometric resolution, which is artificially quantized to 16 bit. The 16 bit resolution can represent much more different grey shades, exactly <code>2^16</code>, or 65,536 values. Let´s put this into code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(raster)

<span class="co"># Define the number of bits</span>
bit &lt;-<span class="st"> </span><span class="dv">4</span>

<span class="co"># How many grey tones can we represent given bit?</span>
<span class="kw">print</span>(<span class="kw">paste0</span>(bit, <span class="st">&#39; bits produce &#39;</span>, <span class="dv">2</span><span class="op">^</span>bit, <span class="st">&#39; grey tones.&#39;</span>))</code></pre></div>
<pre><code>[1] &quot;4 bits produce 16 grey tones.&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a raster with one line and 2^bit columns,</span>
<span class="co"># whereas cell values are filled with a vector of numbers from 1 2^bit</span>
r &lt;-<span class="st"> </span><span class="kw">raster</span>(<span class="dt">nrows =</span> <span class="dv">1</span>, <span class="dt">ncols =</span> <span class="dv">2</span><span class="op">^</span>bit, <span class="dt">vals =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span><span class="op">^</span>bit))

<span class="co"># Plot the image and assign grey-scale values to the cells</span>
<span class="kw">image</span>(r, <span class="dt">col =</span> <span class="kw">grey.colors</span>(<span class="dv">2</span><span class="op">^</span>bit, <span class="dt">start =</span> <span class="dv">0</span>, <span class="dt">end =</span> <span class="dv">1</span>), <span class="dt">main =</span> <span class="kw">paste0</span>(bit, <span class="st">&quot; bit raster&quot;</span>))</code></pre></div>
<p><img src="index_files/figure-html/greyscale-1.png" width="576" /></p>
<hr />
</div>
<div id="dns-to-toa" class="section level2">
<h2>DNs to TOA</h2>
<p>As DNs are dependent on the sensor calibration, identical measurements yield differing DNs across sensors. DNs are physically not meaningful. Instead of DNs, often we are interested in reflectance. Reflectance expresses the fraction of reflected radiance relative to the total incoming energy (sun), and is scaled between 0 and 1. Many applications require data converted to reflectance and/or corrected for the influence of the atmosphere.</p>
<ul>
<li>Sensor calibration (DN to radiance)</li>
<li>Conversion to top-of-atmosphere reflectance (radiance to TOA)</li>
<li>Atmospheric correction yielding bottom-of-atmosphere reflectance (TOA to BOA, or surface reflectance)</li>
</ul>
<p>A conversion of DNs into radiance can be easily undertaken. By accounting for sun-sensor geometries and solar irradiance, we can easily infer top-of-atmosphere reflectance (TOA) from radiance. This facilitates, e.g., a comparison of measurements from different sensors.</p>
<p>Practically, this involves a linear scaling of the DN values which uses two band-specific rescaling factors. One is multiplicative, one is additive. Formulas are explained in the <a href="https://www.usgs.gov/land-resources/nli/landsat/using-usgs-landsat-level-1-data-product">USGS guide for conversion to TOA Reflectance</a>. With Landsat 8 Collection 1 data, we can use a single set of coefficients to convert DNs to TOA reflectance. Note that this is not the case for all sensors.</p>
<p><span class="math inline">\(ρλ&#39; = M_ρ * Q_{cal} + A_ρ\)</span></p>
<p>where</p>
<p><span class="math inline">\(ρλ&#39;\)</span> = TOA planetary reflectance, without correction for solar angle.</p>
<p><span class="math inline">\(M_ρ\)</span> = Band-specific multiplicative rescaling factor from the metadata.</p>
<p><span class="math inline">\(Q_{cal}\)</span> = Quantized and calibrated standard product pixel values (DN).</p>
<p><span class="math inline">\(A_ρ\)</span> = Band-specific additive rescaling factor from the metadata.</p>
<p>In a next step, we can correct for solar angle during image acquisition.</p>
<p><span class="math inline">\(ρλ = ρλ&#39; / cos(θ_{SZ}) = ρλ&#39; / sin(θ_{SE})\)</span></p>
<p>where</p>
<p><span class="math inline">\(ρλ\)</span> = TOA planetary reflectance corrected for solar angle.</p>
<p><span class="math inline">\(θ_{SZ}\)</span> = Local solar zenith angle, whereas <span class="math inline">\(θ_{SZ} = 90° - θ_{SE}\)</span>.</p>
<p><span class="math inline">\(θ_{SE}\)</span> = Local sun elevation angle.</p>
<hr />
</div>
<div id="toa-to-boa" class="section level2">
<h2>TOA to BOA</h2>
<p>A final step towards comparable measurements of the Earth surface is the atmospheric correction. By doing atmospheric correction, we (theoretically) eliminate the influence of the atmosphere. We therefore call the product “surface reflectance” or “bottom of atmosphere reflectance” (BOA). Theoretically, the space-borne sensor´s measurements should be identical to ground-based measurements.</p>
<hr />
</div>
<div id="exercise-1" class="section level2">
<h2>Exercise</h2>
<p>During this exercise you will learn how to convert Landsat-8 Collection 1 DNs to TOA reflectance, and compare and characterize spectral appearance of different land cover types in TOA and BOA imagery. The data are provided in <a href="https://box.hu-berlin.de/f/97b7bd39ac6d48488f74/?dl=1">our repository</a>. After unpacking, you find the following folders:</p>
<ul>
<li><p>DN/LC08_L1TP_189025_20141105_20170417_01_T1/ - the directory containing the “L1TP” Landsat product (i.e. terrain-corrected (“orthorectified”) data in DNs), including the metadata file (MTL.txt) and the Quality Band (BQA.tif) which contains information on radiometric saturation, clouds, cloud shadows and more.</p></li>
<li><p>SR/LC081890252014110501T1-SC20170927102137/ – the directory containing Level-2 data, i.e. atmospherically corrected, or bottom-of-atmosphere data.</p></li>
</ul>
<div id="dn-to-toa-conversion" class="section level3">
<h3>1) DN to TOA conversion</h3>
<p>Familiarize yourself with the L1TP data. Visualize individual bands, have a look at the Level-1 metadata file (LC08_L1TP_ XXX_MTL.txt) and the information provided within. Again, we only use the six reflective bands of the OLI sensor in this exercise (3xVIS, nIR, 2xswIR)</p>
<p>Read the Landsat metadata file in R using <code>read.delim()</code> and extract the necessary information for the top-of-atmosphere conversion: - <code>REFLECTANCE_MULT_BAND</code> - <code>REFLECTANCE_ADD_BAND</code> - <code>SUN_ELEVATION</code></p>
<p>Use the pattern matching function <code>grep()</code> to find the corresponding entries in the metadata file and store them as numeric vectors. Do this as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define the folder that contains your data...</span>
path &lt;-<span class="st"> &#39;course.dir/S02/data/DN&#39;</span>

<span class="co"># Read MTL file</span>
mtl &lt;-<span class="st"> </span><span class="kw">list.files</span>(data.path, <span class="dt">pattern=</span><span class="st">&quot;MTL.txt$&quot;</span>, <span class="dt">recursive=</span>T, <span class="dt">full.names=</span>T)
mtl.txt &lt;-<span class="st"> </span><span class="kw">read.delim</span>(mtl, <span class="dt">sep =</span> <span class="st">&#39;=&#39;</span>, <span class="dt">stringsAsFactors =</span> F)

<span class="co"># Extract numeric values</span>
REFLECTANCE_MULT_BAND &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(mtl.txt[<span class="kw">grep</span>(<span class="st">&quot;REFLECTANCE_MULT_BAND&quot;</span>,mtl.txt<span class="op">$</span>GROUP),][<span class="dv">2</span><span class="op">:</span><span class="dv">7</span>,<span class="dv">2</span>])</code></pre></div>
<p>Convert the DN values to TOA following the “Conversion to TOA Reflectance” section in the <a href="https://www.usgs.gov/land-resources/nli/landsat/using-usgs-landsat-level-1-data-product">USGS guide for conversion to TOA Reflectance</a>. Important things to keep in mind:</p>
<ul>
<li>You can apply the conversion to all bands in a stack simultaneously.</li>
<li>The Landsat data is provided as integer values. When applying the equation, the data will be cast to float. You will have to re-convert to integer at some point. Use a reflectance scaling factor of 10,000 during the conversion.</li>
<li>The sun elevation angle provided in the metadata file is reported in degrees. The <code>sin()</code> function expects that angles are provided in radians. For the conversion to work correctly, you need to convert <code>SUN_ELEVATION</code> into radians. Make use of this helper function:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Helper-function to convert degrees to radians</span>
deg2rad &lt;-<span class="st"> </span><span class="cf">function</span>(deg){ (deg <span class="op">*</span><span class="st"> </span>pi) <span class="op">/</span><span class="st"> </span>(<span class="dv">180</span>) }</code></pre></div>
<p>Write the result of your TOA conversion to disk in the <code>GTiff</code> format. Make sure to use an integer data type to save disk space.</p>
</div>
<div id="compare-toa-with-boa" class="section level3">
<h3>2) Compare TOA with BOA</h3>
<p>Open the BOA and the TOA file in QGIS and visually assess the differences in the spectral signature between the SR and TOA files. Use the info tool´s graph view to investigate the spectral signatures of each of the following land cover types:</p>
<ul>
<li>Deciduous forest</li>
<li>Coniferous forest</li>
<li>Grassland</li>
<li>Cropland</li>
<li>Impervious surfaces</li>
<li>Water</li>
</ul>
<p>In your R script, take notes on the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li><p>What are the most evident differences between TOA and SR reflectance spectra?</p></li>
<li><p>Briefly summarize the spectral appearance of the six land cover types, and the main difference between the TOA and SR.</p></li>
</ol>
<hr />
</div>
</div>
</div>
<div id="session-02-data-quality-cloud-masking" class="section level1">
<h1>Session 02: Data quality &amp; cloud masking</h1>
<div id="learning-goals-2" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Understand how to use the Landsat Collection 1 quality bands</li>
<li>Produce cloud / cloud shadow masks of differing confidence levels</li>
<li>Mask Landsat images</li>
</ul>
<hr />
</div>
<div id="landsat-data-quality" class="section level2">
<h2>Landsat data quality</h2>
<p>Think about the following question and exchange with your neighbor: Which issues affect the data quality of optical satellite images?</p>
<p>How to find out if a pixel is affected by any of these issues? Luckily, the USGS provides quality bands for the Landsat Collection 1 products. If we look up the <a href="https://www.usgs.gov/land-resources/nli/landsat/landsat-collection-1-level-1-quality-assessment-band">Landsat QA band website</a> to find out what this band contains, we find the following information:</p>
<p><em>“Each pixel in the QA band contains unsigned integers that represent bit-packed combinations of surface, atmospheric, and sensor conditions that can affect the overall usefulness of a given pixel.”</em></p>
<div class="figure">
<img src="fig/s02_qa_band.png" alt="False-color representation of a cloudy Landsat Level 1 image (left) and a grey-scale visualization of the Landsat QA band (right)" style="width:60.0%" />
<p class="caption">False-color representation of a cloudy Landsat Level 1 image (left) and a grey-scale visualization of the Landsat QA band (right)</p>
</div>
<p>In the following, you will learn to understand what that means. Please read this section carefully and exchange with your neighbor.</p>
<div id="the-binary-system" class="section level3">
<h3>The binary system</h3>
<p>Remember the binary numeral system (0 / FALSE or 1 / TRUE)?</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/7/75/Binary_counter.gif" alt="The binary number system. Source: Wikipedia" style="width:100.0%" />
<p class="caption">The binary number system. Source: Wikipedia</p>
</div>
<p>The binary system represents numbers through sequences of bits. Each bit has a position and a state. The positions are numbered from 0 to the total number of bits, read from right to left. The state of a bit can be either, 0, or 1. Consider the meaning of 0 as „FALSE“ or „NO“, 1 equals „TRUE“ or „YES“.</p>
<p>In the example above, five bit positions are given. Each position is sequentially numbered, starting with 0, read from right to left. By calculating 2 to the power of the bit position, we receive the value of each bit. For all bits with the value 1, we sum the respective values. Different combinations of states and positions therefore enable us to represent integer numbers.</p>
<p>The more positions or bits we have the higher numbers we can generate. With 5 bits, we can represent the numbers 0 – 31 (32 unique values), with 16 bits, we can represent the numbers 0 - 65,535 (65,536 unique values). Following the example with 5 bits, we can for instance represent these numbers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 00001</span>
<span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">0</span></code></pre></div>
<pre><code>[1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 00111</span>
<span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">0</span></code></pre></div>
<pre><code>[1] 7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 11111</span>
<span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">^</span><span class="dv">0</span></code></pre></div>
<pre><code>[1] 31</code></pre>
</div>
<div id="landsat-qa-band" class="section level3">
<h3>Landsat QA band</h3>
<p>The Landsat QA band is coded in 16 bit, which are arranged from right to left. Each bit has a specific meaning in terms of data quality e.g. bit 4 (or position 5 from the right) means „cloud“. If bit 4 has the state 1 (or”YES“, or”TRUE“), there was a cloud detected in that pixel. If bit 4 has the state 1, the resulting integer value will be 2^4 = 16. Following this logic, different combinations of true / false conditions result in different integer values.</p>
<div class="figure">
<img src="fig/s02_landsat_qa.png" alt="Landsat QA band bit designation" style="width:80.0%" />
<p class="caption">Landsat QA band bit designation</p>
</div>
<p>In the Landsat QA band, there are furthermore single and double bits. Single bits inform about one condition in a binary manner:</p>
<p>Single bits (0, 1, and 4): - 0 = “No” = This condition does not exist - 1 = “Yes” = This condition exists</p>
<p>Double bits can represent conditions in more detail. Some double bits (5-6, 7-8, 9-10, 11-12, read from left to right) represent levels of confidence that a condition exists:</p>
<ul>
<li>00 = “Not Determined” / This condition does not exist</li>
<li>01 = “Low” = Algorithm has low to no confidence that this condition exists (0-33 percent confidence)</li>
<li>10 = “Medium” = Algorithm has medium confidence that this condition exists (34-66 percent confidence)</li>
<li>11 = “High” = Algorithm has high confidence that this condition exists (67-100 percent confidence)</li>
</ul>
<p>The radiometric saturation bits (2-3), read from left to right, represent how many bands contain radiometric saturation:</p>
<ul>
<li>00 - No bands contain saturation</li>
<li>01 - 1-2 bands contain saturation</li>
<li>10 - 3-4 bands contain saturation</li>
<li>11 - 5 or more bands contain saturation</li>
</ul>
<hr />
</div>
</div>
<div id="de-coding-quality-band-values" class="section level2">
<h2>De-coding quality band values</h2>
<p>Let´s look at the integer value 2804 to illustrate how this works. R offers a good way of dealing with bit-packed information. The <code>intToBits()</code> function converts integer numbers into sequences of bits, whereas 32 double-digit bits are returned by default.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Convert integer to bit sequence</span>
<span class="kw">intToBits</span>(<span class="dv">2804</span>)</code></pre></div>
<pre><code> [1] 00 00 01 00 01 01 01 01 00 01 00 01 00 00 00 00 00 00 00 00 00 00 00
[24] 00 00 00 00 00 00 00 00 00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Convert the double-digit into single-digit bits</span>
<span class="kw">as.numeric</span>(<span class="kw">intToBits</span>(<span class="dv">2804</span>))</code></pre></div>
<pre><code> [1] 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We´re looking at 16 bit data, so let´s deprecate the unused bits</span>
<span class="kw">as.numeric</span>(<span class="kw">intToBits</span>(<span class="dv">2804</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">16</span>])</code></pre></div>
<pre><code> [1] 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Bit sequences are read from right to left, so we need to reverse the order</span>
<span class="kw">rev</span>(<span class="kw">as.numeric</span>(<span class="kw">intToBits</span>(<span class="dv">2804</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">16</span>]))</code></pre></div>
<pre><code> [1] 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0</code></pre>
<p>Now we can compare with the Landsat 8 Collection 1 Level 1 QA band bit designation. What´s the pixel quality information of the integer value 2804?</p>
<div class="figure">
<img src="fig/s02_landsat_qa_decode.PNG" alt="Landsat QA band bit designation" style="width:70.0%" />
<p class="caption">Landsat QA band bit designation</p>
</div>
<p>We are looking at a cloudy pixel with radiometric saturation affecting 1-2 bands.</p>
<hr />
</div>
<div id="exercise-2" class="section level2">
<h2>Exercise</h2>
<div id="investigating-the-qa-band" class="section level3">
<h3>1) Investigating the QA band</h3>
<p>Let´s have a look at the Landsat Collection 1 Quality Band (BQA). First, load the BQA band and find the three most frequent values using <code>freq()</code>.</p>
<p>As you can see, the band contains integer values. By decoding the integer values into 16 bit binary strings, we can read the quality information for each pixel. Use the <code>intToBits()</code> function to decode the three most frequent BQA values and decipher their meaning using the Landsat quality band documentation. <code>intToBits()</code> returns 32 bits by default. Make sure you only look at bits 1 to 16. Also, keep in mind the right-to-left order when comparing the decoded bits with the table. You might want to use <code>rev()</code> to invert the order of the outputs by <code>intToBits()</code>. Note your findings as a comment in the script:</p>
<pre><code># What do the most frequent values mean? Decode each integer value into bits and 
# describe their meaning here: 

# Most frequent value: 
# Second most frequent value: 
# Third most frequent value: </code></pre>
</div>
<div id="creating-a-cloud-mask" class="section level3">
<h3>2) Creating a cloud mask</h3>
<p>By converting integer values into binary bits, we can extract specific attributes from the BQA. Let´s to this in a systematic manner by defining a function, which yields TRUE for binary codes with bit 0 = 1:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define function to find fill values from Landsat BQA</span>
fill_pixels &lt;-<span class="st"> </span><span class="cf">function</span>(x) {<span class="kw">intToBits</span>(x)[<span class="dv">1</span>] <span class="op">==</span><span class="st"> </span>T}</code></pre></div>
<p>Next, use indexing and Boolean expressions to define functions which return TRUE for</p>
<ol style="list-style-type: lower-alpha">
<li>high confidence clouds or high confidence cloud shadows or fill values</li>
<li>high and medium confidence clouds or high and medium confidence cloud shadows or fill values</li>
</ol>
<p>Create a mask using the above functions. You can use <code>calc()</code>. Plot the mask and check if clouds, cloud shadows, and fill values are labeled as 1 and clear observations as 0. Write both masks to disk.</p>
<p>Open both masks in QGIS, together with an RGB representation of the image. Which mask is more accurate?</p>
</div>
<div id="masking-images" class="section level3">
<h3>3) Masking images</h3>
<p>Next, load the BOA data as a stack (VIS, nIR, swIR) and use the <code>mask()</code> function to mask clouds, cloud shadows and fill values from the image. Use the mask which you found to be more accurate. Make sure to specify the maskvalue argument accordingly. Write the masked BOA stack to disk in the <code>GTiff</code> format.</p>
</div>
</div>
<div id="reading-materials-1" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following paper:</p>
<p><a href="https://doi.org/10.1016/j.rse.2013.04.022">Griffiths, P. et al. (2014). Forest disturbances, forest recovery, and changes in forest types across the Carpathian ecoregion from 1985 to 2010 based on Landsat image composites. Remote Sensing of Environment, 151, 72–88.</a></p>
<p>In this paper, a long time series of Landsat image composites at five year intervals is used to study the dynamics of forest disturbance, recovery and changes in forest types across the Carpathian ecoregion. Please make sure to read the paper thoroughly and focus on the following broad questions:</p>
<ul>
<li>What is the motivation for this article?</li>
<li>How was it done in principle?</li>
<li>What are the key findings?</li>
<li>Are there uncertainties related to the findings?</li>
</ul>
<hr />
<!-- ################################## SESSION 03 ############################################## -->
</div>
</div>
<div id="session-03-vegetation-indices-and-data-transforms" class="section level1">
<h1>Session 03: Vegetation indices and data transforms</h1>
<div id="learning-goals-3" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Learn how to calculate NDVI, EVI and Tasseled Cap components</li>
</ul>
</div>
<div id="spectral-behavior-of-vegetation" class="section level2">
<h2>Spectral behavior of vegetation</h2>
<p>Vegetation produces a distinct spectral reflectance pattern due to its leaf and cell structure, its physiognomy, and complex stand structure. Photosynthetically inactive plant parts differ considerably from active ones across different wavelength regions. The reflectance of photosynthetically active vegetation is characterized by different factors in the VIS, nIR and SWIR:</p>
<ul>
<li><p>VIS – leaf pigments - In the visible bands the reflectance is relatively low as the majority of light is absorbed by the leaf pigments. Chlorophyll strongly absorbs energy in the blue and red wavelengths and reflects more in the green parts of the spectrum. This is why healthy vegetation appears green to the human eye.</p></li>
<li><p>nIR – cell structure - For healthy vegetation, the reflectance is much higher in the near infrared (NIR) region than in the visible region due to the cellular structure of the leaves, specifically the spongy mesophyll. Therefore healthy vegetation can be easily identified by the high NIR reflectance and generally low visible reflectance.</p></li>
<li><p>SWIR – water content - The reflectance in the shortwave infrared wavelengths is related to the water content of the vegetation and its structure. Water has strong absorption around 1.45, 1.95 and 2.50 µm . Outside these absorption bands in the SWIR region, reflectance of leaves generally increases when water content in the leaf decreases.</p></li>
</ul>
<div class="figure">
<img src="fig/s03_vegetation_spectrum.png" alt="Spectral reflectance curve of vegetation. Source: gsp.humboldt.edu" style="width:80.0%" />
<p class="caption">Spectral reflectance curve of vegetation. Source: gsp.humboldt.edu</p>
</div>
</div>
<div id="vegetation-indices" class="section level2">
<h2>Vegetation indices</h2>
<p>Vegetation indices make use of this particular reflectance signal. Most commonly known are the Normalized Difference Vegetation Index (NDVI) and the Enhanced Vegetation Index (EVI).</p>
<hr />
<div id="normalized-difference-vegetation-index-ndvi" class="section level3">
<h3>Normalized Difference Vegetation Index (NDVI)</h3>
<p>The NDVI relates the difference between the nIR and red reflectance to their sum.</p>
<p><span class="math inline">\(NDVI = (nIR – red) / (nIR + red)\)</span></p>
<p>For instance, a red reflectance of 10% and a nIR reflectance of 50% result in</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">red &lt;-<span class="st"> </span><span class="fl">0.1</span>
nIR &lt;-<span class="st"> </span><span class="fl">0.5</span>

ndvi &lt;-<span class="st"> </span>(nIR <span class="op">-</span><span class="st"> </span>red) <span class="op">/</span><span class="st"> </span>(nIR <span class="op">+</span><span class="st"> </span>red)
<span class="kw">print</span>(ndvi)</code></pre></div>
<pre><code>[1] 0.6666667</code></pre>
<p>The NDVI is not a physical measure, but a proxy integrating different factors, such as land use / cover, incl. the amount of background signal visible in a pixel, photosynthetic activity, vitality and overall vegetation condition. It relates well to vegetation density and structure, e.g., represented by the leaf area index (LAI)</p>
<hr />
</div>
<div id="enhanced-vegetation-index-evi" class="section level3">
<h3>Enhanced Vegetation Index (EVI)</h3>
<p>The EVI often has a better correlation with biomass than NDVI, specifically in vegetation canopies with low and high LAI values.</p>
<div class="figure">
<img src="fig/s03_ndvi_evi.PNG" alt="NDVI and EVI from MODIS image composites (5-20 March 2000). Source: http://earthobservatory.nasa.gov/" style="width:100.0%" />
<p class="caption">NDVI and EVI from MODIS image composites (5-20 March 2000). Source: <a href="http://earthobservatory.nasa.gov/" class="uri">http://earthobservatory.nasa.gov/</a></p>
</div>
<p><span class="math inline">\(EVI = G * ((nIR – red) / (nIR + (C1 * red – C2 * blue) + L))\)</span></p>
<p>The EVI aims at reducing saturation effects which are common for NDVI. It includes a correction for soil background effects (L) to improve sensitivity for low density vegetation canopies. It is less sensitive to high aerosol loads, since the additional coefficients (C1 and C2) steer the aerosol resistance term, and the visible blue reflectance is used to correct for scattering that also affects the visible red.</p>
<p>Indices enhance differences in the reflectance to highlight certain features. Vegetation indices have the advantage of being simple, but the disadvantage of disregarding parts of the spectral feature space. Linear transformations, such as the Tasseled Cap Transformation, can help to overcome this limitation.</p>
<hr />
</div>
</div>
<div id="tasseled-cap-transformation" class="section level2">
<h2>Tasseled Cap Transformation</h2>
<p>The Tasseled Cap Transformation (TC) is a linear transformation of the Landsat spectral bands. It was first presented in 1976 by R.J. Kauth and G.S. Thomas of Environmental Research Institute of Michigan in an article titled “The Tasseled Cap – A Graphic Description of the Spectral-Temporal Development of Agricultural Crops as Seen by Landsat.” The TC was thus developed for analyzing agricultural lands with Landsat MSS data. The name „Tasseled Cap“ was chosen because the of the shape of phenological trajectories of crops in the nIR ~ red feature space.</p>
<div class="figure">
<img src="fig/s03_tc_concept.png" alt="Crop phenological trajectories in near infrared ~ red featurespace. Band numbers relate to Landsat MSS bands, where band 3 (6) is the near infrared and band 2 (5) is the red band." style="width:40.0%" />
<p class="caption">Crop phenological trajectories in near infrared ~ red featurespace. Band numbers relate to Landsat MSS bands, where band 3 (6) is the near infrared and band 2 (5) is the red band.</p>
</div>
<p>We can observe these trajectories by producing near infrared ~ red scatterplots for different points in time.</p>
<div class="figure">
<img src="fig/s03_sct_cln.gif" alt="Animated scatterplot of 10,000 locations of an agricultural system in southeastern Turkey in near infrared ~ red featurespace, observed within the course of one year (2015)." style="width:50.0%" />
<p class="caption">Animated scatterplot of 10,000 locations of an agricultural system in southeastern Turkey in near infrared ~ red featurespace, observed within the course of one year (2015).</p>
</div>
<p>The concept was further developed for the use with other sensors, including Landsat TM, ETM+ and OLI. It is still widely used for different applications in the context of urban, agricultural, or forest-related remote-sensing studies. Resulting from the TC, we mostly analyze three components called Brightness, Greenness, and Wetness:</p>
<ul>
<li><em>Brightness</em>: an axis along the line of soils, indicating soil brightness.</li>
<li><em>Greenness</em>: axis is perpendicular to the soil line, emphasizes near infrared and hence vegetation.</li>
<li><em>Wetness</em>: emphasizes shortwave infrared and is thereby related to water content.</li>
</ul>
<hr />
</div>
<div id="exercise-3" class="section level2">
<h2>Exercise</h2>
<p>We provide the following datasets in <a href="https://box.hu-berlin.de/f/7bb994e5cb414bf49940/?dl=1">our repository</a>:</p>
<p>…sr_data/: Four cloud-masked image chips from Landsat 8 (surface reflectance):</p>
<ul>
<li>LC081890252014031001T1-SC20170927101754 (10 March 2014)</li>
<li>LC081890252014071601T1-SC20171024094741 (16 July 2014)</li>
<li>LC081890252015082001T1-SC20170927120710 (20 August 2015)</li>
<li>LC081890252014110501T1-SC20170927102137 (05 November 2014)</li>
</ul>
<div id="compute-vegetation-indices" class="section level3">
<h3>1) Compute vegetation indices</h3>
<p>Read the March surface reflectance stack in R and calculate NDVI as well as EVI. The correction factors for calculating EVI from Landsat data are:</p>
<ul>
<li><p><span class="math inline">\(G = 2.5\)</span></p></li>
<li><p><span class="math inline">\(C1 = 6\)</span></p></li>
<li><p><span class="math inline">\(C2 = 7.5\)</span>, and</p></li>
<li><p><span class="math inline">\(L = 1\)</span>.</p></li>
</ul>
<p>Always be aware that the reflectance values in our datasets is scaled by 10,000. This will not make a difference when computing the NDVI, as we are only looking at relative differences. However, for the EVI, <span class="math inline">\(L\)</span> must be scaled by 10,000. Also, keep in mind that we commonly want to store datasets in <code>INT2S</code> data type, and therefore we need to scale our results by 10,000. Consequently, we compute the EVI from Landsat data as follows:</p>
<p><code>evi &lt;- 2.5 * ((nIR – red) / (nIR + 6 * red – 7.5 * blue + 10000))</code></p>
<p>While the computation is ongoing, proceed with preparing the code for the next task. When the computation is done, write the results to disk, using the <code>INT2S</code> data type.</p>
</div>
<div id="perform-a-tasseled-cap-transformation" class="section level3">
<h3>2) Perform a Tasseled Cap transformation</h3>
<p>Perform the TC using the March surface reflectance stack. We use the coefficients derived by <a href="https://www.sciencedirect.com/science/article/pii/0034425785901026">Crist (1985)</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tcc &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>( <span class="fl">0.2043</span>,  <span class="fl">0.4158</span>,  <span class="fl">0.5524</span>, <span class="fl">0.5741</span>,  <span class="fl">0.3124</span>,  <span class="fl">0.2303</span>, 
                <span class="op">-</span><span class="fl">0.1603</span>, <span class="op">-</span><span class="fl">0.2819</span>, <span class="op">-</span><span class="fl">0.4934</span>, <span class="fl">0.7940</span>, <span class="op">-</span><span class="fl">0.0002</span>, <span class="op">-</span><span class="fl">0.1446</span>,
                 <span class="fl">0.0315</span>,  <span class="fl">0.2021</span>,  <span class="fl">0.3102</span>, <span class="fl">0.1594</span>, <span class="op">-</span><span class="fl">0.6806</span>, <span class="op">-</span><span class="fl">0.6109</span>), 
                <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;green&#39;</span>, <span class="st">&#39;red&#39;</span>, <span class="st">&#39;nIR&#39;</span>, <span class="st">&#39;swIR1&#39;</span>, <span class="st">&#39;swIR2&#39;</span>), <span class="kw">c</span>(<span class="st">&#39;brightness&#39;</span>, <span class="st">&#39;greenness&#39;</span>, <span class="st">&#39;wetness&#39;</span>)),
                <span class="dt">ncol =</span> <span class="dv">3</span>)

<span class="kw">print</span>(tcc)</code></pre></div>
<pre><code>      brightness greenness wetness
blue      0.2043   -0.1603  0.0315
green     0.4158   -0.2819  0.2021
red       0.5524   -0.4934  0.3102
nIR       0.5741    0.7940  0.1594
swIR1     0.3124   -0.0002 -0.6806
swIR2     0.2303   -0.1446 -0.6109</code></pre>
<p>The TC is a linear band transformation. We can simply multiply the individual bands in the march stack with the corresponding factor and sum up the result, e.g., for TC Brightness:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">brightness &lt;-<span class="st"> </span>march.stack[[<span class="dv">1</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">2</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">3</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">3</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">4</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">4</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">5</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">5</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>march.stack[[<span class="dv">6</span>]] <span class="op">*</span><span class="st"> </span>tcc[<span class="dv">6</span>,<span class="dv">1</span>]</code></pre></div>
<p>Make sure to create an individual layer for Brightness, Greenness, and Wetness. Create a stack from the three layers. While the computation is ongoing, proceed with task 1) of the block on training data collection.</p>
<p>When the computation is completed, write the results to disk, using the <code>INT2S</code> data type. Add the NDVI, EVI, and the TC stack to the QGIS project you started in the next block. Visually explore the vegetation indices and TC components.</p>
<hr />
</div>
</div>
</div>
<div id="session-03-training-data-collection" class="section level1">
<h1>Session 03: Training data collection</h1>
<div id="learning-goals-4" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Gather training data for a broad forest type classification</li>
<li>Understand how forest types differ spectrally</li>
</ul>
<hr />
</div>
<div id="background" class="section level2">
<h2>Background</h2>
<p>Collecting training is an essential step on your way to a classified map. The training pixels will be considered representative for the classes you want to map, as classification algorithms determine class labels for unknown pixels based on their similarity to the training dataset.</p>
<div class="figure">
<img src="fig/s03_training.PNG" alt="Training points for forest type classification and resulting map output" style="width:80.0%" />
<p class="caption">Training points for forest type classification and resulting map output</p>
</div>
<p>Collecting training data is time consuming, regardless if you are collecting in the field or digitally. Small conceptual mistakes may require a revision of your training dataset. As a consequence, training data collection should be well prepared. Consider the following points.</p>
<ul>
<li><p>A precise and robust definition of your target classes based on the study region characteristics is key. Targeting a high thematic detail is beneficial, but spectral(-temporal) similarities between classes might pose limitations to a robust distinction of classes, such as tree species or crop types. In such cases, it is advised to think about a hierarchical structure to aggregate similar classes into higher level classes, such as forest types, or annual / perennial croplands.</p></li>
<li><p>Gathering as much reference information as possible. Can we find additional datasets that guide our interpretation? Is any very high resolution (VHR) imagery available? GoogleEarth is a valuable source of VHR imagery, but it is critical to account for the exact acquisition date.</p></li>
<li><p>Good knowledge of the target classes, and their spectral (temporal) characteristics in the study region is beneficial. We should consider spectrally similar classes and identify potential ways to prevent confusion, e.g., by aggregating those classes or identifying spectral features which help to separate them better.</p></li>
<li><p>A purely random point sampling is not neccessarily the best option (different from collecting independent validation data), as we might want to train small classes that are hardly captured by a random sample. Manual selection of training points is advised to circumvent this problem.</p></li>
<li><p>The image below shows the spatial distribution of six training datasets collected during an earlier iteration of the course. Some training points cluster in a subset of the study region. Ideally, however, training data should be well distributed across the study region to cover regional biophysical variability, such as different soil types, weather patterns, or topography.</p></li>
</ul>
<div class="figure">
<img src="fig/s03_train_dist.PNG" alt="Spatial distribution of six training datasets" />
<p class="caption">Spatial distribution of six training datasets</p>
</div>
<ul>
<li><p>The classification algorithm of your choice might have specific requirements towards the training data, e.g., concerning the number of samples, their distribution in the spectral feature space, or their purity (pure vs. mixed pixel). We discuss these aspects later in the course.</p></li>
<li><p>In practice, it´s important to know your training data well. Are the classes separable with the data at hand? Are essential class characteristics well represented? Are there any outliers? To learn more, it is always wise to explore the spectral characteristics of your training data points. We can do this through investigating the spectral reflectances at our training data locations (e.g., through histograms / boxplots) and comparing them between classes. That´s what we want to do today.</p></li>
</ul>
<hr />
</div>
<div id="exercise-4" class="section level2">
<h2>Exercise</h2>
<p>This exercise has two larger aims. First, you will learn to collect training data for a broad forest type classification. We provide forestry data to find representative sample pixels in QGIS. We will use the data you generate in this exercise for classification in the next session. Second, you will learn how the broad forest types appear spectrally in images acquired in different parts of the growing season.</p>
<p>We provide the following datasets in <a href="https://box.hu-berlin.de/f/7bb994e5cb414bf49940/?dl=1">our repository</a>:</p>
<p>…sr_data/: Four cloud-masked image chips from Landsat 8 (surface reflectance):</p>
<ul>
<li>LC081890252014031001T1-SC20170927101754 (10 March 2014)</li>
<li>LC081890252014071601T1-SC20171024094741 (16 July 2014)</li>
<li>LC081890252015082001T1-SC20170927120710 (20 August 2015)</li>
<li>LC081890252014110501T1-SC20170927102137 (05 November 2014)</li>
</ul>
<p>…vector/: A shapefile and a *.kmz file for GoogleEarth, which will help you to accurately delineate the Landsat pixel locations and extents for training data collection.</p>
<p>…BDL/: Forestry data collected in 2015 which is publicly available <a href="https://www.bdl.lasy.gov.pl/portal/wniosek-en">here</a>. We prepared a shapefile for the use in this session.</p>
<p>It contains the following attributes:</p>
<table>
<thead>
<tr class="header">
<th>Attribute field</th>
<th>Definition</th>
<th>Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>species_en</td>
<td>Dominant genus in each stand</td>
<td>Ash, Beech, Fir, Spruce…</td>
</tr>
<tr class="even">
<td>part_cd</td>
<td>Share of this genus within the stand</td>
<td>0 – 100 (in %)</td>
</tr>
<tr class="odd">
<td>spec_age</td>
<td>Average age of the trees in this stand</td>
<td>Age in years</td>
</tr>
</tbody>
</table>
<div id="prepare-the-training-data-collection" class="section level3">
<h3>1) Prepare the training data collection</h3>
<p>Visualize and arrange all abovementioned datasets in QGIS. Ask us for help if you´re not familiar with QGIS. Consider the following steps:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Find a good <a href="https://www.harrisgeospatial.com/Learn/Blogs/Blog-Details/ArtMID/10198/ArticleID/15691/The-Many-Band-Combinations-of-Landsat-8">false-color representation</a> of the Landsat 8 bands to highlight vegetation.</p></li>
<li><p>Visualize the forestry data by choosing distinct colors for the different tree genera (species_en).</p></li>
</ol>
<p>Which genera are dominant in the study area?</p>
<p>Generate a new point shapefile for storing the training data you will collect in the next task. It should contain the attribute fields ‘classID’ and ‘confID’ (both of type integer).</p>
<p>Watch out - make sure the shapefile has the same spatial reference system as the Landsat data.</p>
</div>
<div id="collect-training-data" class="section level3">
<h3>2) Collect training data</h3>
<p>Switch into the editing mode to locate training points and assign the corresponding class and confidence number. Please collect at least 15 pixels per class and assign them the class numbers and confidence numbers given below.</p>
<table>
<thead>
<tr class="header">
<th>Class name</th>
<th>classID</th>
<th>Confidence level</th>
<th>confID</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deciduous forest</td>
<td>1</td>
<td>Very certain</td>
<td>1</td>
</tr>
<tr class="even">
<td>Mixed forest</td>
<td>2</td>
<td>Some uncertainties</td>
<td>2</td>
</tr>
<tr class="odd">
<td>Coniferous forest</td>
<td>3</td>
<td>Very uncertain</td>
<td>3</td>
</tr>
<tr class="even">
<td>Non-forest</td>
<td>4</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Use the multi-temporal Landsat imagery, the forestry polygons and very high resolution imagery in GoogleEarth to identify training points. The historic imagery tool in Google Earth can be extremely useful to guide your interpretation between deciduous and evergreen trees, as it contains imagery from the leaf-off phenological phase. The Landsat grid shapefile and .kmz will help you to identify and label the precise training locations for the four classes.</p>
<p>Regularly save the collected points and store the final shapefile with 60+ points in your course folder.</p>
</div>
<div id="explore-your-training-data" class="section level3">
<h3>3) Explore your training data</h3>
<p>Load your shape in R using <code>readOGR()</code>. Extract the spectral values at your point locations from the March image in R using the <code>extract()</code> function. Specify <code>sp = TRUE</code> to append the spectral values to the point shapefile.</p>
<p>Make sure the result of this task is an object of type <code>data.frame</code> named <code>sr.march</code>. Your sample points should be represented as rows and the measured variables as columns (i.e. point id, classID, confID, and 6 spectral bands).</p>
<p>Create boxplots of your surface reflectance measurements for all spectral bands, grouped according to the class number.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load required packages</span>
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(reshape2)

<span class="co"># Melt dataframe containing point id, classID, confID, and 6 spectral bands</span>
spectra.df &lt;-<span class="st"> </span><span class="kw">melt</span>(sr.march, <span class="dt">id.vars=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>), <span class="dt">measure.vars=</span><span class="kw">c</span>(<span class="dv">4</span><span class="op">:</span><span class="dv">9</span>))

<span class="co"># Create boxplots of spectral bands per class</span>
<span class="kw">ggplot</span>(spectra.df, <span class="kw">aes</span>(<span class="dt">x=</span>variable, <span class="dt">y=</span>value, <span class="dt">color=</span>classID)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() </code></pre></div>
<p>Make sure you understand what the <code>melt()</code> function is doing. Feel free to adjust the plot layout.</p>
<p>Similarly to the previous task, extract the values at your point locations from the Tasseled Cap stack. Create boxplots of the three Tasseled Cap components, grouped by the target class. Investigate the boxplots in order to investigate the differences between your target classes. Try to answer the following questions:</p>
<ul>
<li>Do the Tasseled Cap components allow for discriminating your target classes?</li>
<li>Which classes are likely difficult to separate?</li>
</ul>
<p><em>Voluntary exercise</em>: If you´re keen on exploring spectral changes over time, repeat the above procedures for the remaining images (July, August, November).</p>
<hr />
</div>
</div>
<div id="reading-materials-2" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following paper:</p>
<p><a href="https://www.researchgate.net/publication/259486369_A_Pixel-Based_Landsat_Compositing_Algorithm_for_Large_Area_Land_Cover_Mapping">Griffiths et al. (2013): Pixel-Based Landsat Compositing for Large Area Land Cover Mapping. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 6(5), 2088-2101.</a></p>
<p>This paper describes the methods used in the article you read last week. Here, a novel algorithm for large-area pixel-based compositing from Landsat data was developed. Please read the paper thoroughly, make sure you understand the underlying concept and write down any question for the discussion in our next session. Also, answer the following broad questions:</p>
<ul>
<li>What is the motivation for this article?</li>
<li>How does the algorithm work in principle?</li>
<li>Which key parameters were used for the parametric scoring?</li>
<li>What is the difference between annual and seasonal consistency?</li>
</ul>
<hr />
<!-- ################################## SESSION 04 ############################################## -->
</div>
</div>
<div id="session-04-pixel-based-compositing" class="section level1">
<h1>Session 04: Pixel-based compositing</h1>
<hr />
<div id="learning-goals-5" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Understand the fundamentals of pixel-based image compositing</li>
<li>Parameterize a compositing function and create your own composite from multiple Landsat images</li>
</ul>
<hr />
</div>
<div id="background-1" class="section level2">
<h2>Background:</h2>
<p>The availability of Landsat images varies by region. Even in areas where obersvation density is high, such as North America, coverage can be reduced by clouds and quality of observations can be impacted by haze or sensor saturation. <img src="fig/s04_landsat_availability.jpg" alt="Global availability of Landsat images. Source: Wulder et al. 2016; https://doi.org/10.1016/j.rse.2015.11.032" /></p>
<p>Using pixel-based-compositing, we can integrate multiple obervations (i.e., from multiple Landsat images) and combine them to one consistent image composite. One of the first global Landsat composites was produced using the WELD compositing approach (Roy et al. 2011, <a href="https://doi.org/10.1016/j.rse.2009.08.011" class="uri">https://doi.org/10.1016/j.rse.2009.08.011</a>). <img src="fig/s04_global_weld.jpg" alt="Monthly global WELD surface reflectance composite for June 2009. Source: https://worldview.earthdata.nasa.gov/" /></p>
<p>Pixel-based compositing allows us to create cloud-free, radiometrically and phenologically consistent image composites that are contiguous over large areas. A set of parameters is used to determine the observations best suited for analysis. These parameters can, e.g., include the distance to clouds in the image, or the temporal proximity to a target day of the year. Defining scoring functions allows for a flexible parametrization according to user´s needs and study area characteristics. Different parameters can also be weighted according to their relevance for a given application.</p>
<div class="figure">
<img src="fig/s04_compositing.png" alt="Creating one pixel-based composite from three images." />
<p class="caption">Creating one pixel-based composite from three images.</p>
</div>
<hr />
</div>
<div id="best-pixel-based-compositing-in-seven-steps" class="section level2">
<h2>Best-pixel-based compositing in seven steps</h2>
<ol style="list-style-type: decimal">
<li><p>Determine compositing parameters, e.g. Target DOY: June 15 +/- 30 days and target year: 2015 +/- 1 year</p></li>
<li><p>Calculate DOY and year suitability (0-1) according to parameters</p></li>
</ol>
<div class="figure">
<img src="fig/s04_suitability_offsets.png" alt="Five images to be used for best-pixel compositing and their DOY and year offsets" />
<p class="caption">Five images to be used for best-pixel compositing and their DOY and year offsets</p>
</div>
<p>We use linear functions to determine the suitability of an observation. In the plot for the DOY function below, we’re chosing a parameterization that favors observations closest to the target DOY and assign a suitability score of 0 for observations that are 50 or more days from the target date. Being 30 days from the target DOY, the May 16 2015 observation receives a suitability score of 0.4.<br />
A second function determines the suitability score for the year of observation. Again, observations close to the target year are assigned high values, while observations acquired five or more years from the target year are considered unsuitable. The resulting suitability score for the May 15 2015 observation is 1.</p>
<div class="figure">
<img src="fig/s04_suitabilityplot_doy_year.png" alt="Linear functions for the DOY and year suitability score" style="width:90.0%" />
<p class="caption">Linear functions for the DOY and year suitability score</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>Calculate pixel-level suitability (0-1), e.g. through min. distance to clouds.</li>
</ol>
<div class="figure">
<img src="fig/s04_suitability_clouds.png" alt="Distance to next cloud in pixels" />
<p class="caption">Distance to next cloud in pixels</p>
</div>
<p>To minimize the likelihood of pixels being affected by clouds, we favor observations further away from clouds. The cloud distance scoring function below assigns a suitability score of 1 as soon as the distance to the next cloud is equal or greater than 100 pixels. An observation that has a distance of 60 pixels (=1800m) to the next cloud receives a cloud distance suitability score of 0.6.</p>
<div class="figure">
<img src="fig/s04_suitabilityplot_clouds.png" alt="Linear function for the cloud suitability score" style="width:40.0%" />
<p class="caption">Linear function for the cloud suitability score</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>Define which criteria are most important for you (W = weights):</li>
</ol>
<p>We can weigh the different parameters according to our needs. For instance, by prioritizing the DOY score over the year score.</p>
<p><span class="math display">\[W_{DOY} = 0.5\]</span> <span class="math display">\[W_{year} = 0.2\]</span> <span class="math display">\[W_{CloudDist} = 0.3\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li>Calculate a score for each pixel in each image. We can use the weighted sum of suitabilities (S = score):</li>
</ol>
<p><span class="math display">\[score = S_{DOY} * W_{DOY} + S_{Year} * W_{Year} + S_{CloudDist} * W_{CloudDist}\]</span></p>
<p>Using the values from the example above yields:<br />
<span class="math display">\[score = 0.4 * 0.5 + 1 * 0.2 + 0.6 * 0.3\]</span> <span class="math display">\[score = 0.58\]</span></p>
<div class="figure">
<img src="fig/s04_suitability_score_weighted.png" alt="Resulting suitability scores for each image" />
<p class="caption">Resulting suitability scores for each image</p>
</div>
<ol start="6" style="list-style-type: decimal">
<li><p>Use the best observation (= highest score) for each pixel to create the final composite.</p></li>
<li><p>Evaluate the results by checking for seasonal/annual consistency and cloud distance (and re-iterate) <img src="fig/s04_final_composite.jpg" /></p></li>
</ol>
<hr />
</div>
<div id="exercise-5" class="section level2">
<h2>Exercise</h2>
<p>The goal of this week’s exercise is to combine several images into best-pixel composites using a parametric compositing function. We provide the data for this exercise in <a href="https://box.hu-berlin.de/f/17f0d06cdc164564a019/?dl=1">our repository</a>. After unpacking, you will find the following folders:</p>
<ul>
<li>sr_data: a total of 43 cloud-masked bottom of atmosphere image chips for our study region, acquired in various seasons, years, and with different degrees of cloud cover.</li>
<li>cloud_dist: data on the distance (in pixels) to the closest cloud for each of the BOA images.</li>
<li>fmask: the cloud masks used to mask the BOA images and to derive the cloud distance layers.</li>
</ul>
<p>Today we will be working with pre-defined scripts. We provide both a script containing the compositing function and a main script which sources and executes the compositing function. Outsourcing functions - especially extensive ones - improves the readability of scripts and allows for using functions in different scripts. Check the help for source() if you are not familiar with sourcing R code in your script. You find both codes below. Please copy them into an empty R script and save to disk.</p>
<p><details> <summary>Click here to see code for the parametric compositing function</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">#############################################################################
<span class="co"># MSc Earth Observation Exercise 4</span>
<span class="co"># Function for creating cloud-free composites of multiple Landsat images</span>
<span class="co"># Requires an input data.frame (here img_list) and eight compositing </span>
<span class="co"># parameters. Please see exercise sheet for further details.</span>
#############################################################################

#############################################################################
<span class="co"># Loading required packages here...</span>
<span class="kw">library</span>(raster)
<span class="kw">library</span>(lubridate)

<span class="co"># Change raster options to store large rasters in temp files on disk</span>
<span class="kw">rasterOptions</span>(<span class="dt">maxmemory =</span> <span class="fl">1e6</span>)

#############################################################################
<span class="co"># Function definition starts here</span>
parametric_compositing &lt;-<span class="st"> </span><span class="cf">function</span>(img_list, target_date, 
                                   W_DOY, W_year, W_cloud_dist, 
                                   max_DOY_offset, max_year_offset, 
                                   min_cloud_dist, max_cloud_dist) {
  <span class="co">#...</span>
  tic &lt;-<span class="st"> </span><span class="kw">Sys.time</span>()
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Start of compositing process: &#39;</span>, tic))
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Target date: &#39;</span>, target_date))

  <span class="co"># Extract target DOY and year from target_date</span>
  target_DOY &lt;-<span class="st"> </span><span class="kw">yday</span>(target_date)
  target_year &lt;-<span class="st"> </span><span class="kw">year</span>(target_date)
  
  <span class="co">#...</span>
  <span class="cf">if</span>(<span class="kw">sum</span>(W_DOY, W_year, W_cloud_dist)<span class="op">!=</span><span class="dv">1</span>) { <span class="kw">stop</span>(<span class="st">&#39;Error: something wrong.&#39;</span>) }
  
  #############################################################################
  <span class="co"># Calculate the scores for the DOY, year, and cloud distance criteria</span>
  <span class="kw">print</span>(<span class="st">&#39;Calculating compositing scores&#39;</span>)
  
  <span class="co">#...</span>
  obs_DOY &lt;-<span class="st"> </span>img_list<span class="op">$</span>DOY
  DOY_score &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="kw">abs</span>(target_DOY <span class="op">-</span><span class="st"> </span>img_list<span class="op">$</span>DOY) <span class="op">/</span><span class="st"> </span>max_DOY_offset)
  DOY_score[DOY_score<span class="op">&lt;</span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="ot">NA</span>
  
  <span class="co">#...</span>
  obs_year &lt;-<span class="st"> </span>img_list<span class="op">$</span>year
  year_score &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">-</span>(<span class="kw">abs</span>(target_year <span class="op">-</span><span class="st"> </span>obs_year) <span class="op">/</span><span class="st"> </span>max_year_offset)
  
  <span class="cf">if</span> (max_year_offset <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {year_score[obs_year<span class="op">==</span>target_year] &lt;-<span class="st"> </span><span class="dv">1</span>}
  year_score[year_score<span class="op">&lt;</span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="ot">NA</span>
  
  <span class="co"># Get candidate images within max_DOY_offset and max_year_offset</span>
  ix &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="op">!</span><span class="kw">is.na</span>(DOY_score) <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(year_score))
  <span class="cf">if</span> (<span class="kw">length</span>(ix)<span class="op">&gt;</span><span class="dv">1</span>) { <span class="kw">print</span>(<span class="kw">paste</span>(<span class="kw">length</span>(ix),  <span class="st">&#39;candidate images selected, calculating scores.&#39;</span>)) }
  <span class="cf">if</span> (<span class="kw">length</span>(ix)<span class="op">&lt;</span><span class="dv">2</span>) { <span class="kw">stop</span>(<span class="st">&#39;Another error because something is wrong.&#39;</span>) }
  
  <span class="co"># Stack cloud distance layers of candidate images and reclassify </span>
  <span class="co"># values &lt; min_cloud_dist to NA, and values &gt; max_cloud_dist to max_cloud_dist</span>
  cloud_dist &lt;-<span class="st"> </span><span class="kw">stack</span>(<span class="kw">as.character</span>(img_list<span class="op">$</span>cloud_dist_files[ix]))
  cloud_dist &lt;-<span class="st"> </span><span class="kw">reclassify</span>(cloud_dist, <span class="dt">rcl=</span><span class="kw">c</span>(<span class="dv">0</span>, min_cloud_dist, <span class="ot">NA</span>), <span class="dt">right=</span><span class="ot">NA</span>, <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
  cloud_dist &lt;-<span class="st"> </span><span class="kw">reclassify</span>(cloud_dist, <span class="dt">rcl=</span><span class="kw">c</span>(max_cloud_dist, <span class="kw">sqrt</span>(<span class="kw">nrow</span>(cloud_dist)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">ncol</span>(cloud_dist)<span class="op">^</span><span class="dv">2</span>), max_cloud_dist), <span class="dt">right=</span><span class="ot">NA</span>, <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
  
  <span class="co">#...</span>
  cloud_score &lt;-<span class="st"> </span>(cloud_dist <span class="op">-</span><span class="st"> </span>min_cloud_dist) <span class="op">/</span><span class="st"> </span>(max_cloud_dist <span class="op">-</span><span class="st"> </span>min_cloud_dist)
  
  <span class="co">#...</span>
  obs_score &lt;-<span class="st"> </span>DOY_score[ix] <span class="op">*</span><span class="st"> </span>W_DOY <span class="op">+</span><span class="st"> </span>year_score[ix] <span class="op">*</span><span class="st"> </span>W_year <span class="op">+</span><span class="st"> </span>cloud_score <span class="op">*</span><span class="st"> </span>W_cloud_dist
  
  <span class="co">#...</span>
  select &lt;-<span class="st"> </span><span class="kw">which.max</span>(obs_score)
  
  <span class="co">#...</span>
  candidates &lt;-<span class="st"> </span><span class="kw">unique</span>(select)
  
  #############################################################################
  <span class="co"># Fill composite image with pixels from the candidate images</span>
  <span class="cf">for</span> (i <span class="cf">in</span> candidates){
    
    <span class="co">#...</span>
    fill_image &lt;-<span class="st"> </span><span class="kw">brick</span>(<span class="kw">as.character</span>(img_list<span class="op">$</span>image_files[ix[i]]), <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
    
    <span class="co">#...</span>
    <span class="cf">if</span> (i <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(candidates)) { 
      composite &lt;-<span class="st"> </span><span class="kw">brick</span>(fill_image, <span class="dt">values=</span><span class="ot">FALSE</span>) 
      <span class="kw">dataType</span>(composite) &lt;-<span class="st"> &#39;INT2S&#39;</span>
      <span class="kw">values</span>(composite) &lt;-<span class="st"> </span><span class="dv">0</span>
    }
    
    <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&#39;Filling raster with acquisition from date &#39;</span>, img_list<span class="op">$</span>date[ix[i]]))
    fill_image.masked &lt;-<span class="st"> </span><span class="kw">mask</span>(fill_image, select, <span class="dt">maskvalue=</span>i, <span class="dt">inverse=</span>T, <span class="dt">updatevalue=</span><span class="dv">0</span>, <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
    fill_image.masked[<span class="kw">is.na</span>(fill_image.masked)] &lt;-<span class="st"> </span><span class="dv">0</span>
    composite &lt;-<span class="st"> </span>composite <span class="op">+</span><span class="st"> </span>fill_image.masked
    
  }
  
  <span class="co">#...</span>
  composite_na &lt;-<span class="st"> </span><span class="kw">mask</span>(composite, select, <span class="dt">maskvalue=</span><span class="ot">NA</span>, <span class="dt">datatype=</span><span class="st">&#39;INT2S&#39;</span>)
  
  #############################################################################
  <span class="co">#...</span>
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;NAs: &#39;</span>, <span class="kw">round</span>(<span class="kw">freq</span>(composite_na[[<span class="dv">1</span>]], <span class="dt">value=</span><span class="ot">NA</span>)<span class="op">/</span><span class="kw">ncell</span>(composite_na[[<span class="dv">1</span>]])<span class="op">*</span><span class="dv">100</span>, <span class="dt">digits=</span><span class="dv">3</span>), <span class="st">&#39; %&#39;</span>))
  
  <span class="co">#...</span>
  rcl_DOY &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">data=</span><span class="kw">c</span>(candidates, obs_DOY[ix[candidates]]))
  select_DOY &lt;-<span class="st"> </span><span class="kw">reclassify</span>(select, rcl_DOY, <span class="dt">datatype =</span> <span class="st">&#39;INT2S&#39;</span>)
  
  <span class="co">#...</span>
  rcl_year &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">data=</span><span class="kw">c</span>(candidates, obs_year[ix[candidates]]))
  select_year &lt;-<span class="st"> </span><span class="kw">reclassify</span>(select, rcl_year)

  <span class="co">#...</span>
  output &lt;-<span class="st"> </span><span class="kw">stack</span>(composite_na, select_DOY, select_year)
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;End of compositing process: &#39;</span>, <span class="kw">Sys.time</span>()))
  
  <span class="co">#...</span>
  <span class="kw">return</span>(output)
  
}</code></pre></div>
<p></details></p>
<p><details> <summary>Click here to see the code for the main script calling the compositing function</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  #############################################################################
<span class="co"># MSc Earth Observation Exercise 4</span>
<span class="co"># [Your Name]</span>
#############################################################################

#############################################################################
<span class="kw">library</span>(rgdal)
<span class="kw">library</span>(raster)
<span class="kw">library</span>(lubridate)
<span class="kw">library</span>(ggplot2)
<span class="kw">source</span>(<span class="st">&#39;&#39;</span>) <span class="co">#path to the parametric_compositing function</span>

<span class="co"># Change raster options to store large rasters in temp files on disk</span>
<span class="kw">rasterOptions</span>(<span class="dt">maxmemory =</span> <span class="fl">1e6</span>)

######## Define the folder that contains your data...
data.path &lt;-<span class="st"> &#39;O:/ST19_MSc-EO/S04/data/&#39;</span>

#############################################################################
<span class="co"># 1)</span>
#############################################################################

sr &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="kw">paste0</span>(data.path, <span class="st">&#39;/sr_data&#39;</span>), <span class="dt">pattern=</span><span class="st">&quot;.tif$&quot;</span>, <span class="dt">full.names=</span>T, <span class="dt">recursive=</span>F)
fmask &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="kw">paste0</span>(data.path, <span class="st">&#39;/fmask&#39;</span>), <span class="dt">pattern=</span><span class="st">&quot;.tif$&quot;</span>, <span class="dt">full.names=</span>T, <span class="dt">recursive=</span>F)
cd &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="kw">paste0</span>(data.path, <span class="st">&#39;/cloud_dist&#39;</span>), <span class="dt">pattern=</span><span class="st">&quot;.tif$&quot;</span>, <span class="dt">full.names=</span>T, <span class="dt">recursive=</span>F)

sta &lt;-<span class="st"> </span><span class="kw">nchar</span>(<span class="kw">paste0</span>(data.path,<span class="st">&#39;/sr_data/LT05228082&#39;</span>)) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
end &lt;-<span class="st"> </span>sta <span class="op">+</span><span class="st"> </span><span class="dv">6</span>

dates &lt;-<span class="st"> </span><span class="kw">as.Date</span>(<span class="kw">substr</span>(sr, sta, end), <span class="dt">format=</span><span class="st">&#39;%Y%j&#39;</span>)

sr.sorted &lt;-<span class="st"> </span>sr[<span class="kw">order</span>(dates)]
cd.sorted &lt;-<span class="st"> </span>cd[<span class="kw">order</span>(dates)]

img_list &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&quot;image_files&quot;</span>=<span class="kw">as.character</span>(sr.sorted), <span class="st">&quot;cloud_dist_files&quot;</span>=<span class="kw">as.character</span>(cd.sorted) ,<span class="st">&quot;date&quot;</span>=<span class="kw">sort</span>(dates), <span class="st">&quot;DOY&quot;</span>=<span class="kw">yday</span>(<span class="kw">sort</span>(dates)), <span class="st">&quot;year&quot;</span>=<span class="kw">year</span>(<span class="kw">sort</span>(dates)))


#############################################################################
<span class="co"># 2)</span>
#############################################################################
target_date_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">ymd</span>(<span class="st">&#39;YYYYMMDD&#39;</span>)
target_date_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">ymd</span>(<span class="st">&#39;YYYYMMDD&#39;</span>)

W_DOY &lt;-<span class="st"> </span><span class="fl">0.0</span>
W_year &lt;-<span class="st"> </span><span class="fl">0.0</span>
W_cloud_dist &lt;-<span class="st"> </span><span class="fl">0.0</span>
  
max_DOY_offset &lt;-<span class="st"> </span><span class="dv">0</span>
max_year_offset &lt;-<span class="st"> </span><span class="dv">0</span>

min_cloud_dist &lt;-<span class="st"> </span><span class="dv">0</span>
max_cloud_dist &lt;-<span class="st"> </span><span class="dv">0</span>

composite_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">parametric_compositing</span>(img_list, target_date_<span class="dv">1</span>, 
                                       W_DOY, W_year, W_cloud_dist, 
                                       max_DOY_offset, max_year_offset, 
                                       min_cloud_dist, max_cloud_dist)

#############################################################################
<span class="co"># 4)</span>
#############################################################################

composite_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">parametric_compositing</span>(img_list, target_date_<span class="dv">2</span>, 
                                      W_DOY, W_year, W_cloud_dist, 
                                      max_DOY_offset, max_year_offset, 
                                      min_cloud_dist, max_cloud_dist)</code></pre></div>
<p></details></p>
<div id="parameterization" class="section level3">
<h3>1) Parameterization</h3>
<p>The function above allows you to produce cloud-free best-pixel composites for a pre-defined target DOY. This function requires one input data.frame (see code template on how to create it) and eight input parameters:</p>
<ol style="list-style-type: lower-alpha">
<li><p><em>img_list</em>: a data frame containing five variables for each of the Landsat images:</p>
<p>$image_files: the full paths to the files in …sr_data.<br />
$cloud_dist_files: the full paths to the files in …cloud_dist.<br />
$date: the acquisition day in Date format (YYYY-MM-DD).<br />
$DOY: the acquisition day of the year.<br />
$year: the acquisition year.</p></li>
<li><p><em>target_date</em>: the target date for your composite in Date format (YYYY-MM-DD)</p></li>
<li><p><em>W_DOY</em>, <em>W_year</em>, <em>W_cloud_dist</em>: weights for the three available parameters DOY, year and distance to clouds. Must be scaled between 0 and 1 and sum up to 1, the higher the weight, the higher the importance of the criterion.</p></li>
<li><p><em>max_DOY_offset</em>, <em>max_year_offset</em>: Thresholds for the maximum allowed differences between target DOY and acquisition DOY, as well target year and acquisition year. Images exceeding these thresholds (further away in time) will be fully ignored. For instance, max_year_offset = 0 will not allow observations from a year other than the target year. By choosing these parameters, you will determine whether you prefer seasonal consistency (close to target DOY but from different years) over annual consistency (observations from same year but potentially distant DOYs). Discuss the parametrization in your group.</p></li>
<li><p><em>min_cloud_dist</em>, <em>max_cloud_dist</em>: The minimum and maximum distance to clouds. min_cloud_dist = 10 will exclude all observations which are less than 10 pixels away from a cloud. The cloud scores are linearly scaled between the minimum (score = 0) and maximum cloud distance (score = 1). Pixels with distances above max_cloud_dist will receive a score of 1.</p></li>
</ol>
</div>
<div id="defining-target-dates-and-parameters" class="section level3">
<h3>2) Defining target dates and parameters</h3>
<p>Relying on last week´s insights during training data collection, define two target days of the year (DOY) to capture contrasting phenological stages of the different forest types. These will be used as target_date parameters later on. Next, make a decision concerning the compositing parameters explained in 1c, 1d and 1e.</p>
</div>
<div id="exploring-the-script-and-adding-documentation" class="section level3">
<h3>3) Exploring the script and adding documentation</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Open the parametric_compositing.R script and take time to read through it in groups. Run the code line by line. Make sure you understand how the function operates. Discuss questions in your group and seek the help pages of functions you don´t know.</p></li>
<li><p>The developer did not spend sufficient time on the documentation. Make the script a bit more user-friendly by adding missing comments (#…). Make sure your comments explain what happens in each step of the function, and why. Are there bugs or sections which you would code differently?</p></li>
<li><p>Next, run the parametric_compositing() function with your parameters and write the result to disk. Include the target DOY in the filename. While the function executes, proceed with the next exercise.</p></li>
</ol>
</div>
<div id="second-run-for-target-date-2" class="section level3">
<h3>4) Second run for target date 2</h3>
<p>Repeat the compositing for the second target DOY you specified in 2) and write the results to disk.</p>
</div>
<div id="visual-inspection-and-evaluation-of-results" class="section level3">
<h3>5) Visual inspection and evaluation of results</h3>
<p>Visually inspect the quality of your compositing results in QGIS. Look at the bands containing the DOY and year flags (band 7 and 8). What worked out well, what did not? How could the quality of the composites be improved? Re-iterate with different parameters if you wish.</p>
</div>
<div id="improving-the-user-friendliness-of-the-script-optionalvoluntary-task" class="section level3">
<h3>6) Improving the user-friendliness of the script (optional/voluntary task)</h3>
<p>Make the compositing function more user friendly. Insert a couple of plot and print commands to enable the users to follow the progress of the compositing while the function is running.</p>
<p>For instance, print() how many images were used for the final composite, their acquisition dates, etc. Also, you might want to plot() the composited image after each iteration. You could add further status messages telling the user how much time single steps took.</p>
<p>Don´t forget to save the script and run the source() command in your R script to update the function after you made these changes.</p>
<hr />
</div>
</div>
<div id="reading-materials-3" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following paper:</p>
<p><a href="https://doi.org/10.1080/01431161.2018.1433343">Maxwell et al. (2018): Implementation of machine-learning classification in remote sensing: an applied review. International Journal of Remote Sensing 39(9), 2784-2817</a></p>
<p>In this review, Maxwell and colleagues outline the use of several machine learning classification algorithms for application in remote sensing. Please read the article and note questions and points to discuss. Feel free to omit the details of ANNs and boosted DTs. Please read the paper thoroughly, make sure you understand the underlying concept and write down question for the discussion in our next session. Also, answer the following broad questions:</p>
<ul>
<li>What is the motivation for this article?</li>
<li>What are the key differences between classification algorithms from a user perspective?</li>
<li>Which classifiers do you reckon to be most suitable for typical EO applications?</li>
<li>What do the following terms refer to: “overfitting”, “imbalanced data”, “ensemble classifiers”, “parameter optimization”?</li>
</ul>
<hr />
<!-- ################################## SESSION 05 ############################################## -->
</div>
</div>
<div id="session-05-machine-learning-for-image-classification" class="section level1">
<h1>Session 05: Machine learning for image classification</h1>
<div id="learning-goals-6" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Understanding fundamentals of the Random Forest classification algorithm</li>
<li>Conduct sensitivity analyses to parametrize the classification model</li>
<li>Produce a forest type map for the Carpathian study region</li>
</ul>
<hr />
</div>
<div id="background-2" class="section level2">
<h2>Background</h2>
<p>This session widely focuses on the Random Forest (RF) algorithm <a href="https://doi.org/10.1023/A:1010933404324">(Breimann 2001)</a> and its implementation in R. Some background might be useful in this context. The basic principles of the RF algorithm can be easily understood when disentangling its name:</p>
<p><strong>“Forest”</strong>: The RF is an ensemble of self-learning decision trees, which - metaphorically speaking - shape a forest. The idea behind the ensemble is that many weak learners can come to one strong decision. Each decision tree consists of a large number of splits, which essentially represent simple binary (yes / no) decisions, e.g. “is reflectance in band 4 &gt; 0.65?”. The sequential binary branching creates a tree-like shape. The user defines the number of decision trees in the forest (e.g. via the <code>ntrees</code> parameter in the R implementation).</p>
<div class="figure">
<img src="fig/s05_dt.PNG" alt="Exemplary illustration of a two-dimensional feature space, divided by binary thresholds (left), as well as the corresponding decision tree (right)." style="width:80.0%" />
<p class="caption">Exemplary illustration of a two-dimensional feature space, divided by binary thresholds (left), as well as the corresponding decision tree (right).</p>
</div>
<p><strong>“Random”</strong>: The RF has two layers of randomness. First, it uses a bootstrapped random sample (with replacement) of the training dataset for growing each individual decision tree. The user can control for the fraction of training data used (e.g. via the <code>sampsize</code> parameter in the R implementation). The second random component is the selection of the features considered at each binary split.</p>
<div id="how-a-self-learning-tree-grows" class="section level3">
<h3>How a self-learning tree grows</h3>
<p>Self-learning decision trees automatically find the “best split” in the feature space to break the training data into two groups, which will be further subdivided until a clear decision for each sample can be made. The algorithm attempts to create splits that divide the data into relatively homogeneous subgroups.</p>
<div class="figure">
<img src="fig/s05_split.PNG" alt="A good split creates relatively homogeneous subgroups of the data." style="width:90.0%" />
<p class="caption">A good split creates relatively homogeneous subgroups of the data.</p>
</div>
<p>Finding a good split requires infomration on which feature (e.g. spectral band) and which value (e.g. reflectance) should be used to separate the data. The best split is identified based on a measure of dataset heterogeneity (Gini impurity index). The RF selects the split (feature and value) which minimizes the heterogeneity of the resulting datasets:</p>
<p><span class="math inline">\(Gini_{D} = 1 - \sum_{j=1}^n p_j^2\)</span></p>
<p>where <span class="math inline">\(D\)</span> is the dataset at hand and <span class="math inline">\(p_j\)</span> is the fraction of class <span class="math inline">\(j\)</span> in the dataset. High class proportions in the dataset produce low Gini index values. The Gini index for a split is a combined measure of the Gini index for the two datasets <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> resulting from the split. To account for different sizes of the two datasets, the Gini of each sub-dataset is weighted according to the number of observations <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> in the resulting sub-groups of each dataset.</p>
<p><span class="math inline">\(Gini_{split} = \frac{N_1}{N} Gini_{D_1} + \frac{N_2}{N} Gini_{D_2}\)</span></p>
<p>In case we would consider all available features at each split, the decision trees would probably be very similar. This is why we produce a random selection of features, for which identify the split (feature and value) that produces the most homogeneous sub-groups of training data. The user controls for the number of features which should be considered (or tried) at each split (e.g. via the <code>mtry</code> parameter in the R implementation). Decreasing the number of variables tried at each split de-correlates the structure of the trees.</p>
<p>Once <code>ntree</code> decision trees are built, we have a readily applicable rule-set to classify each pixel in our image. Every tree produces one “vote” regarding the final class outcome and the majority of votes determines the final class label for each pixel.</p>
</div>
<div id="the-out-of-bag-error" class="section level3">
<h3>The out-of-bag error</h3>
<p>The RF has a great feature: the out-of-bag error. After growing each tree, the left-out samples (i.e. not used for training of this particular tree) can be classified to investigate the performance of our classification model. Doing this allows us to calculate a classification error. For illustration, consider 8 OOB samples, of which 2 are wrongly and 6 are correctly classified. The OOB error will be 2/8 = 0.25. This simple measure of model performance can be useful to parametrize our classification model. We can for instance test various combinations of input features, training datasets, or investigate the RF performance with increasing <code>ntrees</code>.</p>
<div class="figure">
<img src="fig/s05_oob.PNG" alt="Grid representation of the out-of-bag classification error for various combinations of two input images from different days of the year (left). Decreasing out-of-bag error with increasing number of trees in the RF model (right)." style="width:80.0%" />
<p class="caption">Grid representation of the out-of-bag classification error for various combinations of two input images from different days of the year (left). Decreasing out-of-bag error with increasing number of trees in the RF model (right).</p>
</div>
</div>
<div id="variable-importances-partial-dependence-plots" class="section level3">
<h3>Variable importances &amp; partial dependence plots</h3>
<p>Next, the RF allows us to investigate which features (e.g. spectral bands) were most useful for our classification problem. Variable importances measures for each individual feature can be expressed either as mean decrease in Gini (how much “better”&quot; can we split the data based on this feature) or mean increase in accuracy (how much does the out-of-bag error increase when the feature is left out of the model). It is important to note that these variable importances should be interpreted cautiously, as they are always strongly dependent on the available input features and their collinearity structure, as well as the training data used. Still, the variable importance of a hypothetical model might reveal, for instance, that the swIR 1 reflectance at DOY 196 is quite important.</p>
<div class="figure">
<img src="fig/s05_varimp.png" alt="Mean decrease in accuracy (increase in OOB error) caused by the features included in the classification model." style="width:60.0%" />
<p class="caption">Mean decrease in accuracy (increase in OOB error) caused by the features included in the classification model.</p>
</div>
<p>In a next step, we can use partial dependence plots to further investigate the relationship between individual features and the likelihood of class occurrence. We can try to interpret this graph and assess relationships between individual features and our classes of interest. While generating meaningful insights is somewhat tricky with reflectance values, we could also imagine including, e.g., a digital elevation model into the RF model to learn about the distribution of our classes across elevation gradients.</p>
<div class="figure">
<img src="fig/s05_pdp.png" alt="Change in class likelihood across the value range of the swIR 1 reflectance at DOY 196." style="width:80.0%" />
<p class="caption">Change in class likelihood across the value range of the swIR 1 reflectance at DOY 196.</p>
</div>
<p>Summing it up, there are a some key advantages of the RF algorithm, which make it highly popular in data science and remote sensing. The RF is easy to understand, computationally efficient, parametrization is pretty straightforward, and it often produces great results. It provides quick insights into model performance via the out of bag error and allows for investigating variable importance measures and partial dependence plots.</p>
<hr />
</div>
</div>
<div id="exercise-6" class="section level2">
<h2>Exercise</h2>
<p>In this exercise, we will deal with image classification using the Random Forest algorithm, as implemented in the <code>randomForest</code> package. Specifically, we will use your training data and your pixel-based composites from the last exercises to map forest types in the Western Beskids. We will assess the performance of multiple classification models through the out of bag error and investigate variable importances. In case you missed the last sessions, we provide exemplary composites and training data <a href="https://box.hu-berlin.de/f/85a35c373cdd490180fe/?dl=1">here</a>.</p>
<div id="training-a-random-forest-model" class="section level3">
<h3>1) Training a Random Forest model</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Load the vector file containing your training data points using <code>readOGR()</code>. Next, create a <code>stack()</code> / <code>brick()</code> of your favourite pixel-based composite (last week´s result).</p></li>
<li><p>Use <code>extract()</code> to create a <code>data.frame</code> with training points as rows, and class labels (<code>classID</code>) as well as the spectral bands of your composites as columns. Remove the day of year and year flags (band 7 and 8) for the next steps.</p></li>
<li><p>As we want to train a classification (and not a regression), the <code>randomForest()</code> function expects the dependent variable to be of type factor. Use <code>as.factor()</code> for conversion of the <code>classID</code> column. The RF algorithm cannot deal with NoData (<code>NA</code>) values. Remove <code>NAs</code> from the <code>data.frame</code>.</p></li>
<li><p>Train a <code>randomForest()</code> classification model with the <code>data.frame</code> created in the prior step. Make sure to include only useful predictors.</p></li>
<li><p>Repeat the RF training procedure and produce additional model objects. Use i) the other pixel-based composite, and ii) a stack of both composites as input features.</p></li>
</ol>
</div>
<div id="investigating-model-performance" class="section level3">
<h3>2) Investigating model performance</h3>
<p>The RF model objects contain a wealth of information on the model parameters and performance. Assess the out of bag (OOB) error estimates of the trained models by inspecting the <code>err.rate</code> attribute of your model objects. Answer the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li>Which model has the lowest OOB error?</li>
<li>How does the OOB behave when increasing the number of trees in your model (<code>ntrees</code>)? You can access the OOB per number of trees via <code>err.rate</code>. Use this information to roughly determine a suitable value for <code>ntrees</code>.</li>
<li>In the model with the lowest OOB error, Which of the four classes has the highest OOB error?</li>
<li>In case you are not satisfied with your model performance, consider using only high-quality training samples from your dataset, i.e. those samples with <code>confID = 1</code>. You may join forces with your neighbor and merge your training datasets. Feel free to experiment and document your findings.</li>
</ol>
</div>
<div id="final-model-parametrization-and-variable-importances" class="section level3">
<h3>3) Final model parametrization and variable importances</h3>
<ol style="list-style-type: lower-alpha">
<li>Train a final model with the best combination of images and <code>ntrees</code>.</li>
<li>Investigate the variable importances using <code>varImpPlot()</code>. Use <code>partialPlot()</code> to produce partial dependence plots for your most important predictor and all four classes. Can you explain the differences between classes?</li>
</ol>
</div>
<div id="classification" class="section level3">
<h3>4) Classification</h3>
<p>Perform a classification of the image stack using the <code>predict()</code> function. Write the resulting map to disk in <code>GTiff</code> format. When doing so, consider choosing the appropriate <code>datatype</code> argument. Take a look at your result in QGIS.</p>
</div>
</div>
<div id="reading-materials-4" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss the following reading (until page 23):</p>
<p><a href="http://www.fao.org/3/a-i5601e.pdf">FAO (2016): Map Accuracy Assessment and Area Estimation - A Practical Guide</a></p>
<p>This is a user guide for map accuracy assessment and area estimation, following the good practice recommendations outlined by <a href="https://doi.org/10.1016/j.rse.2014.02.015">Olofsson et al. (2014)</a>. A great piece of literature focussing on the applied side. Please read the document thoroughly, make sure you understand the basic idea behind it and write down question for the discussion in our next session. Also, answer the following broad questions:</p>
<ul>
<li>What is the motivation for this technical report?</li>
<li>Why is a stratified sampling advised?</li>
<li>How can we determine how many samples are needed for accuracy assessment?</li>
<li>What is the difference of populating an error matrix with <span class="math inline">\(p_{ij}\)</span> instead of <span class="math inline">\(n_{ij}\)</span>?</li>
<li>Which data source should be used for producing class area estimates?</li>
</ul>
<p>Please take a look at the Excel sheet provided <a href="https://box.hu-berlin.de/f/269470b96af54ba687ef/?dl=1">here</a>. Use this table to improve your understanding of the area adjusted accuracy assessment. You can manipulate values in the confusion matrix / class proportions and trace the formulas which link these values with the resulting class-specific and overall accuracies as well as the area estimates.</p>
<hr />
<!-- ################################## SESSION 06 ##############################################  -->
</div>
</div>
<div id="session-06-accuracy-assessment-and-area-estimation" class="section level1">
<h1>Session 06: Accuracy assessment and area estimation</h1>
<div id="learning-goals-7" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Improve your understanding of area-adjusted accuracy assessment</li>
<li>Assess the map errors of your forest type classification</li>
<li>Calculate error-adjusted class area estimates</li>
</ul>
</div>
<div id="background-3" class="section level2">
<h2>Background</h2>
<p>Independent map accuracy assessment aims at getting an unbiased estimate of map accuracy. Generally, we identify a number of sample locations and compare the predicted class in the map with “true” class, which we determine using high quality reference information (e.g., very high resolution imagery). The following section requires understanding of the basic concepts of map accuracy assessment, i.e. the confusion matrix, overall, producer´s, and user´s accuracy. Read in, e.g. <a href="http://gsp.humboldt.edu/olm_2015/Courses/GSP_216_Online/lesson6-2/metrics.html">here</a> if you don´t know what these are.</p>
<div class="figure">
<img src="fig/s06_map.png" alt="Forest type map with stratified random sample" />
<p class="caption">Forest type map with stratified random sample</p>
</div>
<div id="why-is-area-adjustment-necessary" class="section level3">
<h3>Why is area-adjustment necessary?</h3>
<p>Sample-based estimates of map accuracy are statistical estimates - we infer information about the total population (i.e. all pixels in the map) from a sub-population (i.e. the sample pixels). In order to get a statistically sound estimate of map accuracy, we need to pay attention to the way our sample locations are selected. Sample selection should be random and all pixels should have an inclusion probability greater than zero.</p>
<p>Typical sampling schemes include pure random, systematic random, and stratified random sampling. When is it useful to use stratified sampling over a systematic or pure random sample? Imagine a map forest loss in an area dominated by forest, where deforestation accounts for roughly 0.2% of the area. In order to get a sufficiently large sample (let´s assume 60 samples) for the deforestation class using pure random sampling, we would approximately need to sample 30,000 locations. Stratified random sample helps us to overcome this issue by precisely allocating the number of desired samples within each class (or region) of interest.</p>
<div class="figure">
<img src="fig/s06_sampling.jpg" alt="Illustration of pure random, systematic, and stratified random sampling designs. Source: gsp.humboldt.edu" />
<p class="caption">Illustration of pure random, systematic, and stratified random sampling designs. Source: gsp.humboldt.edu</p>
</div>
<p>Let´s bring this into the context of our study site. A map of our study region reveals imbalances in class extent, e.g., the deciduous forest class accounts for only 7% of the study area, while coniferous forests account for 40%.</p>
<table>
<thead>
<tr class="header">
<th>Class name</th>
<th>Proportion (<span class="math inline">\(w_i\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deciduous forest</td>
<td><span class="math inline">\(0.07\)</span></td>
</tr>
<tr class="even">
<td>Mixed forest</td>
<td><span class="math inline">\(0.25\)</span></td>
</tr>
<tr class="odd">
<td>Coniferous forest</td>
<td><span class="math inline">\(0.40\)</span></td>
</tr>
<tr class="even">
<td>Non-forest</td>
<td><span class="math inline">\(0.28\)</span></td>
</tr>
</tbody>
</table>
<p>Stratified random sampling helps us to validate the map using a sufficient amount of samples for each for the four classes, let´s say 50. Below, you find a confusion matrix of the map validation. Rows here relate to the map classes <span class="math inline">\(i\)</span> and columns indicate reference classes <span class="math inline">\(j\)</span>. Note that the sum of samples per class differs quite substantially between map (<span class="math inline">\(n_i\)</span>) and reference (<span class="math inline">\(n_j\)</span>).</p>
<table>
<thead>
<tr class="header">
<th>Class name</th>
<th>Deciduous forest</th>
<th>Mixed forest</th>
<th>Coniferous forest</th>
<th>Non-forest</th>
<th></th>
<th><span class="math inline">\(n_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deciduous forest</td>
<td><span class="math inline">\(39\)</span></td>
<td><span class="math inline">\(5\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(5\)</span></td>
<td></td>
<td><span class="math inline">\(50\)</span></td>
</tr>
<tr class="even">
<td>Mixed forest</td>
<td><span class="math inline">\(15\)</span></td>
<td><span class="math inline">\(19\)</span></td>
<td><span class="math inline">\(10\)</span></td>
<td><span class="math inline">\(6\)</span></td>
<td></td>
<td><span class="math inline">\(50\)</span></td>
</tr>
<tr class="odd">
<td>Coniferous forest</td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(7\)</span></td>
<td><span class="math inline">\(39\)</span></td>
<td><span class="math inline">\(4\)</span></td>
<td></td>
<td><span class="math inline">\(50\)</span></td>
</tr>
<tr class="even">
<td>Non-forest</td>
<td><span class="math inline">\(11\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(4\)</span></td>
<td><span class="math inline">\(34\)</span></td>
<td></td>
<td><span class="math inline">\(50\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><span class="math inline">\(n_j\)</span></td>
<td><span class="math inline">\(65\)</span></td>
<td><span class="math inline">\(32\)</span></td>
<td><span class="math inline">\(54\)</span></td>
<td><span class="math inline">\(49\)</span></td>
<td></td>
<td><span class="math inline">\(200\)</span></td>
</tr>
</tbody>
</table>
<p>From this table, we can derive the unadjusted accuracy values, such as the overall accuracy as follows:</p>
<p><span class="math inline">\(OA_{unadjusted} = \frac{(39 + 19 + 39 + 34)}{200} = 0.63 = 63\%\)</span>.</p>
<p>The above estimate of overall accuracy implicitly weighs each class according to the number of correctly classified samples in it. The more samples, the higher the influence on the overall accuracy score. The unadjusted overall accuracy is thus independent of the true area proportions of the map classes.</p>
<p>Furthermore, the unadjusted overall accuracy does not account for the sampling bias introduced by the stratified sampling. You can imagine this bias as varying sampling densities between the map classes. Let´s assume our map contains a total of 10,000 pixels, we have mapped 700 pixels as deciduous forest and 4,000 pixels as coniferous forest. By verifying 50 samples of each class, we assessed <span class="math inline">\(\frac{50}{700} = 0.071 = 7.1\%\)</span> of the deciduous forest class pixels, while only <span class="math inline">\(\frac{50}{4,000} = 0.013 = 1.3\%\)</span> of the coniferous forest class were investigated. The other way around, one sample of the deciduous forest class is considered representative for <span class="math inline">\(\frac{700}{50} = 14\)</span> pixels, while one sample of the coniferous forest class represents <span class="math inline">\(\frac{4,000}{50} = 80\)</span> pixels.</p>
<p><strong>An assumption</strong>: The samples drawn from the deciduous forest stratum represent a much smaller area in our map (7%), as compared to the coniferous forest class (40%). In terms of overall accuracy, this means that the 39 correctly classified pixels of the deciduous forest class are probably less “relevant” than the 39 correctly classified pixels in the coniferous class. According to the numbers above, one sample of coniferous forest shold thus weigh <span class="math inline">\(\frac{80}{14} = 5.7\)</span> times as much as a deciduous forest pixel.</p>
</div>
<div id="how-to-implement-it" class="section level3">
<h3>How to implement it?</h3>
<p>We have to account for this bias when estimating map accuracies from a stratified random sample. This can be accomplished by populating the confusion matrix with probabilities of encountering the combination of map class <span class="math inline">\(i\)</span> and reference class <span class="math inline">\(j\)</span>, expressed as:</p>
<p><span class="math inline">\(p_{ij} = \frac{n_{ij}}{n_i} * w_i\)</span></p>
<p><span class="math inline">\(n_{ij}\)</span> = Number of pixels belonging to map class <span class="math inline">\(i\)</span> and reference class <span class="math inline">\(j\)</span></p>
<p><span class="math inline">\(n_i\)</span> = Total number of pixels of map class <span class="math inline">\(i\)</span></p>
<p><span class="math inline">\(w_i\)</span> = Proportion of map class <span class="math inline">\(i\)</span></p>
<p>In the resulting matrix, each cell value represents the probability of occurrence of map class <span class="math inline">\(i\)</span> and reference class <span class="math inline">\(j\)</span>.</p>
<table style="width:97%;">
<colgroup>
<col width="19%" />
<col width="19%" />
<col width="19%" />
<col width="19%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th>Class name</th>
<th>Deciduous forest</th>
<th>Mixed forest</th>
<th>Coniferous forest</th>
<th>Non-forest</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deciduous forest</td>
<td><span class="math inline">\(\frac{39}{50}0.07\)</span></td>
<td><span class="math inline">\(\frac{5/}{50}0.07\)</span></td>
<td><span class="math inline">\(\frac{1}{50}0.07\)</span></td>
<td><span class="math inline">\(\frac{5}{50}0.07\)</span></td>
</tr>
<tr class="even">
<td>Mixed forest</td>
<td><span class="math inline">\(\frac{15}{50}0.25\)</span></td>
<td><span class="math inline">\(\frac{19}{50}0.25\)</span></td>
<td><span class="math inline">\(\frac{10}{50}0.25\)</span></td>
<td><span class="math inline">\(\frac{6}{50}0.25\)</span></td>
</tr>
<tr class="odd">
<td>Coniferous forest</td>
<td><span class="math inline">\(\frac{0}{50}0.40\)</span></td>
<td><span class="math inline">\(\frac{7}{50}0.40\)</span></td>
<td><span class="math inline">\(\frac{39}{50}0.40\)</span></td>
<td><span class="math inline">\(\frac{4}{50}0.40\)</span></td>
</tr>
<tr class="even">
<td>Non-forest</td>
<td><span class="math inline">\(\frac{11}{50}0.28\)</span></td>
<td><span class="math inline">\(\frac{1}{50}0.28\)</span></td>
<td><span class="math inline">\(\frac{4}{50}0.28\)</span></td>
<td><span class="math inline">\(\frac{34}{50}0.28\)</span></td>
</tr>
</tbody>
</table>
<p>Which yields the confusion matrix populated with probabilities:</p>
<table style="width:100%;">
<colgroup>
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th>Class name</th>
<th>Deciduous forest</th>
<th>Mixed forest</th>
<th>Coniferous forest</th>
<th>Non-forest</th>
<th></th>
<th><span class="math inline">\(\sum p_{i}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deciduous forest</td>
<td><span class="math inline">\(0.055\)</span></td>
<td><span class="math inline">\(0.007\)</span></td>
<td><span class="math inline">\(0.001\)</span></td>
<td><span class="math inline">\(0.007\)</span></td>
<td></td>
<td><span class="math inline">\(0.070\)</span></td>
</tr>
<tr class="even">
<td>Mixed forest</td>
<td><span class="math inline">\(0.075\)</span></td>
<td><span class="math inline">\(0.095\)</span></td>
<td><span class="math inline">\(0.050\)</span></td>
<td><span class="math inline">\(0.030\)</span></td>
<td></td>
<td><span class="math inline">\(0.250\)</span></td>
</tr>
<tr class="odd">
<td>Coniferous forest</td>
<td><span class="math inline">\(0.000\)</span></td>
<td><span class="math inline">\(0.056\)</span></td>
<td><span class="math inline">\(0.312\)</span></td>
<td><span class="math inline">\(0.032\)</span></td>
<td></td>
<td><span class="math inline">\(0.400\)</span></td>
</tr>
<tr class="even">
<td>Non-forest</td>
<td><span class="math inline">\(0.062\)</span></td>
<td><span class="math inline">\(0.006\)</span></td>
<td><span class="math inline">\(0.022\)</span></td>
<td><span class="math inline">\(0.190\)</span></td>
<td></td>
<td><span class="math inline">\(0.280\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sum p_{j}\)</span></td>
<td><span class="math inline">\(0.192\)</span></td>
<td><span class="math inline">\(0.164\)</span></td>
<td><span class="math inline">\(0.385\)</span></td>
<td><span class="math inline">\(0.259\)</span></td>
<td></td>
<td><span class="math inline">\(1.000\)</span></td>
</tr>
</tbody>
</table>
<p>This table sums to 1 and the row sums correspond to the map class proportions from above. We can now confirm our assumption from above. The 39 correct pixels of the deciduous forest class represents a smaller map proportion as compared to the 39 correctly classified pixels of the coniferous forest class (5.5% vs. 31.2%). The coniferous forest samples are therefore more relevant for the area-adjusted overall map accuracy than the deciduous forest samples. More precisely, they are <span class="math inline">\(\frac{0.312}{0.055} = 5.7\)</span> times more important.</p>
<p>Based on this probabilistic confusion matrix, we can calculate the overall and class-wise accuracy scores as usual, while implicitly accounting for different class proportions and the sampling bias.</p>
<p><span class="math inline">\(OA_{adjusted} = 0.055 + 0.095 + 0.312 + 0.190 = 0.652\)</span></p>
<p>We can notice that our overall accuracy increased after area-adjustment. This makes sense, as we were doing quite good in mapping coniferous forests accurately. Now let´s have a look at the user´s and producer´s accuracy of the deciduous forest class. First, we calculate the unadjusted accuracy scores:</p>
<p><span class="math inline">\(UA_{unadjusted} = \frac{39}{50} = 0.78 = 78\%\)</span></p>
<p><span class="math inline">\(PA_{unadjusted} = \frac{39}{65} = 0.60 = 60\%\)</span></p>
<p>Let´s do the same using the probability matrix:</p>
<p><span class="math inline">\(UA_{adjusted} = \frac{0.055}{0.070} = 0.78 = 78\%\)</span></p>
<p><span class="math inline">\(PA_{adjusted} = \frac{0.055}{0.192} = 0.29 = 29\%\)</span></p>
<p>We can see that the user´s accuracy stays the same after adjustment. This makes sense, as we do not incorporate the sampling bias when considering only samples drawn from one map stratum. For the producer´s accuracy, we combine information from samples across all strata. We hence automatically consider the different class proportions and sampling densities when calculating the producer´s accuracy from the confusion matrix populated with probabilities. The producer´s accuracy is relatively low after adjustment, owing to the fact that we likely omit a lot of deciduous forest, which we falsely classified as mixed forest or non-forest.</p>
</div>
<div id="error-adjusted-area-estimates" class="section level3">
<h3>Error-adjusted area estimates</h3>
<p>There are several methods to obtain class-wise area estimates. They can either be derived directly from the map by multiplying the map´s class proportions with the total size of the study area. Assuming a study area of <span class="math inline">\(S = 10,000 ha\)</span>, our area estimates for deciduous forest would thereby be:</p>
<p><span class="math inline">\(area_{DF} = w_{i=DF} * S\)</span></p>
<p><span class="math inline">\(area_{DF} = 0.070 * 10,000 ha = 700 ha\)</span></p>
<p>However, we already know that the reference data at hand is of better quality as compared to our map. Therefore class-area should preferably be estimated from the reference data:</p>
<p><span class="math inline">\(area_{DF} = \sum_{p_{j=DF}}^i * S\)</span></p>
<p><span class="math inline">\(area_{DF} = 0.192 * 10,000 ha = 1,920 ha\)</span></p>
<p>So the better area estimate for deciduous forest in our study region is 1,920 ha, as compared to the map-based estimate of 700 ha. Here again, the substantial omission of deciduous forest in our map was taken into account, which in turn largely increases the area estimate for this class.</p>
<p>Wrapping it up, area adjustment can make quite a substantial difference for overall accuracy and producer´s accuracy. User´s accuracy remains unaffected. Producing area estimates directly from the reference data is advised in order to produce more accurate estimations of land cover, land use, or land change.</p>
<p>All procedures which were only briefly described here and are explained in more detail in <a href="https://doi.org/10.1016/j.rse.2014.02.015">Olofsson et al. (2014)</a>. We recommend this read, as it is documenting the current state-of-the-art in regards to accuracy assessment in high detail. Another source of background information is the <a href="https://area2.readthedocs.io/en/latest/background.html">technical documentation of the AREA² tools for accuracy assessment</a>.</p>
</div>
</div>
<div id="exercise-7" class="section level2">
<h2>Exercise</h2>
<p>In this exercise, you will perform an area-adjusted accuracy assessment of your forest type map and estimate error-adjusted area for each forest type class. In a first step, the accuracy scores will be investigated in an <a href="https://box.hu-berlin.de/f/269470b96af54ba687ef/?dl=1">Excel spreadsheet</a>. In case you do not yet have a forest type map, please use the one provided <a href="https://box.hu-berlin.de/f/00fd4fdd12b34560b836/?dl=1">here</a>.</p>
<div id="producing-reference-data" class="section level3">
<h3>1) Producing reference data</h3>
<p>Load your forest type map in R. Create a stratified reference sample using <code>sampleStratified()</code>. We want to have a stratified equalized sample of 20 pixels per class. Specify the <code>size</code> parameter accordingly. Specify <code>sp = TRUE</code> to receive a spatial points object and <code>na.rm = TRUE</code> to ignore unclassified regions. Write the points to disk using <code>writeOGR()</code>.</p>
<p>Load the point shapefile in QGIS and add two fields of type Integer: <code>classID</code> and <code>confID</code>. Visit each point and determine the class label according to the high resolution imagery in GoogleEarth. Install and use the Send2GE plugin in QGIS to identify the precise pixel location. Enter the class and confidence labels in the attribute table. Work efficiently through the points and save your changes regularly.</p>
<table>
<thead>
<tr class="header">
<th>Class name</th>
<th>classID</th>
<th>Confidence level</th>
<th>confID</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deciduous forest</td>
<td>1</td>
<td>Very certain</td>
<td>1</td>
</tr>
<tr class="even">
<td>Mixed forest</td>
<td>2</td>
<td>Some uncertainties</td>
<td>2</td>
</tr>
<tr class="odd">
<td>Coniferous forest</td>
<td>3</td>
<td>Very uncertain</td>
<td>3</td>
</tr>
<tr class="even">
<td>Non-forest</td>
<td>4</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="area-adjusted-accuracy-assessment" class="section level3">
<h3>2) Area-adjusted accuracy assessment</h3>
<p>Assess the class proportions of your map in R. To do so, use the <code>freq()</code> function to get pixel counts. Use these for calculating class propotions (0-1). Remember to exclude <code>NAs</code>, so the proportions of your 4 classes sum up to 1.</p>
<p>Load the reference point shapefile in R and <code>extract()</code> the map values at the point locations. Create a confusion matrix from the resulting <code>data.frame</code> using <code>table()</code>.</p>
<p>Copy &amp; paste the values from your confusion matrix as well as the class proportions into the Excel spreadsheet. Next, answer the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li>Which class has the highest / lowest user´s accuracy?</li>
<li>Which class has the highest / lowest producer´s accuracy?</li>
<li>How does the overall accuracy differ after area-adjustment? Why?</li>
<li>How do the map-based area estimates differ from those obtained using the reference data?</li>
</ol>
</div>
<div id="knowledge-transfer" class="section level3">
<h3>3) Knowledge transfer</h3>
<p>Implement three basic components of the accuracy assessment in R.</p>
<ol style="list-style-type: lower-alpha">
<li>Generate the confusion matrix containing probabilities. In this matrix, each cell value represents the probability of occurrence on map class <span class="math inline">\(i\)</span> and reference class <span class="math inline">\(j\)</span>:</li>
</ol>
<p><span class="math inline">\(p_{ij} = \frac{n_{ij}}{n_i} * w_i\)</span></p>
<p><span class="math inline">\(n_{ij}\)</span> = Number of pixels belonging to map class <span class="math inline">\(i\)</span> and reference class <span class="math inline">\(j\)</span></p>
<p><span class="math inline">\(n_i\)</span> = Total number of pixels of map class <span class="math inline">\(i\)</span></p>
<p><span class="math inline">\(w_i\)</span> = Proportion of map class <span class="math inline">\(i\)</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Calculate overall accuracy and class-wise user´s and producer´s accuracy from the confusion matrix.</li>
<li>Produce error-adjusted area estimates from the confusion matrix.</li>
</ol>
<p>To do this, track the underlying formulas of the respective sections in the Excel sheet and transfer them to R. Use helper functions such as <code>diag()</code>, <code>sum()</code> and <code>apply()</code>. Avoid <code>for</code> loops.</p>
<p>Compare the results with the values in the Excel table. If they differ, something went wrong. Check again!</p>
</div>
</div>
<div id="reading-materials-5" class="section level2">
<h2>Reading materials</h2>
<p>In this paper, <a href="http://doi.org/10.1126/science.1244693">Hansen et al. (2013)</a> presented the first global Landsat-based map of forest cover change. Please make sure to read the paper and the supplementary materials and take notes on questions of understanding and issues to discuss during the course.</p>
<hr />
</div>
</div>
<div id="session-07-multi-temporal-change-detection" class="section level1">
<h1>Session 07: Multi-temporal change detection</h1>
<div id="learning-goals-8" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Bundle your experiences to develop a supervised classification workflow</li>
<li>Map forest dynamics in the Southern Brazilian Amazon</li>
<li>Generate map-based estimates of forest loss and gain in the study region</li>
</ul>
</div>
<div id="background-4" class="section level2">
<h2>Background</h2>
<p>Deforestation processes are widespread across the globe. Remote sensing technologies are often used to map these processes, their spatial patterns and temporal evolution over large areas. An impressive case of the use of operational forest monitoring schemes is Brazil´s Amazon forest. Here, large-area deforestation for cattle ranching and crop cultivation occurs.</p>
<div class="figure">
<img src="fig/s07_para.jpg" alt="Aerial photograph of deforestation in southern Pará, Brazil. Source: Patrick Hostert" style="width:80.0%" />
<p class="caption">Aerial photograph of deforestation in southern Pará, Brazil. Source: Patrick Hostert</p>
</div>
<p>The <a href="http://www.dpi.inpe.br/prodesdigital/prodes.php?LANGUAGE=EN&amp;">PRODES</a> project, run by the National Institute for Space Research <a href="http://www.inpe.br/">INPE</a> provides information on spatially explicit information of forest loss using visual interpretation of Landsat and CBERS data. Often, visual interpretation is however not feasible and we desire an automated detection of change processes.</p>
<p>How do we get to the change map? Similar to the production of a land cover map, we can produce maps of land cover or land use change. This inherently incorporates the integration of multiple observation dates. Broadly speaking, change detection can occur in two ways:</p>
<ul>
<li><p><strong>Post-classification change detection</strong>: This refers to the independent production of land use or land cover maps for each time point of interest. In a next stept, differencing techniques are applied to investigate the change between two (or more) maps. While training data collection is relatively straight-forward, error propagation is a core issue for post-classification comparisons. It is therefore advised to assess the map quality of the final change map independently.</p></li>
<li><p><strong>Multi-temporal change classification</strong>: This refers to the direct classification of a stack of two (or more) images to assess the change classes of interest. Image stacks can be based on reflectance data, vegetation indices, or any other indicators that are comparable over time. Defining good change classes and collecting training data can be challenging. As an example: mapping change between 4 classes A, B, C and D, already requires training data for 16 change trajectories: A-A, A-B, A-C, A-D, B-A, B-B, …, D-C, D-D.</p></li>
</ul>
<div class="figure">
<img src="fig/s07_change.png" alt="Schematic illustration of change detection methods." />
<p class="caption">Schematic illustration of change detection methods.</p>
</div>
</div>
<div id="exercise-8" class="section level2">
<h2>Exercise</h2>
<p>After completing the last exercises, you gained a lot of experience in using R &amp; QGIS for data handling and pre-processing, training data collection, classification, and accuracy assessment. In other terms: you´re well prepared for performing a change detection using Landsat data. The goal of this exercise is to map forest cover change in the Southern Brazilian Amazon.</p>
<div class="figure">
<img src="fig/s07_ls.png" alt="Landsat images of study area for 2000, 2005, and 2010." />
<p class="caption">Landsat images of study area for 2000, 2005, and 2010.</p>
</div>
<p>We provided the following datasets in <a href="https://box.hu-berlin.de/f/bb4553666dff42c29cb9/?dl=1">our repository</a> and on drive O:</p>
<p>…sr_data/: Three Landsat 5 surface reflectance stacks (*_crp.tif) at five-year intervals from 2000 to 2010.</p>
<ul>
<li>LT052260682000061001T1-SC20170927060330 (10 June 2000)<br />
</li>
<li>LT052260682005060801T1-SC20170927060440 (08 June 2005)<br />
</li>
<li>LT052260682010062201T1-SC20170927060241 (22 June 2010)</li>
</ul>
<p>…vector/: A shapefile and a *.kmz file for GoogleEarth, which will help you to accurately delineate the Landsat pixel locations and extents for training data collection.</p>
<p>…validation/: A shapefile containing reference data for the accuracy assessment.</p>
<p>…gfc/: A subset of the Global Forest Change dataset by Hansen et al. (2013). We reclassified the data to match the target classes of today´s exercises.</p>
<p>Your task is to create a change map, which contains the following classes:</p>
<table>
<thead>
<tr class="header">
<th>classID</th>
<th>Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Stable forest</td>
</tr>
<tr class="even">
<td>2</td>
<td>Deforestation 2001-2005</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Deforestation 2006-2010</td>
</tr>
<tr class="even">
<td>4</td>
<td>Forest gain</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Stable non-forest</td>
</tr>
</tbody>
</table>
<p>Different from the previous exercises, you can decide on how the workflow of your map production looks like in detail. Use the code and tools from the last sessions and earlier experiences with remote sensing and associated software packages, if applicable. Consider the following steps:</p>
<div id="choosing-an-approach-for-change-detection" class="section level3">
<h3>1) Choosing an approach for change detection</h3>
<p>Several change detection approaches can be used to map forest cover changes. You can choose a methodological approach for your change detection. We suggest two approaches for supervised classification:</p>
<ul>
<li>Post-classification comparison: You perform a classification of land cover for each composite dataset. You then use raster algebra in R (or any other tool of your choice) to convert the individual results into a change map.</li>
<li>Integrated multi-temporal change classification: You stack all features into a single file and collect training data for the change classes of interest. You obtain the change map directly from your classification.</li>
</ul>
</div>
<div id="screening-of-input-data-and-training-data-collection" class="section level3">
<h3>2) Screening of input data and training data collection</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Visualize the Landsat images in QGIS. Test a true color (RGB), as well as a nIR-swIR1-red false color combination.<br />
Optional: Derive additional features from your data. Do you want to classify based on the spectral bands? What about vegetation indices, principal components, tasseled cap components?</p></li>
<li><p>Consider that the way you collect your training data varies with the approach you choose in 1).</p></li>
<li><p>Collecting training data is time consuming - try to be efficient. Make use of the QGIS Info-Tool to compare spectral profiles of the three images. You can also link QGIS and Google Earth through the GearthView plugin. For exact pixel locations, use the Landsat pixel grid we provided.</p></li>
</ol>
</div>
<div id="model-training-classification" class="section level3">
<h3>3) Model training &amp; classification</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Choose a classification algorithm: Feel free to use any classifier. In case you´re interested in differences between classifiers, test more than one!</p></li>
<li><p>Parametrize the model. Depending on your choice of classification model, think about how to make an informed decision about the model parameters. Optimize parameters wherever possible, e.g. by using the out-of-bag error.</p></li>
</ol>
</div>
<div id="change-map-validation-area-estimates" class="section level3">
<h3>4) Change map validation &amp; area estimates</h3>
<ol style="list-style-type: lower-alpha">
<li><p>We provided reference data for the study area as a shapefile. Use it to perform an accuracy assessment. Why is it challenging to perform an area-adjusted accuracy assessment in this case?</p></li>
<li><p>Calculate map-based area estimates for your map and the Global Forest Change data using simple pixel counting. Compare the estimates. What are the major differences?</p></li>
</ol>
<p>We collect your main results in <a href="https://ethercalc.org/rlko1iq98pqi">this table</a>. Please fill in all required fields, i.e. your name and selected approach (e.g. post-classification comparison), overall accuracy and class accuracies in decimal values (e.g. 0.75), as well as class areas in hectares.</p>
<hr />
</div>
</div>
<div id="reading-materials-6" class="section level2">
<h2>Reading materials</h2>
<p>In the next session, we would like to discuss <a href="https://doi.org/10.1016/j.rse.2014.10.014">Müller et al. (2015): Mining dense Landsat time series for separating cropland and pasture in a heterogeneous Brazilian savanna landscape</a>. In this paper, the authors explored the concept of spectral-temporal metrics for separating spectrally similar land cover classes in the Brazilian Cerrado. Please read the paper and make sure you understand the underlying concept. Write down questions of understanding and issues to discuss during the next session. Also, try to answer the following questions:</p>
<ul>
<li>How do spectral-temporal metrics conceptually differ from pixel-based composites?</li>
<li>For which type of (classification) problems can spectral-temporal metrics be useful?</li>
</ul>
<hr />
</div>
</div>
<div id="session-08-spectral-temporal-metrics" class="section level1">
<h1>Session 08: Spectral-temporal metrics</h1>
<div id="learning-goals-9" class="section level2">
<h2>Learning goals</h2>
<ul>
<li>Exploring the spectral-temporal characteristics of pastures and croplands in the Brazilian Amazon.</li>
<li>Generate spectral-temporal metrics from an intra-annual time series of Landsat-derived TC components.</li>
<li>Map pastures and cropland in the deforested areas.</li>
</ul>
</div>
<div id="background-5" class="section level2">
<h2>Background</h2>
<p>Understanding post-deforestation land use is essential to disentangle the drivers of deforestation.</p>
<div class="figure">
<img src="fig/s08_lu.png" alt="Post-deforestation land uses in the study region include intensive soy cultivation (left) and extensive cattle ranching (right). Images: Philippe Rufin" />
<p class="caption">Post-deforestation land uses in the study region include intensive soy cultivation (left) and extensive cattle ranching (right). Images: Philippe Rufin</p>
</div>
<p>From a remote sensing perspective, separating cropland and grassland is challenging due to similar spectral signatures in most periods of the year. In selected periods however, differences in land surface phenology become apparent. We can investigate these differences by creating an intra-annual time series of Tasseled Cap Greenness.</p>
<div class="figure">
<img src="fig/s08_tcg_ts.gif" alt="Intra-annual time series of Tasseled Cap Greenness for 2010." style="width:80.0%" />
<p class="caption">Intra-annual time series of Tasseled Cap Greenness for 2010.</p>
</div>
<p>Let´s investigate if we can see differences between pastures and croplands which play out over time. We collected 50 reference samples for pastures and 50 for cropland in the study region. We then extract the TCG value for all those locations in each of the time series bands. In a next step, we can create class-wise boxplots for each class and observation date, which shape a trajectory of land surface dynamics for each class.</p>
<div class="figure">
<img src="fig/s08_boxplots_ts.png" alt="Boxplots showing the distribution of TCG for pastures and cropland across all observations of the year 2010." />
<p class="caption">Boxplots showing the distribution of TCG for pastures and cropland across all observations of the year 2010.</p>
</div>
<p>Can you identify differences between the classes? Looking at the above graph suggests that croplands have a somewhat higher variability throughout the year, but on average, pastures show a slightly elevated level of TCG. We can try to verify this by calculating descriptive statistics of all TCG observations in the year and then comparing those between the classes. Let´s take a look at the maximum, mean, minimum, and standard deviation of TCG throughout 2010.</p>
<div class="figure">
<img src="fig/s08_boxplots_stm.png" alt="Boxplots showing the distribution of maximum, mean, minimum and standard deviation of TCG for 50 samples of pastures and 50 samples of cropland in 2010." />
<p class="caption">Boxplots showing the distribution of maximum, mean, minimum and standard deviation of TCG for 50 samples of pastures and 50 samples of cropland in 2010.</p>
</div>
<p>We can see that these “temporal statistics” can be useful to discriminate the two spectrally similar classes. Features combining the spectral appearance of a surface with its behaviour over time are increasingly popular in the remote sensing community. We refer to such features as spectral-temporal metrics (STM). STM are descriptive statistics of the spectral behaviour over time. They provide an efficient means to capture seasonal variations of land surface dynamics, while reducing the impact of clouds and cloud-shadows. STM can be calculated directly from the spectral bands, but also indices such as the NDVI, or TC components.</p>
<div class="figure">
<img src="fig/s08_stm.png" alt="Exemplary metrics calculated from intra-annual time series of Tasseled Cap Greenness." style="width:80.0%" />
<p class="caption">Exemplary metrics calculated from intra-annual time series of Tasseled Cap Greenness.</p>
</div>
<p>Different metrics might be more or less useful depending on the application. Good process understanding is key in finding out which STM are relevant for the problem at hand. We should decide on the type of spectral information and a set of statistics (metrics) to compute. Providing such information to a classifier can enable the production of thematically detailed land cover maps over large areas, such as a <a href="https://www2.hu-berlin.de/geomultisens/europeanLandCover/euroLandCover.html">pan-European land cover map</a>.</p>
<hr />
</div>
<div id="exercise-9" class="section level2">
<h2>Exercise</h2>
<p>In this exercise you will map pastures and croplands in deforested areas of the Brazilian Amazon. You will use and generate spectral-temporal metrics to separate these two land covers which are difficult to discriminate using single images. you will thereby be able to investigate which land cover is primarily replacing forests in the region.</p>
<p>We provide the following data in <a href="https://box.hu-berlin.de/f/cdc86f7993194946930f/?dl=1">our repository</a> and on drive O:</p>
<ol style="list-style-type: lower-alpha">
<li><p>…ts_stacks/: cloud-masked stacks of TC brightness (TCB), TC greenness (TCG), TC wetness (TCW) derived from 30 Landsat images acquired throughout 2010. Observation dates and sensors for each band in the stacks are listed in *.csv file.</p></li>
<li><p>…spectemp/: eight spectral temporal metrics for TCB, TCG and TCW: min, max, mean: minimum, maximum and mean value<br />
p25, p50, p75: 25%, 50%, 75% percentile values<br />
iqr: interquartile-range (25%-75%)<br />
std: standard deviation.</p></li>
<li><p>…vector/: a shapefile and *.kmz file with the study area and pixel extents</p></li>
<li><p>….TerraClass2010/: the TerraClass 2010 map product and a corresponding QGIS layer style file which contains the class catalogue.</p></li>
<li><p>…training/: a set of points to explore the spectral-temporal behaviour of your classes and to train your classification model.</p></li>
<li><p>…validation/: a set of points to assess the accuracy of your final classification.</p></li>
</ol>
<div id="inspect-the-input-data" class="section level3">
<h3>1) Inspect the input data</h3>
<p>Open a new QGIS project and inspect the spectral-temporal metrics. Choose one indicator (e.g. Tasseled Cap Greenness) and load all layers. Make sure you understand what the metrics represent and discuss their meaning, robustness (e.g. against cloud contamination), and behaviour over different land cover types with your neighbor(s).</p>
</div>
<div id="assess-suitability-of-spectral-temporal-metrics" class="section level3">
<h3>2) Assess suitability of spectral-temporal metrics</h3>
<p>We provide a shapefile with training points for the following classes</p>
<table>
<thead>
<tr class="header">
<th>classID</th>
<th>Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Pasture</td>
</tr>
<tr class="even">
<td>2</td>
<td>Cropland</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Others</td>
</tr>
</tbody>
</table>
<p><code>extract()</code> the values at your training locations from the temporal stacks (e.g., …/ts_stacks/TCG.tif). Compare the phenological behavior of the classes with boxplots or line plots. Use the following code to import the acquisition dates of the Landsat images:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data.path &lt;-<span class="st"> &#39;O:/ST19_MSc-EO/S08/data/&#39;</span>
sensor_date &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="kw">paste0</span>(data.path, <span class="st">&#39;ts_stacks/Landsat_2010_Sensor_Date.csv&#39;</span>, <span class="dt">sep=</span><span class="st">&quot;;&quot;</span>, <span class="dt">header=</span>T)</code></pre></div>
<p>Summarize the main characteristics of the three classes over the growing season and identify major differences. Please add your insights in comments in your script. Select spectral-temporal metrics that capture the differences between croplands, pastures, and others well. Select at most six different metrics from the ones provided.</p>
</div>
<div id="compute-additional-spectral-temporal-metrics" class="section level3">
<h3>3) Compute additional spectral-temporal metrics</h3>
<p>Let´s compute two additional spectral-temporal metrics from the time series stacks.</p>
<ol style="list-style-type: lower-alpha">
<li><p>First, think about creating a temporal subset of your time series for highlighting differences which are most pronounced during specific seasons (e.g. ‘dry season’, ‘third quarter’ or ‘August’).</p></li>
<li><p>Second, decide on the statistic to calculate. Your metrics can be for instance additional percentiles, information on the timing of specific events, such as the observation date of the minimum value, or differences between metrics. Be creative!</p></li>
</ol>
<p>How to do that: A number of summarizing functions can be applied directly to raster stacks (<code>mean()</code>, <code>max()</code>, <code>min()</code>, <code>range()</code>). If you want to calculate metrics other than those, you need to convert your raster stack into a matrix using <code>as.matrix()</code>. The below figure helps to understand how a three-dimensional raster stack is being transformed into a two-dimensional matrix.</p>
<div class="figure">
<img src="fig/s08_matrix.png" alt="Schematic illstration of converting a raster stack to a matrix using the as.matrix() function." />
<p class="caption">Schematic illstration of converting a raster stack to a matrix using the as.matrix() function.</p>
</div>
<p>Based on the resulting matrix, you can perform basic calculations using, e.g., <code>apply()</code>. You can then write the results into a new raster, which you create using <code>raster()</code> and assign the results of your computation with the <code>vals</code> parameter.</p>
<p>The following code template gives an example for computing the median Tasseled Cap Greenness. Computation might take several minutes - make use of the processing time by preparing the subsequent tasks.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Read TCG time series stack</span>
tcg.stack &lt;-<span class="st"> </span><span class="kw">stack</span>(<span class="st">&#39;O:/ST19_MSc-EO/S08/data/ts_stacks/TCG_stack.tif&#39;</span>)

<span class="co"># Convert to matrix</span>
tcg.matrix &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(tcg.stack)

<span class="co"># Calculate median across rows in matrix</span>
tcg.median &lt;-<span class="st"> </span><span class="kw">apply</span>(tcg.matrix, <span class="dv">1</span>, <span class="dt">FUN=</span>median, <span class="dt">na.rm=</span>T)

<span class="co"># Write results to empty raster</span>
tcg.median.raster &lt;-<span class="st"> </span><span class="kw">raster</span>(<span class="dt">nrows=</span>tcg.stack<span class="op">@</span>nrows, 
                            <span class="dt">ncols=</span>tcg.stack<span class="op">@</span>ncols, 
                            <span class="dt">crs=</span>tcg.stack<span class="op">@</span>crs, 
                            <span class="dt">vals=</span>tcg.median)

<span class="co"># Plot result</span>
<span class="kw">plot</span>(tcg.median.raster)</code></pre></div>
</div>
<div id="train-and-apply-random-forest-model" class="section level3">
<h3>4) Train and apply Random Forest Model</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Train a Random Forest model based on your selected and computed spectral-temporal metrics. As always, make an informed decision about the <code>ntree</code> and <code>mtry</code> parameters.</p></li>
<li><p>Produce a map using your Random Forest model and <code>mask()</code> the result with last week´s forest map, so only non-forest areas in 2010 are left. Write the result to disk using the appropriate <code>datatype</code>.</p></li>
</ol>
</div>
<div id="accuracy-assessment-and-area-estimates" class="section level3">
<h3>5) Accuracy assessment and area estimates</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Assess the accuracy of the map using the validation data we provided. Run an additional iteration using different metrics in case you are not satisifed with your results.</p></li>
<li><p>Combine the resulting map with your forest change map from last week to get a better insight into the land change processes of the study area. Calculate the fractions of deforested land occupied by pasture or cropland for each of the two time periods. Was forest primarily replaced by pastures or by cropland? Did this process change over the periods 2001-2005 and 2006-2010? Please enter your main results in <a href="https://ethercalc.org/sputdinazkck">this table</a>.</p></li>
</ol>
<hr />
</div>
</div>
</div>
<div id="session-09-project-phase-i" class="section level1">
<h1>Session 09: Project phase I</h1>
<div id="learning-goals-10" class="section level2">
<h2>Learning goals:</h2>
<ul>
<li>Develop a research question and specific objectives.</li>
<li>Sketch a workflow.</li>
<li>Acquire remote sensing data.</li>
</ul>
</div>
<div id="general-advice" class="section level2">
<h2>General advice</h2>
<p>We offer you three weeks of time for project work in teams of two. You should use this time to develop and execute a research objective in a guided setting. We iteratively give feedback. The overall goal is to strengthen your competence in problem-solving and thus to increase your confidence in working with remote sensing and other geospatial datasets. Developing a research idea is an iterative process. Running into problems and realizing things don´t work as expected is <a href="doi.org/10.1016/j.molcel.2009.09.013">part of research</a>. In case you notice that your project idea will not work out, try to identify alternative pathways out of “the cloud”.</p>
<div class="figure">
<img src="fig/s09_cloud.png" alt="The cloud (Alon 2009)" />
<p class="caption">“The cloud” (Alon 2009)</p>
</div>
<p>So how to find an interesting research strand? Existing literature can be a good source of inspiration. Think about papers you have read, which deserve further investigation or could be interesting to reproduce using different methods or datasets. Further, you may have stumbled upon interesting study sites in other classes. Did you ever consider mapping that peculiarity in your home region? The <a href="https://na.unep.net/atlas/google.php">UNEP Atlas of Environmental Change</a> can maybe get your thinking going.</p>
<p>Once you come up with an idea, answer the following questions:</p>
<p><strong>Is this research relevant?</strong> Try to chose a relevant topic for your project. However, remember that time is very limited, so don´t be too ambitious. A lot of research presents relatively incremental progress in a specific field. You could, e.g., improve existing research using different datasets, methods, class catalogs, or study periods.</p>
<p><strong>Are there existing studies guiding a way forward?</strong> If there are none, we recommend to rethink your proposed idea. Most relevant research strands were already approached, proposing valuable knowledge (gaps) to build upon. If there are several studies available, identify one key piece of literature, along which you can specify your research question and objective(s).</p>
<p><strong>How much data will I need?</strong> If the answer is “a lot”, you might be on the wrong path. Keep data volumes and processing time at a manageable scale to avoid frustration. Limit the size of your study region, the study period, and the temporal resolution to the minimum required to map the process of interest. Do not acquire more than 20GB of uncompressed raw satellite data. Always keep your working directory tidy by removing redundant data. Experienced students may consider using Google Earth Engine for the project.</p>
<p><strong>Are there reference datasets?</strong> Think about training and validation data collection early on. Make sure there is VHR data in GoogleEarth that allows for training your classes of interest, or figure out a way to extract reference data from the imagery itself. If the answer to this question is “no”, things will become very complex.</p>
</div>
<div id="practical-guidelines" class="section level2">
<h2>Practical guidelines</h2>
<p>Today you will start working on your individual projects! As a first step, we need to define a clear research question, objective(s) and sketch a workflow. The outcome of this block will be an abstract of max. 200 words per group. The abstract should be brief, but contain your motivation &amp; objectives, study area &amp; period, datasets (image data, training / validation) as well as a description of the main methods used for your analysis. Here is how you get there in three broad steps:</p>
<div id="defining-the-scope-of-your-project" class="section level3">
<h3>1) Defining the scope of your project:</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Specify a research question or hypotheses, around which you can develop one or two main objectives.</p></li>
<li><p>Search for relevant literature in your project context. Stick to recent publications in good remote sensing journals, (e.g. Remote Sensing of Environment, Remote Sensing, Applied Earth Observation and Geoinformation, IEEE JSTARS).</p></li>
<li><p>Evaluate the relevance of selected publications in more detail after you made a first selection. Go through the papers and try to identify aspects of the study which deserve further research.</p></li>
</ol>
</div>
<div id="defining-data-requirements-and-methods" class="section level3">
<h3>2) Defining data requirements and methods:</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Specify the exact study area (location, extent) and time frame relevant for your analysis.</p></li>
<li><p>Define the sensor and data product of your choice (e.g. Landsat BOA, Sentinel-2 BOA).</p></li>
<li><p>Develop a class catalogue including precise class definitions.</p></li>
<li><p>Define the required temporal resolution (e.g. 5-yearly, annual, intra-annual).</p></li>
<li><p>Choose a suitable method (e.g. pixel-based compositing, spectral-temporal metrics).</p></li>
</ol>
</div>
<div id="screen-data-availability" class="section level3">
<h3>3) Screen data availability:</h3>
<ol style="list-style-type: lower-alpha">
<li><p>Visit the <a href="https://earthexplorer.usgs.gov/">USGS EarthExplorer</a> or any other data distributio service you might know.</p></li>
<li><p>Define your study area and period, and have a look at the available data sets.</p></li>
<li><p>Select the required data product (e.g. Landsat Level-2) and specify additional criteria (e.g. cloud cover, collection category).</p></li>
<li><p>Browse through the search results. Are there images available for you time period? How about cloudiness in the study region? Any other limiting factors?</p></li>
<li><p>Check any other source of data that you might know of.</p></li>
</ol>
<p><strong>Until next week:</strong> Order and download the required data, so we can proceed with pre-processing.</p>
</div>
</div>
</div>
<div id="session-10-project-phase-ii" class="section level1">
<h1>Session 10: Project phase II</h1>
<p>Until here, you successfully managed to develop a research idea and acquired remote sensing datasets for further analyses. The next steps will vary largely depending on the specific project workflow and the following guidelines might therefore not suit every project. Feel free to follow, modify, or ignore them as you like.</p>
<div id="practical-guidelines-1" class="section level2">
<h2>Practical guidelines</h2>
<div id="produce-level-2-data" class="section level3">
<h3>1) Produce level 2 data</h3>
<p>In this step, you should perform the necessary pre-processing steps to prepare your downloaded data for the next steps. Often these data come as compressed archives with individual band and metadata files. To avoid unnecessary processing time in further steps, we need a cloud-masked stack covering the extent of our study region.</p>
<p>To get there, we need to do the following:</p>
<ul>
<li>Unpack the archives.</li>
<li>Stack the image bands of interest.</li>
<li>Crop the image(s) to the study region extent.</li>
<li>Mask clouds &amp; cloud shadows in the image(s).</li>
</ul>
</div>
<div id="further-processing-to-produce-level-3-data" class="section level3">
<h3>2) Further processing to produce level 3 data</h3>
<p>Producing meaningful features for classification or regression algorithms is a critical step towards good reults. Make use of the methods and skillsets you acquired during the course. Depending on your research project, it might make sense to perform pixel-based compositing, or to calculate spectral-temporal metrics. Remember, many of these steps can be performed using spectral bands, band indices, or Tasseled Cap components. Take enough time to discuss these issues in your team before making a final choice.</p>
</div>
<div id="training-data-collection" class="section level3">
<h3>3) Training data collection</h3>
<p>You have already done this a couple of times by now. Here are some guidelines that can make your life easier.</p>
<ul>
<li>Clearly define the class catalogue of interest and aggregate thematically irrelevant classes wherever it appears useful.</li>
<li>Define a minimum number of samples per class.<br />
</li>
<li>Set up an efficient work environment, e.g., a QGIS project containing your image data, other ancillary datasets, and a VHR baselayer (e.g., using the QuickMapServices plugin).</li>
<li>Make use of VHR data from Google Earth, but always consult the historic imagery toolbar to verify the acquisition dates of these data.<br />
</li>
<li>Save your progress after each collected point or polygon.</li>
<li>Explore the spectral and / or temporal characteristics of your training locations. Do they behave as expected? Are there outliers? Are your image features good discriminators for your classes of interest?</li>
</ul>
<p><strong>Until next week:</strong> Have your input features and training data ready, so we can proceed with producing results. Decide for an algorithm to use in your classification / regression problem. Discuss the parametrization of these algorithms in your team, i.e., which are the required parameters and how to determine useful parameter settings.</p>
<!-- 


## Phase III

------ 

-->
</div>
</div>
</div>
<div id="terminology" class="section level1">
<h1>Terminology</h1>
<p>This is a non-exhaustive list of common terms.</p>
<div id="observations" class="section level2">
<h2>Observations</h2>
<ul>
<li>Pixel</li>
<li>Observation</li>
<li>Path/Row; WRS-2; Footprint; Scene: Partitions of Landsat images into approximately 185 × 185 km squares.</li>
<li>Grid: an arbitrary subdivision with square units in the target coordinate system.</li>
<li>Tile: an entity of the grid with a unique tile identifier.</li>
<li>Chip: the individual gridded images that are affiliated with the tile.</li>
<li>Scene, Image</li>
<li>Data cube, Stack</li>
<li>Time series</li>
<li>Archive</li>
<li>Collection, Tier, Processing level</li>
<li>DN, TOA, BOA</li>
<li>VIS, nIR, swIR</li>
</ul>
</div>
<div id="sensor-characteristics" class="section level2">
<h2>Sensor characteristics</h2>
<ul>
<li>Satellite</li>
<li>Sensor</li>
<li>Spatial resolution</li>
<li>Temporal resolution</li>
<li>Temporal coverage</li>
<li>Spectral resolution</li>
<li>Radiometric resolution</li>
<li>Bits, Bytes</li>
</ul>
</div>
<div id="data-characteristics" class="section level2">
<h2>Data characteristics</h2>
<ul>
<li>Spectral band</li>
<li>Image quality</li>
<li>Very High Resolution (VHR)</li>
<li>High Resolution</li>
<li>Medium Resolution</li>
<li>Moderate Resolution</li>
<li>Coarse Resolution</li>
<li>Time Series</li>
<li>Observation density</li>
<li>Multi-temporal</li>
<li>Hyper-temporal</li>
</ul>
</div>
<div id="higher-level-products" class="section level2">
<h2>Higher-level products</h2>
<ul>
<li>Composite / Mosaic</li>
<li>Pixel-based composites (Best observation composite)</li>
<li>Phenology-adaptive composites</li>
<li>Spectral-temporal metrics</li>
<li>Rank-band composite / metric</li>
<li>Phenometrics</li>
</ul>
</div>
<div id="time-intervals" class="section level2">
<h2>Time intervals</h2>
<ul>
<li>Multi-annual</li>
<li>Inter-annual</li>
<li>Annual</li>
<li>Intra-annual</li>
<li>Seasonal</li>
</ul>
</div>
</div>

    </div>
    <div class="col-xs-2">
        </div>
  </div>
  </div>
  </div>
  <div class="row">
    </div>
  </div>

<script>
$(document).ready(function () {
  // add bootstrap table styles to pandoc tables
  $('tr.header').parent('thead').parent('table').addClass('table table-striped table-hover');

    var images = $('.pages img');
  images.filter(function() {
      if ($(this).parent().attr("class") == "figure") {
          return(false)
      } else {
          return(true);
      }
  }).wrap("<div class='figure'></div>");
  images.addClass("image-thumb").wrap("<div class='panel-body'></div>");
  $('.figure p.caption').wrap("<div class='panel-footer'></div>");
  $('.figure').addClass('panel panel-default');
  
    $('.pages img')
 	  .addClass("image-lb");
  $('.pages').magnificPopup({
	      type:'image',
	      closeOnContentClick: false,
	      closeBtnInside: false,
        delegate: 'img',
	      gallery: {enabled: false },
          removalDelay: 500,
          callbacks: {
              beforeOpen: function() {
                // just a hack that adds mfp-anim class to markup
                this.st.image.markup = this.st.image.markup.replace('mfp-figure', 'mfp-figure mfp-with-anim');
              }
          },
          mainClass: 'mfp-move-from-top',
	      image: {
	        verticalFit: true,
            titleSrc: 'alt'
	      }
 	    });
 	
    
    $('#toc ul li').first().addClass("active");
    $('#toc ul li').attr("data-target", function() {
        return($(this).children("a").attr("href"));
    })
    $('body .section.level1').first().addClass("active");
    
    $('#toc a[href*="#"]').click(function() {

      var id = $(this).attr("href");
      if (id === "#") return;
      if (id.substring(0, 8) === "#dyntab-") return;
      toggle_page(id);

      // Menu
      var menu_entry = $(".menu li[data-target='"+id+"']");
      menu_entry.addClass("active");
      $(".menu li").not(menu_entry).removeClass("active"); 
      

    });

    function toggle_page(id) {
      $(".page").not(page).removeClass("active").hide();
      window.page = id;
      var page = $(window.page);
      window.location.hash = window.page;
      //$(this).addClass("active");

      page.show();

      var totop = setInterval(function () {
        $(".pages").animate({scrollTop: 0}, 0);
      }, 10);

      setTimeout(function () {
        page.addClass("active");
        setTimeout(function () {
          clearInterval(totop);
        }, 1000);
      }, 100);

      window.dispatchEvent(new Event('resize'));

    }


    $(".menu li").click(function () {

      toggle_page($(this).data("target"));

      // Menu
      if (!$(this).data("target")) return;
      if ($(this).is(".active")) return;
      $(".menu li").not($(this)).removeClass("active");
      $(this).addClass("active");

    });
  
    


    window.page = window.location.hash;
    if (window.page != "") {
      $(".menu").find("li[data-target=" + window.page + "]").trigger("click");
    }

    /* init material bootstrap js */
    $.material.init();
});
</script>




<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
